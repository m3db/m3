[
{
	"uri": "/",
	"title": "M3 Introduction",
	"tags": [],
	"description": "",
	"content": "About After using open-source metrics solutions and finding issues with them at scale – such as reliability, cost, and operational complexity – M3 was created from the ground up to provide Uber with a native, distributed time series database, a highly-dynamic and performant aggregation service, a query engine, and other supporting infrastructure.\nKey Features M3 has several features, provided as discrete components, which make it an ideal platform for time series data at scale:\n A distributed time series database, M3DB, that provides scalable storage for time series data and a reverse index. A sidecar process, M3Coordinator, that allows M3DB to act as the long-term storage for Prometheus. A distributed query engine, M3Query, with native support for PromQL and Graphite (M3QL coming soon).  An aggregation tier, M3Aggregator, that runs as a dedicated metrics aggregator/downsampler allowing metrics to be stored at various retentions at different resolutions.  Getting Started Note: Make sure to read our Operational Guides before running in production!\nGetting started with M3 is as easy as following one of the How-To guides.\n Single M3DB node deployment Clustered M3DB deployment M3DB on Kubernetes Isolated M3Query on deployment  Support For support with any issues, questions about M3 or its operation, or to leave any comments, the team can be reached in a variety of ways:\n Slack (main chat channel) Gitter (old chat channel) Email Github issues  "
},
{
	"uri": "/m3coordinator/api/remote/",
	"title": "API",
	"tags": [],
	"description": "",
	"content": "The M3 Coordinator implements the Prometheus Remote Read and Write HTTP endpoints, they also can be used however as general purpose metrics write and read APIs. Any metrics that are written to the remote write API can be queried using PromQL through the query APIs as well as being able to be read back by the Prometheus Remote Read endpoint.\nRemote Write Write a Prometheus Remote write query to M3.\nURL /api/v1/prom/remote/write\nMethod POST\nURL Params None.\nHeader Params Optional  M3-Metrics-Type:\nIf this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting downsample options to all: false for a namespace in the coordinator config, for more see disabling automatic aggregation.\nMust be one of:\nunaggregated: Write metrics directly to configured unaggregated namespace.\naggregated: Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the M3-Storage-Policy header to be set to resolve which namespace to write metrics to.\n M3-Storage-Policy:\nIf this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation).\nThe value of the header must be in the format of resolution:retention in duration shorthand. e.g. 1m:48h specifices 1 minute resolution and 48 hour retention. Valid time units are \u0026ldquo;ns\u0026rdquo;, \u0026ldquo;us\u0026rdquo; (or \u0026ldquo;µs\u0026rdquo;), \u0026ldquo;ms\u0026rdquo;, \u0026ldquo;s\u0026rdquo;, \u0026ldquo;m\u0026rdquo;, \u0026ldquo;h\u0026rdquo;.\nHere is an example of querying metrics from a specific namespace.\n   M3-Map-Tags-JSON:\nIf this header is set it enables dynamically mutating tags in a Prometheus write request. See issue 2254 for further context.\nCurrently only write is supported. As an example, the following header would unconditionally cause globaltag=somevalue to be added to all metrics in a write request:  M3-Map-Tags-JSON: '{\u0026quot;tagMappers\u0026quot;:[{\u0026quot;write\u0026quot;:{\u0026quot;tag\u0026quot;:\u0026quot;globaltag\u0026quot;,\u0026quot;value\u0026quot;:\u0026quot;somevalue\u0026quot;}}]}' \nData Params Binary snappy compressed Prometheus WriteRequest protobuf message.\nAvailable Tuning Params Refer here for an up to date list of remote tuning parameters.\nSample Call There isn\u0026rsquo;t a straightforward way to Snappy compress and marshal a Prometheus WriteRequest protobuf message using just shell, so this example uses a specific command line utility instead.\nThis sample call is made using promremotecli which is a command line tool that uses a Go client to Prometheus Remote endpoints. For more information visit the GitHub repository.\nThere is also a Java client that can be used to make requests to the endpoint.\nEach -t parameter specifies a label (dimension) to add to the metric.\nThe -h parameter can be used as many times as necessary to add headers to the outgoing request in the form of \u0026ldquo;Header-Name: HeaderValue\u0026rdquo;.\nHere is an example of writing the datapoint at the current unix timestamp with value 123.456:\ndocker run -it --rm \\  quay.io/m3db/prometheus_remote_client_golang:latest \\  -u http://host.docker.internal:7201/api/v1/prom/remote/write \\  -t __name__:http_requests_total \\  -t code:200 \\  -t handler:graph \\  -t method:get \\  -d $(date +\u0026#34;%s\u0026#34;),123.456 promremotecli_log 2019/06/25 04:13:56 writing datapoint [2019-06-25 04:13:55 +0000 UTC 123.456] promremotecli_log 2019/06/25 04:13:56 labelled [[__name__ http_requests_total] [code 200] [handler graph] [method get]] promremotecli_log 2019/06/25 04:13:56 writing to http://host.docker.internal:7201/api/v1/prom/remote/write {\u0026#34;success\u0026#34;:true,\u0026#34;statusCode\u0026#34;:200} promremotecli_log 2019/06/25 04:13:56 write success # If you are paranoid about image tags being hijacked/replaced with nefarious code, you can use this SHA256 tag: # quay.io/m3db/prometheus_remote_client_golang@sha256:fc56df819bff9a5a087484804acf3a584dd4a78c68900c31a28896ed66ca7e7b For more details on querying data in PromQL that was written using this endpoint, see the query API documentation.\nRemote Read Read Prometheus metrics from M3.\nURL /api/v1/prom/remote/read\nMethod POST\nURL Params None.\nHeader Params Optional  M3-Metrics-Type:\nIf this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting downsample options to all: false for a namespace in the coordinator config, for more see disabling automatic aggregation.\nMust be one of:\nunaggregated: Write metrics directly to configured unaggregated namespace.\naggregated: Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the M3-Storage-Policy header to be set to resolve which namespace to write metrics to.\n M3-Storage-Policy:\nIf this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation).\nThe value of the header must be in the format of resolution:retention in duration shorthand. e.g. 1m:48h specifices 1 minute resolution and 48 hour retention. Valid time units are \u0026ldquo;ns\u0026rdquo;, \u0026ldquo;us\u0026rdquo; (or \u0026ldquo;µs\u0026rdquo;), \u0026ldquo;ms\u0026rdquo;, \u0026ldquo;s\u0026rdquo;, \u0026ldquo;m\u0026rdquo;, \u0026ldquo;h\u0026rdquo;.\nHere is an example of querying metrics from a specific namespace.\n  {{% fileinclude file=\u0026quot;/includes/headers_optional_read_limits.md\u0026quot; %}}\n M3-Restrict-By-Tags-JSON:\nIf this header is set it can ensure specific label matching is performed as part of every query including series metadata endpoints. As an example, the following header would unconditionally cause globaltag=somevalue to be a part of all queries issued regardless of if they include the label or not in a query and also strip the \u0026ldquo;globaltag\u0026rdquo; from appearing as a label in any of the resulting timeseries:  M3-Restrict-By-Tags-JSON: '{\u0026quot;match\u0026quot;:[{\u0026quot;name\u0026quot;:\u0026quot;globaltag\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;EQUAL\u0026quot;,\u0026quot;value\u0026quot;:\u0026quot;somevalue\u0026quot;}],\u0026quot;strip\u0026quot;:[\u0026quot;globaltag\u0026quot;]}' \nData Params Binary snappy compressed Prometheus WriteRequest protobuf message.\n"
},
{
	"uri": "/m3query/api/query/",
	"title": "API",
	"tags": [],
	"description": "",
	"content": "Please note: This documentation is a work in progress and more detail is required.\nQuery using PromQL Query using PromQL and returns JSON datapoints compatible with the Prometheus Grafana plugin.\nURL /api/v1/query_range\nMethod GET\nURL Params Required  start=[time in RFC3339Nano] end=[time in RFC3339Nano] step=[time duration] target=[string]  Optional  debug=[bool] lookback=[string|time duration]: This sets the per request lookback duration to something other than the default set in config, can either be a time duration or the string \u0026ldquo;step\u0026rdquo; which sets the lookback to the same as the step request parameter.  Header Params Optional  M3-Metrics-Type:\nIf this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting downsample options to all: false for a namespace in the coordinator config, for more see disabling automatic aggregation.\nMust be one of:\nunaggregated: Write metrics directly to configured unaggregated namespace.\naggregated: Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the M3-Storage-Policy header to be set to resolve which namespace to write metrics to.\n M3-Storage-Policy:\nIf this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation).\nThe value of the header must be in the format of resolution:retention in duration shorthand. e.g. 1m:48h specifices 1 minute resolution and 48 hour retention. Valid time units are \u0026ldquo;ns\u0026rdquo;, \u0026ldquo;us\u0026rdquo; (or \u0026ldquo;µs\u0026rdquo;), \u0026ldquo;ms\u0026rdquo;, \u0026ldquo;s\u0026rdquo;, \u0026ldquo;m\u0026rdquo;, \u0026ldquo;h\u0026rdquo;.\nHere is an example of querying metrics from a specific namespace.\n  {{% fileinclude file=\u0026quot;/includes/headers_optional_read_limits.md\u0026quot; %}}\n M3-Restrict-By-Tags-JSON:\nIf this header is set it can ensure specific label matching is performed as part of every query including series metadata endpoints. As an example, the following header would unconditionally cause globaltag=somevalue to be a part of all queries issued regardless of if they include the label or not in a query and also strip the \u0026ldquo;globaltag\u0026rdquo; from appearing as a label in any of the resulting timeseries:  M3-Restrict-By-Tags-JSON: '{\u0026quot;match\u0026quot;:[{\u0026quot;name\u0026quot;:\u0026quot;globaltag\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;EQUAL\u0026quot;,\u0026quot;value\u0026quot;:\u0026quot;somevalue\u0026quot;}],\u0026quot;strip\u0026quot;:[\u0026quot;globaltag\u0026quot;]}' \nData Params None.\nSample Call curl \u0026#39;http://localhost:7201/api/v1/query_range?query=abs(http_requests_total)\u0026amp;start=1530220860\u0026amp;end=1530220900\u0026amp;step=15s\u0026#39; { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;resultType\u0026#34;: \u0026#34;matrix\u0026#34;, \u0026#34;result\u0026#34;: [ { \u0026#34;metric\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;handler\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;get\u0026#34; }, \u0026#34;values\u0026#34;: [ [ 1530220860, \u0026#34;6\u0026#34; ], [ 1530220875, \u0026#34;6\u0026#34; ], [ 1530220890, \u0026#34;6\u0026#34; ] ] }, { \u0026#34;metric\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;handler\u0026#34;: \u0026#34;label_values\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;get\u0026#34; }, \u0026#34;values\u0026#34;: [ [ 1530220860, \u0026#34;6\u0026#34; ], [ 1530220875, \u0026#34;6\u0026#34; ], [ 1530220890, \u0026#34;6\u0026#34; ] ] } ] } } "
},
{
	"uri": "/m3query/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "Please note: This documentation is a work in progress and more detail is required.\nOverview M3 Query and M3 Coordinator are written entirely in Go, M3 Query is as a query engine for M3DB and M3 Coordinator is a remote read/write endpoint for Prometheus and M3DB. To learn more about Prometheus\u0026rsquo;s remote endpoints and storage, see here.\n"
},
{
	"uri": "/m3query/architecture/blocks/",
	"title": "Blocks",
	"tags": [],
	"description": "",
	"content": "Please note: This documentation is a work in progress and more detail is required.\nOverview The fundamental data structures that M3 Query uses are Blocks. Blocks are what get created from the series iterators that M3DB returns. A Block is associated with a start and end time. It contains data from multiple time series stored in columnar format.\nMost transformations within M3 Query will be applied across different series for each time interval. Therefore, having data stored in columnar format helps with the memory locality of the data. Moreover, most transformations within M3 Query can work in parallel on different blocks which can significantly increase the computation speed.\nDiagram Below is a visual representation of a set of Blocks. On top is the M3QL query that gets executed, and on the bottom, are the results of the query containing 3 different Blocks.\n ┌───────────────────────────────────────────────────────────────────────┐ │ │ │ fetch name:sign_up city_id:{new_york,san_diego,toronto} os:* │ │ │ └───────────────────────────────────────────────────────────────────────┘ │ │ │ │ │ │ │ │ │ ▼ ▼ ▼ ┌────────────┐ ┌────────────┐ ┌─────────────┐ │ Block One │ │ Block Two │ │ Block Three │ └────────────┘ └────────────┘ └─────────────┘ ┌──────┬──────┬──────┐ ┌──────┬──────┬──────┐ ┌──────┬──────┬──────┐ │ t │ t+1 │ t+2 │ │ t+3 │ t+4 │ t+5 │ │ t+6 │ t+7 │ t+8 │ ├──────┼──────┼──────▶ ├──────┼──────┼──────▶ ├──────┼──────┼──────▶ ┌───────────────────────────┐ │ │ │ │ │ │ │ │ │ │ │ │ │ name:sign_up │ │ │ │ │ │ │ │ │ │ │ │ │ │ city_id:new_york os:ios │ │ 5 │ 2 │ 10 │ │ 10 │ 2 │ 10 │ │ 5 │ 3 │ 5 │ └───────────────────────────┘ │ │ │ │ │ │ │ │ │ │ │ │ ├──────┼──────┼──────▶ ├──────┼──────┼──────▶ ├──────┼──────┼──────▶ ┌───────────────────────────┐ │ │ │ │ │ │ │ │ │ │ │ │ │ name:sign_up │ │ │ │ │ │ │ │ │ │ │ │ │ │city_id:new_york os:android│ │ 10 │ 8 │ 5 │ │ 20 │ 4 │ 5 │ │ 10 │ 8 │ 5 │ └───────────────────────────┘ │ │ │ │ │ │ │ │ │ │ │ │ ├──────┼──────┼──────▶ ├──────┼──────┼──────▶ ├──────┼──────┼──────▶ ┌───────────────────────────┐ │ │ │ │ │ │ │ │ │ │ │ │ │ name:sign_up │ │ │ │ │ │ │ │ │ │ │ │ │ │ city_id:san_diego os:ios │ │ 10 │ 5 │ 10 │ │ 2 │ 5 │ 10 │ │ 8 │ 6 │ 6 │ └───────────────────────────┘ │ │ │ │ │ │ │ │ │ │ │ │ ├──────┼──────┼──────▶ ├──────┼──────┼──────▶ ├──────┼──────┼──────▶ ┌───────────────────────────┐ │ │ │ │ │ │ │ │ │ │ │ │ │ name:sign_up │ │ │ │ │ │ │ │ │ │ │ │ │ │ city_id:toronto os:ios │ │ 2 │ 5 │ 10 │ │ 2 │ 5 │ 10 │ │ 2 │ 5 │ 10 │ └───────────────────────────┘ │ │ │ │ │ │ │ │ │ │ │ │ └──────┴──────┴──────┘ └──────┴──────┴──────┘ └──────┴──────┴──────┘ M3DB =\u0026gt; M3 Query Blocks In order to convert M3DB blocks into M3 Query blocks, we need to consolidate across different namespaces. In short, M3DB namespaces are essentially different resolutions that metrics are stored at. For example, a metric might be stored at both 1min and 10min resolutions- meaning this metric is found in two namespaces.\nAt a high level, M3DB returns to M3 Query SeriesBlocks that contain a list of SeriesIterators for a given timeseries per namespace. M3 Query then aligns the blocks across common time bounds before applying consolidation.\nFor example, let\u0026rsquo;s say we have a query that returns two timeseries from two different namespaces- 1min and 10min. When we create the M3 Query Block, in order to accurately consolidate results from these two namespaces, we need to convert everything to have a 10min resolution. Otherwise it will not be possible to perform correctly apply functions.\n Coming Soon: More documentation on how M3 Query applies consolidation.\n "
},
{
	"uri": "/overview/components/",
	"title": "Components",
	"tags": [],
	"description": "",
	"content": "M3 Coordinator M3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and M3DB. It is a bridge that users can deploy to access the benefits of M3DB such as long term storage and multi-DC setup with other monitoring systems, such as Prometheus. See this presentation for more on long term storage in Prometheus.\nM3DB M3DB is a distributed time series database that provides scalable storage and a reverse index of time series. It is optimized as a cost effective and reliable realtime and long term retention metrics store and index. For more details, see the M3DB documentation.\nM3 Query M3 Query is a service that houses a distributed query engine for querying both realtime and historical metrics, supporting several different query languages. It is designed to support both low latency realtime queries and queries that can take longer to execute, aggregating over much larger datasets, for analytical use cases. For more details, see the query engine documentation.\nM3 Aggregator M3 Aggregator is a service that runs as a dedicated metrics aggregator and provides stream based downsampling, based on dynamic rules stored in etcd. It uses leader election and aggregation window tracking, leveraging etcd to manage this state, to reliably emit at-least-once aggregations for downsampled metrics to long term storage. This provides cost effective and reliable downsampling \u0026amp; roll up of metrics. These features also reside in the M3 Coordinator, however the dedicated aggregator is sharded and replicated, whereas the M3 Coordinator is not and requires care to deploy and run in a highly available way. There is work remaining to make the aggregator more accessible to users without requiring them to write their own compatible producer and consumer.\n"
},
{
	"uri": "/m3query/config/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": "Default query engine By default M3 runs two query engines:\n Prometheus (default) - robust and de-facto query language for metrics M3 Query Engine - high-performance query engine but doesn\u0026rsquo;t support all the functions yet  Prometheus Query Engine is the default one when calling query endpoint:\nhttp://localhost:7201/api/v1/query?query=count(http_requests)\u0026amp;time=1590147165 But you can switch between the two in the following ways:\n  Changing default query engine in config file (see defaultEngine parameter in Configuration)\n  Passing HTTP header M3-Engine:\ncurl -H \u0026quot;M3-Engine: m3query\u0026quot; \u0026quot;http://localhost:7201/api/v1/query?query=count(http_requests)\u0026amp;time=1590147165\u0026quot;\nor\ncurl -H \u0026quot;M3-Engine: prometheus\u0026quot; \u0026quot;http://localhost:7201/api/v1/query?query=count(http_requests)\u0026amp;time=1590147165\u0026quot;\n  Passing HTTP query URL parameter engine:\ncurl \u0026quot;http://localhost:7201/api/v1/query?engine=m3query\u0026amp;query=count(http_requests)\u0026amp;time=1590147165\u0026quot;\nor\ncurl \u0026quot;http://localhost:7201/api/v1/query?engine=prometheus\u0026amp;query=count(http_requests)\u0026amp;time=1590147165\u0026quot;\n  Using different URLs:\n /prometheus/api/v1/* - to call Prometheus query engine /m3query/api/v1/* - to call M3 Query query engine  curl \u0026quot;http://localhost:7201/m3query/api/v1/query?query=count(http_requests)\u0026amp;time=1590147165\u0026quot;\nor\ncurl \u0026quot;http://localhost:7201/prometheus/api/v1/query?query=count(http_requests)\u0026amp;time=1590147165\u0026quot;\n  "
},
{
	"uri": "/how_to/single_node/",
	"title": "M3DB Single Node Deployment",
	"tags": [],
	"description": "",
	"content": "Deploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it has to offer. Our Docker image by default configures a single M3DB instance as one binary containing:\n An M3DB storage instance (m3dbnode) for timeseries storage. This includes an embedded tag-based metrics index, as well as as an embedded etcd server for storing the above mentioned cluster topology and runtime configuration. A \u0026ldquo;coordinator\u0026rdquo; instance (m3coordinator) for writing and querying tagged metrics, as well as managing cluster topology and runtime configuration.  To begin, first start up a Docker container with port 7201 (used to manage the cluster topology), port 7203 which is where Prometheus scrapes metrics produced by M3DB and M3Coordinator, and port 9003 (used to read and write metrics) exposed. We recommend you create a persistent data directory on your host for durability:\ndocker pull quay.io/m3db/m3dbnode:latest docker run -p 7201:7201 -p 7203:7203 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3db/m3dbnode:latest Note: For the single node case, we use this sample config file. If you inspect the file, you\u0026rsquo;ll see that all the configuration is grouped by coordinator or db. That\u0026rsquo;s because this setup runs M3DB and M3Coordinator as one application. While this is convenient for testing and development, you\u0026rsquo;ll want to run clustered M3DB with a separate M3Coordinator in production. You can read more about that here..\nNext, create an initial namespace for your metrics in the database using the cURL below. Keep in mind that the provided namespaceName must match the namespace in the local section of the M3Coordinator YAML configuration, and if you choose to add any additional namespaces you\u0026rsquo;ll need to add them to the local section of M3Coordinator\u0026rsquo;s YAML config as well.\ncurl -X POST http://localhost:7201/api/v1/database/create -d \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;namespaceName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;retentionTime\u0026#34;: \u0026#34;12h\u0026#34; }\u0026#39; Note: The api/v1/database/create endpoint is abstraction over two concepts in M3DB called placements and namespaces. If a placement doesn\u0026rsquo;t exist, it will create one based on the type argument, otherwise if the placement already exists, it just creates the specified namespace. For now it\u0026rsquo;s enough to just understand that it creates M3DB namespaces (tables), but if you\u0026rsquo;re going to run a clustered M3 setup in production, make sure you familiarize yourself with the links above.\nPlacement initialization may take a minute or two and you can check on the status of this by running the following:\ncurl http://localhost:7201/api/v1/placement | jq . Once all of the shards become AVAILABLE, you should see your node complete bootstrapping! Don\u0026rsquo;t worry if you see warnings or errors related to a local cache file, such as [W] could not load cache from file /var/lib/m3kv/m3db_embedded.json. Those are expected for a local instance and in general any warn-level errors (prefixed with [W]) should not block bootstrapping.\n02:28:30.008072[I] updating database namespaces [{adds [default]} {updates []} {removals []}] 02:28:30.270681[I] node tchannelthrift: listening on 0.0.0.0:9000 02:28:30.271909[I] cluster tchannelthrift: listening on 0.0.0.0:9001 02:28:30.519468[I] node httpjson: listening on 0.0.0.0:9002 02:28:30.520061[I] cluster httpjson: listening on 0.0.0.0:9003 02:28:30.520652[I] bootstrap finished [{namespace metrics} {duration 55.4µs}] 02:28:30.520909[I] bootstrapped The node also self-hosts its OpenAPI docs, outlining available endpoints. You can access this by going to localhost:7201/api/v1/openapi in your browser.\nNow you can experiment with writing tagged metrics:\ncurl -sS -X POST http://localhost:9003/writetagged -d \u0026#39;{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;__name__\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;user_login\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new_york\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;endpoint\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/request\u0026#34; } ], \u0026#34;datapoint\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#39;\u0026#34;$(date \u0026#34;+%s\u0026#34;)\u0026#34;\u0026#39;, \u0026#34;value\u0026#34;: 42.123456789 } } \u0026#39; Note: In the above example we include the tag __name__. This is because __name__ is a reserved tag in Prometheus and will make querying the metric much easier. For example, if you have M3Query setup as a Prometheus datasource in Grafana, you can then query for the metric using the following PromQL query:\nuser_login{city=\u0026#34;new_york\u0026#34;,endpoint=\u0026#34;/request\u0026#34;} And reading the metrics you\u0026rsquo;ve written using the M3DB /query endpoint:\ncurl -sS -X POST http://localhost:9003/query -d \u0026#39;{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;regexp\u0026#34;: \u0026#34;.*\u0026#34; } }, \u0026#34;rangeStart\u0026#34;: 0, \u0026#34;rangeEnd\u0026#34;: \u0026#39;\u0026#34;$(date \u0026#34;+%s\u0026#34;)\u0026#34;\u0026#39; }\u0026#39; | jq . { \u0026#34;results\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;__name__\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;user_login\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new_york\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;endpoint\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/request\u0026#34; } ], \u0026#34;datapoints\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: 1527039389, \u0026#34;value\u0026#34;: 42.123456789 } ] } ], \u0026#34;exhaustive\u0026#34;: true } Now that you\u0026rsquo;ve got the M3 stack up and running, take a look at the rest of our documentation to see how you can integrate with Prometheus and Graphite\n"
},
{
	"uri": "/integrations/prometheus/",
	"title": "Prometheus",
	"tags": [],
	"description": "",
	"content": "This document is a getting started guide to integrating M3DB with Prometheus.\nM3 Coordinator configuration To write to a remote M3DB cluster the simplest configuration is to run m3coordinator as a sidecar alongside Prometheus.\nStart by downloading the config template. Update the namespaces and the client section for a new cluster to match your cluster\u0026rsquo;s configuration.\nYou\u0026rsquo;ll need to specify the static IPs or hostnames of your M3DB seed nodes, and the name and retention values of the namespace you set up. You can leave the namespace storage metrics type as unaggregated since it\u0026rsquo;s required by default to have a cluster that receives all Prometheus metrics unaggregated. In the future you might also want to aggregate and downsample metrics for longer retention, and you can come back and update the config once you\u0026rsquo;ve setup those clusters. You can read more about our aggregation functionality here.\nIt should look something like:\nlistenAddress: type: \u0026quot;config\u0026quot; value: \u0026quot;0.0.0.0:7201\u0026quot; logging: level: info metrics: scope: prefix: \u0026quot;coordinator\u0026quot; prometheus: handlerPath: /metrics listenAddress: 0.0.0.0:7203 # until https://github.com/m3db/m3/issues/682 is resolved sanitization: prometheus samplingRate: 1.0 extended: none tagOptions: idScheme: quoted clusters: - namespaces: # We created a namespace called \u0026quot;default\u0026quot; and had set it to retention \u0026quot;48h\u0026quot;. - namespace: default retention: 48h type: unaggregated client: config: service: env: default_env zone: embedded service: m3db cacheDir: /var/lib/m3kv etcdClusters: - zone: embedded endpoints: # We have five M3DB nodes but only three are seed nodes, they are listed here. - M3DB_NODE_01_STATIC_IP_ADDRESS:2379 - M3DB_NODE_02_STATIC_IP_ADDRESS:2379 - M3DB_NODE_03_STATIC_IP_ADDRESS:2379 writeConsistencyLevel: majority readConsistencyLevel: unstrict_majority writeTimeout: 10s fetchTimeout: 15s connectTimeout: 20s writeRetry: initialBackoff: 500ms backoffFactor: 3 maxRetries: 2 jitter: true fetchRetry: initialBackoff: 500ms backoffFactor: 2 maxRetries: 3 jitter: true backgroundHealthCheckFailLimit: 4 backgroundHealthCheckFailThrottleFactor: 0.5 Now start the process up:\nm3coordinator -f \u0026lt;config-name.yml\u0026gt; Or, use the docker container:\ndocker pull quay.io/m3db/m3coordinator:latest docker run -p 7201:7201 --name m3coordinator -v \u0026lt;config-name.yml\u0026gt;:/etc/m3coordinator/m3coordinator.yml quay.io/m3db/m3coordinator:latest Prometheus configuration Add to your Prometheus configuration the m3coordinator sidecar remote read/write endpoints, something like:\nremote_read: - url: \u0026quot;http://localhost:7201/api/v1/prom/remote/read\u0026quot; # To test reading even when local Prometheus has the data read_recent: true remote_write: - url: \u0026quot;http://localhost:7201/api/v1/prom/remote/write\u0026quot; Also, we recommend adding M3DB and M3Coordinator/M3Query to your list of jobs under scrape_configs so that you can monitor them using Prometheus. With this scraping setup, you can also use our pre-configured M3DB Grafana dashboard.\n- job_name: \u0026#39;m3db\u0026#39; static_configs: - targets: [\u0026#39;\u0026lt;M3DB_HOST_NAME_1\u0026gt;:7203\u0026#39;, \u0026#39;\u0026lt;M3DB_HOST_NAME_2\u0026gt;:7203\u0026#39;, \u0026#39;\u0026lt;M3DB_HOST_NAME_3\u0026gt;:7203\u0026#39;] - job_name: \u0026#39;m3coordinator\u0026#39; static_configs: - targets: [\u0026#39;\u0026lt;M3COORDINATOR_HOST_NAME_1\u0026gt;:7203\u0026#39;] NOTE: If you are running M3DB with embedded M3Coordinator, you should only have one job. We recommend just calling this job m3. For example:\n- job_name: \u0026#39;m3\u0026#39; static_configs: - targets: [\u0026#39;\u0026lt;HOST_NAME\u0026gt;:7203\u0026#39;] Querying With Grafana When using the Prometheus integration with Grafana, there are two different ways you can query for your metrics. The first option is to configure Grafana to query Prometheus directly by following these instructions.\nAlternatively, you can configure Grafana to read metrics directly from M3Coordinator in which case you will bypass Prometheus entirely and use M3\u0026rsquo;s PromQL engine instead. To set this up, follow the same instructions from the previous step, but set the url to: http://\u0026lt;M3_COORDINATOR_HOST_NAME\u0026gt;:7201.\n"
},
{
	"uri": "/operational_guide/replication_and_deployment_in_zones/",
	"title": "Replication and Deployment in Zones",
	"tags": [],
	"description": "",
	"content": "M3DB supports both deploying across multiple zones in a region or deploying to a single zone with rack-level isolation. It can also be deployed across multiple regions for a global view of data, though both latency and bandwidth costs may increase as a result.\nIn addition, M3DB has support for automatically replicating data between isolated M3DB clusters (potentially running in different zones / regions). More details can be found in the Replication between clusters operational guide.\nReplication A replication factor of at least 3 is highly recommended for any M3DB deployment, due to the consistency levels (for both reads and writes) that require quorum in order to complete an operation. For more information on consistency levels, see the documentation concerning tuning availability, consistency and durability.\nM3DB will do its best to distribute shards evenly among the availability zones while still taking each individual node\u0026rsquo;s weight into account, but if some of the availability zones have less available hosts than others then each host in that zone will be responsible for more shards than hosts in the other zones and will thus be subjected to heavier load.\nReplication Factor Recommendations Running with RF=1 or RF=2 is not recommended for any multi-node use cases (testing or production). In the future such topologies may be rejected by M3DB entirely. It is also recommended to only run with an odd number of replicas.\nRF=1 is not recommended as it is impossible to perform a safe upgrade or tolerate any node failures: as soon as one node is down, all writes destined for the shards it owned will fail. If the node\u0026rsquo;s storage is lost (e.g. the disk fails), the data is gone forever.\nRF=2, despite having an extra replica, entails many of the same problems RF=1 does. When M3DB is configured to perform quorum writes and reads (the default), as soon as a single node is down (for planned maintenance or an unplanned disruption) clients will be unable to read or write (as the quorum of 2 nodes is 2). Even if clients relax their consistency guarantees and read from the remaining serving node, users may experience flapping results depending on whether one node had data for a time window that the other did not.\nFinally, it is only recommended to run with an odd number of replicas. Because the quorum size of an even-RF N is (N/2)+1, any cluster with an even replica factor N has the same failure tolerance as a cluster with RF=N-1. \u0026ldquo;Failure tolerance\u0026rdquo; is defined as the number of isolation groups you can concurrently lose nodes across. The following table demonstrates the quorum size and failure tolerance of various RF\u0026rsquo;s, inspired by etcd\u0026rsquo;s failure tolerance documentation.\n   Replica Factor Quorum Size Failure Tolerance     1 1 0   2 2 0   3 2 1   4 3 1   5 3 2   6 4 2   7 4 3    Upgrading hosts in a deployment When an M3DB node is restarted it has to perform a bootstrap process before it can serve reads. During this time the node will continue to accept writes, but will not be available for reads.\nObviously, there is also a small window of time during between when the process is stopped and then started again where it will also be unavailable for writes.\nDeployment across multiple availability zones in a region For deployment in a region, it is recommended to set the isolationGroup host attribute to the name of the availability zone a host is in.\nIn this configuration, shards are distributed among hosts such that each will not be placed more than once in the same availability zone. This allows an entire availability zone to be lost at any given time, as it is guaranteed to only affect one replica of data.\nFor example, in a multi-zone deployment with four shards spread over three availability zones:\nTypically, deployments have many more than four shards - this is a simple example that illustrates how M3DB maintains availability while losing an availability zone, as two of three replicas are still intact.\nDeployment in a single zone For deployment in a single zone, it is recommended to set the isolationGroup host attribute to the name of the rack a host is in or another logical unit that separates groups of hosts in your zone.\nIn this configuration, shards are distributed among hosts such that each will not be placed more than once in the same defined rack or logical unit. This allows an entire unit to be lost at any given time, as it is guaranteed to only affect one replica of data.\nFor example, in a single-zone deployment with three shards spread over four racks:\nTypically, deployments have many more than three shards - this is a simple example that illustrates how M3DB maintains availability while losing a single rack, as two of three replicas are still intact.\nDeployment across multiple regions For deployment across regions, it is recommended to set the isolationGroup host attribute to the name of the region a host is in.\nAs mentioned previously, latency and bandwidth costs may increase when using clusters that span regions.\nIn this configuration, shards are distributed among hosts such that each will not be placed more than once in the same region. This allows an entire region to be lost at any given time, as it is guaranteed to only affect one replica of data.\nFor example, in a multi-region deployment with four shards spread over five regions:\nTypically, deployments have many more than four shards - this is a simple example that illustrates how M3DB maintains availability while losing up to two regions, as three of five replicas are still intact.\n"
},
{
	"uri": "/m3db/architecture/engine/",
	"title": "Storage Engine",
	"tags": [],
	"description": "",
	"content": "M3DB is a time series database that was primarily designed to be horizontally scalable and able to handle high data throughput.\nTime Series Compression One of M3DB\u0026rsquo;s biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. There are two compression algorithms used in M3DB: M3TSZ and protobuf encoding.\nM3TSZ M3TSZ is used when values are floats. A variant of the streaming time series compression algorithm described in Facebook\u0026rsquo;s Gorilla paper, it achieves a high compression ratio. The compression ratio will vary depending on the workload and configuration, but we found that we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber\u0026rsquo;s production workloads. This was a 40% improvement over standard TSZ, which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.\nProtobuf Encoding For more complex value types, M3DB also supports generic Protobuf messages with a few exceptions. The algorithm takes on a hybrid approach and uses different compression schemes depending on the field types within the Protobuf message.\nDetails on the encoding, marshaling and unmarshaling methods can be read here.\nArchitecture M3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.\nIn-Memory Object Layout  ┌────────────────────────────────────────────────────────────┐ │ Database │ ├────────────────────────────────────────────────────────────┤ │ │ │ ┌────────────────────────────────────────────────────┐ │ │ │ Namespaces │ │ │ ├────────────────────────────────────────────────────┤ │ │ │ │ │ │ │ ┌────────────────────────────────────────────┐ │ │ │ │ │ Shards │ │ │ │ │ ├────────────────────────────────────────────┤ │ │ │ │ │ │ │ │ │ │ │ ┌────────────────────────────────────┐ │ │ │ │ │ │ │ Series │ │ │ │ │ │ │ ├────────────────────────────────────┤ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ┌────────────────────────────┐ │ │ │ │ │ │ │ │ │ Buffer │ │ │ │ │ │ │ │ │ └────────────────────────────┘ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ┌────────────────────────────┐ │ │ │ │ │ │ │ │ │ Cached blocks │ │ │ │ │ │ │ │ │ └────────────────────────────┘ │ │ │ │ │ │ │ │ ... │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └────────────────────────────────────┘ │ │ │ │ │ │ ... │ │ │ │ │ │ │ │ │ │ │ └────────────────────────────────────────────┘ │ │ │ │ ... │ │ │ │ │ │ │ └────────────────────────────────────────────────────┘ │ │ ... │ │ │ └────────────────────────────────────────────────────────────┘  The in-memory portion of M3DB is implemented via a hierarchy of objects:\n A database of which there is only one per M3DB process. The database owns multiple namespaces. A namespace is similar to a table in other databases. Each namespace has a unique name and a set of configuration options, such as data retention and block size (which we will discuss in more detail later). A namespace owns multiple shards. A shard is effectively the same as a \u0026ldquo;virtual shard\u0026rdquo; in Cassandra in that it provides an arbitrary distribution of time series data via a simple hash of the series ID. A shard owns multiple series. A series represents a sequence of time series datapoints. For example, the CPU utilization for a host could be represented as a series with the ID \u0026ldquo;host1.system.cpu.utilization\u0026rdquo; and a vector of (TIMESTAMP, CPU_LEVEL) tuples. Visualizing this example in a graph, there would a single line with time on the x-axis and CPU utilization on the y-axis. A series owns a buffer and any cached blocks. The buffer is where all data that has yet to be written to disk gets stored in memory. This includes both new writes to M3DB and data obtained through bootstrapping. More details on the buffer is explained below. Upon flushing, the buffer creates a block of its data to be persisted to disk. A block represents a stream of compressed time series data for a pre-configured block size, for example, a block could hold data for 6-8PM (block size of two hours). A block can arrive directly into the series only as a result of getting cached after a read request. Since blocks are in a compressed format, individual datapoints cannot be read from it. In other words, in order to read a single datapoint, the entire block up to that datapoint needs to be decompressed beforehand.  Persistent storage While in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy, it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.\nIn addition, with large volumes of data, it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \u0026ldquo;write-once, read-never\u0026rdquo; pattern where less than a few percent of all the data that\u0026rsquo;s stored is ever read. With that type of workload, it\u0026rsquo;s wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.\nM3DB takes a two-pronged approach to persistant storage that involves combining a commit log for disaster recovery with periodic flushing (writing fileset files to disk) for efficient retrieval:\n All writes are persisted to a commit log (the commit log can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commit log is completely uncompressed and exists only to recover unflushed data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request. Periodically (based on the configured block size), all data in the buffer is flushed to disk as immutable fileset files. These files are highly compressed and can be indexed into via their complementary index files. Check out the flushing section to learn more about the background flushing process.  The block size parameter is the most important variable that needs to be tuned for a particular workload. A small block size will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and data will take up more space on disk.\nIf the database is stopped for any reason in between flushes, then when the node is started back up those writes will be recovered by reading the commit log or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than one).\nWhile the fileset files are designed to support efficient data retrieval via the series ID, there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB supports various caching policies which can significantly improve the performance of reads by caching data in memory.\nWrite Path We now have enough context of M3DB\u0026rsquo;s architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the writeBatchRaw endpoint on M3DB\u0026rsquo;s embedded thrift server. The write itself will contain the following information:\n The namespace The series ID (byte blob) The timestamp The value itself  M3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then an encoder in the buffer will encode the datapoint into the compressed stream. If the encoder doesn\u0026rsquo;t exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some additional logic for handling multiple encoders and filesets which is discussed in the buffer section.\nAt the same time, the write will be appended to the commit log, which is periodically compacted via a snapshot process. Details of this is outlined in the commit log page.\nNote: Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured consistency level.\nRead Path A read begins when an M3DB client calls the FetchBatchResult or FetchBlocksRawResult endpoint on M3DB\u0026rsquo;s embedded thrift server. The read request will contain the following information:\n The namespace The series ID (byte blob) The period of time being requested (start and end)  M3DB will consult the database object to check if the namespace exists, and if it does, it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:\n Whether the series exists and if it does, Whether the data exists in the buffer, cached in-memory, on disk, or some combination of all three.  Determining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn\u0026rsquo;t, then M3DB consults in-memory bloom filters(s) for all shard/block start combinations(s) that overlap the query range to determine if the series exists on disk.\nIf the series exists, then for every block that the request spans, M3DB needs to consolidate data from the buffer, in-memory cache, and fileset files (disk).\nLet\u0026rsquo;s imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a block size of 2 hours, i.e. we need to find 3 different blocks.\nIf the current time is 8PM, then the location of the requested blocks might be as follows:\n[2PM - 4PM (fileset file)] - Flushed block that isn't cached [4PM - 6PM (in-memory cache)] - Flushed block that is cached [4PM - 6PM (cold write in active buffer)] - Cold write that hasn't been flushed yet [6PM - 8PM (active buffer)] - Hasn't been flushed yet  Then M3DB will need to consolidate:\n The not-yet-sealed block from the buffer (located inside an internal lookup in the Series object) [6PM - 8PM] The in-memory cached block (also located inside an internal lookup in the Series object). Since there are also cold writes in this block, the cold writes will be consolidated in memory with data found in the cached block before returning. [4PM - 6PM] The block from disk (the block will be retrieved from disk and will then be cached according to the current caching policy) [2PM - 4PM]  Retrieving blocks from the buffer and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:\n Consult the in-memory bloom filter to determine if it\u0026rsquo;s possible the series exists on disk. If the bloom filter returns negative, we are sure that the series isn\u0026rsquo;t there, so return that result. If the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is before the series ID that we\u0026rsquo;re searching for. Review the index_lookup.go file for implementation details. Jump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we\u0026rsquo;re looking for or we get far enough in the index file that it becomes clear that the ID we\u0026rsquo;re looking for doesn\u0026rsquo;t exist (this is possible because the index file is sorted by ID) Jump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.  Once M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or not the client returns a success to the caller for the read is dependent on the configured consistency level.\nNote: Since M3DB nodes return compressed blocks (the M3DB client decompresses them), it\u0026rsquo;s not possible to return \u0026ldquo;partial results\u0026rdquo; for a given block. If any portion of a read request spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.\nBuffer Each series object contains a buffer, which is in charge of handling all data that has yet to be flushed - new writes and bootstrapped data. To accomplish this, it keeps mutable \u0026ldquo;buckets\u0026rdquo; of encoders (for new writes) and immutable blocks (for bootstrapped data). M3TSZ, the database\u0026rsquo;s encoding scheme, is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For metrics workloads this works very well because every subsequent datapoint is almost always after the previous one. However, out of order writes will occasionally be received, for example due to clock skew. When this happens, M3DB will allocate a new encoder for the out of order datapoints. These encoders are contained in a bucket along with any blocks that got bootstrapped.\nUpon a flush (discussed further below), all data within a bucket gets merged and its version gets incremented - the specific version it gets set to depends on the number of times this block has previously been flushed. This bucket versioning allows the buffer to know which data has been flushed so that subsequent flushes will not try to flush it again. It also indicates to the clean up process (also discussed below) that that data can be evicted.\nGiven this complex, concurrent logic, this has been modeled in TLA.\n ┌─────────────────────────┐ │ Buffer │ ├─────────────────────────┤ │ │ │ ┌─────────────────┐ │ │ │ 2-4PM buckets │ │ │ └─────────────────┘ │ │ │ │ ┌─────────────────┐ │ ┌────│───│ 4-6PM buckets │ | │ │ └─────────────────┘ │ │ │ │ │ │ ... │ │ └─────────────────────────┘ │ │ v After flush: ┌─────────────────────┐ ┌─────────────────────┐ │ 4-6PM buckets │ │ 4-6PM buckets │ ├─────────────────────┤ ├─────────────────────┤ │ │ │ │ │ ┌─────────────┐ │ │ ┌─────────────┐ │ │ │ Bucket v0 │\u0026lt;--│--writes │ │ Bucket v3 │ │ │ └─────────────┘ │ │ └─────────────┘ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └─────────────────────┘ └─────────────────────┘ More writes after flush: After clean up: ┌─────────────────────┐ ┌─────────────────────┐ │ 4-6PM buckets │ │ 4-6PM buckets │ ├─────────────────────┤ ├─────────────────────┤ │ │ │ │ │ ┌─────────────┐ │ │ ┌─────────────┐ │ │ │ Bucket v3 │ │ │ │ Bucket v0 │\u0026lt;--│--writes │ └─────────────┘ │ │ └─────────────┘ │ │ │ │ │ │ ┌─────────────┐ │ │ │ │ │ Bucket v0 │\u0026lt;--│--writes │ │ │ └─────────────┘ │ │ │ │ │ │ │ └─────────────────────┘ └─────────────────────┘  Background processes M3DB has a variety of processes that run in the background during normal operation.\nFlushing As discussed in the architecture section, writes are actively buffered / compressed in memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of fileset files to facilitate efficient storage and retrieval.\nThis is where the configurable \u0026ldquo;block size\u0026rdquo; comes into play. The block size is simply a duration of time that dictates how long new writes will be compressed (in a streaming manner) in memory before being flushed to disk. Let\u0026rsquo;s use a block size of two hours as an example.\nIf the block size is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the fileset files will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.\nIf a flush happens for a namespace/shard/series/block for which there is already a fileset, in-memory data will get merged with data on disk from the fileset. The resultant merged data will then be flushed as a separate fileset.\nTicking The ticking process runs continously in the background and is responsible for a variety of tasks:\n Merging all encoders for a given series / block start combination Removing expired / flushed series and blocks from memory Clean up of expired data (fileset/commit log) from the filesystem  Merging all encoders If there are multiple encoders for a block, they need to be merged before flushing the data to disk. To prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.\nRemoving expired / flushed series and blocks from memory Depending on the configured caching policy, the in-memory object layout can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer needed to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.\nClean up of expired data Fileset files can become no longer necessary for two reasons:\n The fileset files for a block that has fallen out of retention A flush occurred for a block that already has a fileset file. The new fileset will be a superset of the existing fileset with any new data that for that block, hence, the existing fileset is no longer required  During the clean up process, these fileset files will get deleted.\nCaveats / Limitations  Currently M3DB does not support deletes. M3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value, it\u0026rsquo;s still required and generally speaking M3DB is optimized for workloads with a well-defined TTL. M3DB does not support either background data repair or Cassandra-style read repairs. Future versions of M3DB will support automatic repairs of data as an ongoing background process. M3DB does not support writing far into the future. Support for this will be added in future.  "
},
{
	"uri": "/m3query/config/annotated_config/",
	"title": "Annotated Config",
	"tags": [],
	"description": "",
	"content": "Please see here for a link to the annotated config.\n"
},
{
	"uri": "/m3db/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "Overview M3DB is written entirely in Go and does not have any required dependencies. For larger deployments, one may use an etcd cluster to manage M3DB cluster membership and topology definition.\nHigh Level Goals Some of the high level goals for the project are defined as:\n Monitoring support: M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware. As such time series that are not read frequently are not kept in memory. Highly configurable: Provide a high level of configuration to support a wide set of use cases and runtime environments. Variable durability: Providing variable durability guarantees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarantees that data was replicated to a quorum of nodes and that the data was durable if desired.  "
},
{
	"uri": "/m3query/architecture/fanout/",
	"title": "Fetching and querying",
	"tags": [],
	"description": "",
	"content": "Fetch fanout Since m3query does not currently have a view into the M3DB index, fanout to multiple clusters is rather complicated. Since not every metric is necessarily in every cluster (as an example, carbon metrics routed to a certain resolution), it is not trivial to determine which namespaces should be queried to return a fully correct set of recorded metrics.\nThe general approach is therefore to attempt to fanout to any namespace which has a complete view of all metrics, for example, Unaggregated, and take that if it fulfills the query range; if not, m3query will attempt to stitch together namespaces with longer retentions to try and build the most complete possible view of stored metrics.\nFor further details, please ask questions on our gitter, and we\u0026rsquo;ll be happy to help!\n"
},
{
	"uri": "/integrations/graphite/",
	"title": "Graphite",
	"tags": [],
	"description": "",
	"content": "This document is a getting started guide to integrating the M3 stack with Graphite.\nOverview M3 supports ingesting Graphite metrics using the Carbon plaintext protocol. We also support a variety of aggregation and storage policies for the ingestion pathway (similar to storage-schemas.conf when using Graphite Carbon) that are documented below. Finally, on the query side, we support the majority of graphite query functions.\nIngestion Setting up the M3 stack to ingest carbon metrics is straightforward. First, make sure you\u0026rsquo;ve followed our other documentation to get m3coordinator and M3DB setup. Also, familiarize yourself with how M3 handles aggregation.\nOnce you have both of those services running properly, modify your m3coordinator configuration to add the following lines and restart it:\ncarbon: ingester: listenAddress: \u0026#34;0.0.0.0:7204\u0026#34; This will enable a line-based TCP carbon ingestion server on the specified port. By default, the server will write all carbon metrics to every aggregated namespace specified in the m3coordinator configuration file and aggregate them using a default strategy of mean (equivalent to Graphite\u0026rsquo;s Average).\nThis default setup makes sense if your carbon metrics are unaggregated, however, if you\u0026rsquo;ve already aggregated your data using something like statsite then you may want to disable M3 aggregation. In that case, you can do something like the following:\ncarbon: ingester: listenAddress: \u0026#34;0.0.0.0:7204\u0026#34; rules: - pattern: .* aggregation: enabled: false policies: - resolution: 1m retention: 48h This replaces M3\u0026rsquo;s default behavior with a single rule which states that all metrics (since .* will match any string) should be written to whichever aggregated M3DB namespace has been configured with a resolution of 1 minute and a retention of 48 hours, bypassing aggregation / downsampling altogether. Note that there must be a configured M3DB namespace with the specified resolution/retention or the coordinator will fail to start.\nIn the situation that you choose to use M3\u0026rsquo;s aggregation functionality, there are a variety of aggregation types you can choose from. For example:\ncarbon: ingester: listenAddress: \u0026#34;0.0.0.0:7204\u0026#34; rules: - pattern: .* aggregation: type: last policies: - resolution: 1m retention: 48h The config above will aggregate ingested carbon metrics into 1 minute tiles, but instead of taking the mean of every datapoint, it will emit the last datapoint that was received within a given tile\u0026rsquo;s window.\nSimilar to Graphite\u0026rsquo;s storage-schemas.conf, M3 carbon ingestion rules are applied in order and only the first pattern that matches is applied. In addition, the rules can be as simple or as complex as you like. For example:\ncarbon: ingester: listenAddress: \u0026#34;0.0.0.0:7204\u0026#34; rules: - pattern: stats.internal.financial-service.* aggregation: type: max policies: - resolution: 1m retention: 4320h - resolution: 10s retention: 24h - pattern: stats.internal.rest-proxy.* aggregation: type: mean policies: - resolution: 10s retention: 2h - pattern: stats.cloud.* aggregation: enabled: false policies: - resolution: 1m retention: 2h - pattern: .* aggregation: type: mean policies: - resolution: 1m retention: 48h Lets break that down.\nThe first rule states that any metric matching the pattern stats.internal.financial-service.* should be aggregated using the max function (meaning the datapoint with the highest value that is received in a given window will be retained) to generate two different tiles, one with 1 minute resolution and another with 10 second resolution which will be written out to M3DB namespaces with 4320 hour and 24 hour retentions respectively.\nThe second rule will aggregate all the metrics coming from our rest-proxy service using a mean type aggregation (all datapoints within a given window will be averaged) to generate 10 second tiles and write them out to an M3DB namespace that stores data for two hours.\nThe third will match any metrics coming from our cloud environment. In this hypoethical example, our cloud metrics are already aggregated using an application like statsite, so instead of aggregating them again, we just write them directly to an M3DB namespace that retains data for two hours. Note that while we\u0026rsquo;re not aggregating the data in M3 here, we still need to provide a resolution so that the ingester can match the storage policy to a known M3DB namespace, as well as so that when we fan out queries to multiple namespaces we know the resolution of the data contained in each namespace.\nFinally, our last rule uses a \u0026ldquo;catch-all\u0026rdquo; pattern to capture any metrics that don\u0026rsquo;t match any of our other rules and aggregate them using the mean function into 1 minute tiles which we store for 48 hours.\nDebug mode If at any time you\u0026rsquo;re not sure which metrics are being matched by which patterns, or want more visibility into how the carbon ingestion rule are being evaluated, modify the config to enable debug mode:\ncarbon: ingester: debug: true listenAddress: \u0026#34;0.0.0.0:7204\u0026#34; This will make the carbon ingestion emit logs for every step that is taking. Note: If your coordinator is ingesting a lot of data, enabling this mode could bring the proccess to a halt due to the I/O overhead, so use this feature cautiously in production environments.\nSupported Aggregation Functions  last min max mean median count sum sumsq stdev p10 p20 p30 p40 p50 p60 p70 p80 p90 p95 p99 p999 p9999  Querying M3 supports the the majority of graphite query functions and can be used to query metrics that were ingested via the ingestion pathway described above.\nGrafana M3Coordinator implements the Graphite source interface, so you can add it as a graphite source in Grafana by following these instructions.\nNote that you\u0026rsquo;ll need to set the URL to: http://\u0026lt;M3_COORDINATOR_HOST_NAME\u0026gt;:7201/api/v1/graphite\nDirect You can query for metrics directly by issuing HTTP GET requests directly against the M3Coordinator /api/v1/graphite/render endpoint which runs on port 7201 by default. For example:\n(export now=$(date +%s) \u0026amp;\u0026amp; curl \u0026#34;localhost:7201/api/v1/graphite/render?target=transformNull(foo.*.baz)\u0026amp;from=$(($now-300))\u0026#34; | jq .) will query for all metrics matching the foo.*.baz pattern, applying the transformNull function, and returning all datapoints for the last 5 minutes.\n"
},
{
	"uri": "/how_to/cluster_hard_way/",
	"title": "M3DB Cluster Deployment, Manually (The Hard Way)",
	"tags": [],
	"description": "",
	"content": "This document lists the manual steps involved in deploying a M3DB cluster. In practice, you\u0026rsquo;d be automating this using Terraform or using Kubernetes rather than doing this by hand; guides for doing so are available under the How-To section.\nPrimer Architecture A quick primer on M3DB architecture. Here’s what a typical deployment looks like:\nA few different things to highlight about the diagram:\nRole Type There are three ‘role types’ for a m3db deployment -\n Coordinator: m3coordinator serves to coordinate reads and writes across all hosts in the cluster. It’s a lightweight process, and does not store any data. This role would typically be run alongside a Prometheus instance, or be baked into a collector agent. Storage Node: m3dbnode processes running on these hosts are the workhorses of the database, they store data; and serve reads and writes. Seed Node: First and foremost, these hosts are storage nodes themselves. In addition to that responsibility, they run an embedded ETCD server. This is to allow the various M3DB processes running across the cluster to reason about the topology/configuration of the cluster in a consistent manner.  Note: In very large deployments, you’d use a dedicated ETCD cluster, and only use M3DB Storage and Coordinator Nodes\nProvisioning Enough background, lets get you going with a real cluster! Provision your host (be it VMs from AWS/GCP/etc) or bare-metal servers in your DC with the latest and greatest flavour of Linux you favor. M3DB works on all popular distributions - Ubuntu/RHEL/CentOS, let us know if you run into issues on another platform and we’ll be happy to assist.\nNetwork If you’re using AWS or GCP it is highly advised to use static IPs so that if you need to replace a host, you don’t have to update your configuration files on all the hosts, you simply decomission the old seed node and provision a new seed node with the same host ID and static IP that the old seed node had. For AWS you can use a Elastic Network Interface on a VPC and for GCP you can simply use an internal static IP address.\nIn this example you will be creating three static IP addresses for the three seed nodes.\nFurther, we assume you have hostnames configured correctly too. i.e. running hostname on a host in the cluster returns the host ID you\u0026rsquo;ll be using when specifying instance host IDs when creating the M3DB cluster placement. E.g. running hostname on a node m3db001 should return it\u0026rsquo;s host ID m3db001.\nIn GCP the name of your instance when you create it will automatically be it\u0026rsquo;s hostname. When you create an instance click \u0026ldquo;Management, disks, networking, SSH keys\u0026rdquo; and under \u0026ldquo;Networking\u0026rdquo; click the default interface and click the \u0026ldquo;Primary internal IP\u0026rdquo; drop down and select \u0026ldquo;Reserve a static internal IP address\u0026rdquo; and give it a name, i.e. m3db001, a description that describes it\u0026rsquo;s a seed node IP address and use \u0026ldquo;Assign automatically\u0026rdquo;.\nIn AWS it might be simpler to just use whatever the hostname you get for the provisioned VM as your host ID when specifying M3DB placement. Either that or use the environment host ID resolver and pass your host ID when launching the database process with an environment variable. You can set to the host ID and specify the environment variable name in config as envVarName: M3DB_HOST_ID if you are using an environment variable named M3DB_HOST_ID.\nRelevant config snippet:\nhostID: resolver: environment envVarName: M3DB_HOST_ID Then start your process with:\nM3DB_HOST_ID=m3db001 m3dbnode -f config.yml Kernel Ensure you review our recommended kernel configuration before running M3DB in production as M3DB may exceed the default limits for some default kernel values.\nConfig files We wouldn’t feel right to call this guide, “The Hard Way” and not require you to change some configs by hand.\nNote: the steps that follow assume you have the following 3 seed nodes - make necessary adjustment if you have more or are using a dedicated ETCD cluster. Example seed nodes:\n m3db001 (Region=us-east1, Zone=us-east1-a, Static IP=10.142.0.1) m3db002 (Region=us-east1, Zone=us-east1-b, Static IP=10.142.0.2) m3db003 (Region=us-east1, Zone=us-east1-c, Static IP=10.142.0.3)  We’re going to start with the M3DB config template and modify it to work for your cluster. Start by downloading the config. Update the config ‘service’ and \u0026lsquo;seedNodes\u0026rsquo; sections to read as follows:\nconfig: service: env: default_env zone: embedded service: m3db cacheDir: /var/lib/m3kv etcdClusters: - zone: embedded endpoints: - 10.142.0.1:2379 - 10.142.0.2:2379 - 10.142.0.3:2379 seedNodes: initialCluster: - hostID: m3db001 endpoint: http://10.142.0.1:2380 - hostID: m3db002 endpoint: http://10.142.0.2:2380 - hostID: m3db003 endpoint: http://10.142.0.3:2380 Start the seed nodes Transfer the config you just crafted to each host in the cluster. And then starting with the seed nodes, start up the m3dbnode process:\nm3dbnode -f \u0026lt;config-name.yml\u0026gt; Note, remember to daemon-ize this using your favourite utility: systemd/init.d/supervisor/etc\nCreate Namespace and Initialize Topology The recommended way to create a namespace and initialize a topology is to use the /api/v1/database/create api. Below is an example.\nNote: In order to create a more custom setup, please refer to the namespace configuration and placement configuration guides, though this is discouraged.\ncurl -X POST http://localhost:7201/api/v1/database/create -d \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;cluster\u0026#34;, \u0026#34;namespaceName\u0026#34;: \u0026#34;1week_namespace\u0026#34;, \u0026#34;retentionTime\u0026#34;: \u0026#34;168h\u0026#34;, \u0026#34;numShards\u0026#34;: \u0026#34;1024\u0026#34;, \u0026#34;replicationFactor\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;hosts\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;m3db001\u0026#34;, \u0026#34;isolationGroup\u0026#34;: \u0026#34;us-east1-a\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;address\u0026#34;: \u0026#34;10.142.0.1\u0026#34;, \u0026#34;port\u0026#34;: 9000 }, { \u0026#34;id\u0026#34;: \u0026#34;m3db002\u0026#34;, \u0026#34;isolationGroup\u0026#34;: \u0026#34;us-east1-b\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;address\u0026#34;: \u0026#34;10.142.0.2\u0026#34;, \u0026#34;port\u0026#34;: 9000 }, { \u0026#34;id\u0026#34;: \u0026#34;m3db003\u0026#34;, \u0026#34;isolationGroup\u0026#34;: \u0026#34;us-east1-c\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;address\u0026#34;: \u0026#34;10.142.0.3\u0026#34;, \u0026#34;port\u0026#34;: 9000 } ] }\u0026#39; Note: Isolation group specifies how the cluster places shards to avoid more than one replica of a shard appearing in the same replica group. As such you must be using at least as many isolation groups as your replication factor. In this example we use the availibity zones us-east1-a, us-east1-b, us-east1-c as our isolation groups which matches our replication factor of 3.\nShortly after, you should see your node complete bootstrapping:\n20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}] 20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000 20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001 20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002 20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003 20:10:13.747763[I] bootstrapping shards for range starting ... ... 20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}] 20:10:13.758001[I] bootstrapped 20:10:14.764771[I] successfully updated topology to 3 hosts If you need to setup multiple namespaces, you can run the above /api/v1/database/create command multiple times with different namespace configurations.\nReplication factor (RF) Recommended is RF3, where each replica is spread across failure domains such as a rack, data center or availability zone. See Replication Factor Recommendations for more specifics.\nShards See placement configuration to determine the appropriate number of shards to specify.\nTest it out Now you can experiment with writing tagged metrics:\ncurl -sS -X POST localhost:9003/writetagged -d \u0026#39;{ \u0026#34;namespace\u0026#34;: \u0026#34;metrics\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new_york\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;endpoint\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/request\u0026#34; } ], \u0026#34;datapoint\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#39;\u0026#34;$(date \u0026#34;+%s\u0026#34;)\u0026#34;\u0026#39;, \u0026#34;value\u0026#34;: 42.123456789 } }\u0026#39; And reading the metrics you\u0026rsquo;ve written:\ncurl -sS -X POST http://localhost:9003/query -d \u0026#39;{ \u0026#34;namespace\u0026#34;: \u0026#34;metrics\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;regexp\u0026#34;: \u0026#34;.*\u0026#34; } }, \u0026#34;rangeStart\u0026#34;: 0, \u0026#34;rangeEnd\u0026#34;: \u0026#39;\u0026#34;$(date \u0026#34;+%s\u0026#34;)\u0026#34;\u0026#39; }\u0026#39; | jq . Integrations Prometheus as a long term storage remote read/write endpoint.\n"
},
{
	"uri": "/operational_guide/monitoring/",
	"title": "Metrics",
	"tags": [],
	"description": "",
	"content": "It is best to use Prometheus to monitor M3DB, M3 Coordinator and M3 Query using the Grafana dashboards.\nLogs Logs are printed to process output in JSON by default for semi-structured log processing.\nTracing M3DB is integrated with opentracing to provide insight into query performance and errors.\nJaeger To enable Jaeger as the tracing backend, set tracing.backend to \u0026ldquo;jaeger\u0026rdquo; (see also our sample local config:\ntracing: backend: jaeger # enables jaeger with default configs jaeger: # optional configuration for jaeger -- see # https://github.com/jaegertracing/jaeger-client-go/blob/master/config/config.go#L37 # for options ... Jaeger can be run locally with docker as described in https://www.jaegertracing.io/docs/1.9/getting-started/.\nThe default configuration will report traces via udp to localhost:6831; using the all-in-one jaeger container, they will be accessible at\nhttp://localhost:16686\nN.B.: for production workloads, you will almost certainly want to use sampler.type=remote with adaptive sampling for Jaeger, as write volumes are likely orders of magnitude higher than read volumes in most timeseries systems.\nLightStep To use LightStep as the tracing backend, set tracing.backend to \u0026quot;lightstep\u0026quot; and configure necessary information for your client under lightstep. Any options exposed in lightstep-tracer-go can be set in config. Any environment variables may be interpolated. For example:\ntracing: serviceName: m3coordinator backend: lightstep lightstep: access_token: ${LIGHTSTEP_ACCESS_TOKEN:\u0026#34;\u0026#34;} collector: scheme: https host: my-satellite-address.domain port: 8181 Alternative backends If you\u0026rsquo;d like additional backends, we\u0026rsquo;d love to support them!\nFile an issue against M3 and we can work with you on how best to add the backend. The first time\u0026rsquo;s going to be a little rough\u0026ndash;opentracing unfortunately doesn\u0026rsquo;t support Go plugins (yet\u0026ndash;see https://github.com/opentracing/opentracing-go/issues/133), and Go\u0026rsquo;s dependency model means that adding dependencies directly will update everything, which isn\u0026rsquo;t ideal for an isolated dependency change. These problems are all solvable though, and we\u0026rsquo;ll work with you to make it happen!\nUse cases Note: all URLs assume a local jaeger setup as described in Jaeger\u0026rsquo;s docs.\nFinding slow queries To find prom queries longer than , filter for minDuration \u0026gt;= \u0026lt;threshold\u0026gt; on operation=\u0026quot;GET /api/v1/query_range\u0026quot;.\nSample query: http://localhost:16686/search?end=1548876672544000\u0026amp;limit=20\u0026amp;lookback=1h\u0026amp;maxDuration\u0026amp;minDuration=1ms\u0026amp;operation=GET%20%2Fapi%2Fv1%2Fquery_range\u0026amp;service=m3query\u0026amp;start=1548873072544000\nFinding queries with errors Search for error=true on operation=\u0026quot;GET /api/v1/query_range\u0026quot; http://localhost:16686/search?operation=GET%20%2Fapi%2Fv1%2Fquery_range\u0026amp;service=m3query\u0026amp;tags=%7B%22error%22%3A%22true%22%7D\nFinding 500 (Internal Server Error) responses Search for http.status_code=500.\nhttp://localhost:16686/search?limit=20\u0026amp;lookback=24h\u0026amp;maxDuration\u0026amp;minDuration\u0026amp;operation=GET%20%2Fapi%2Fv1%2Fquery_range\u0026amp;service=m3query\u0026amp;start=1548802430108000\u0026amp;tags=%7B\u0026quot;http.status_code\u0026quot;%3A\u0026quot;500\u0026quot;%7D\n"
},
{
	"uri": "/overview/motivation/",
	"title": "Motivation",
	"tags": [],
	"description": "",
	"content": "We decided to open source the M3 platform as a scalable remote storage backend for Prometheus and Graphite so that others may attempt to reuse our work and avoid building yet another scalable metrics platform. As documentation for Prometheus states, it is limited by single nodes in its scalability and durability. The M3 platform aims to provide a turnkey, scalable, and configurable multi-tenant store for Prometheus, Graphite and other standard metrics schemas.\n"
},
{
	"uri": "/overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/m3db/architecture/sharding/",
	"title": "Sharding",
	"tags": [],
	"description": "",
	"content": "Timeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default murmur3 is used as the hashing function and 4096 virtual shards are configured.\nBenefits Shards provide a variety of benefits throughout the M3DB stack:\n They make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level. They provide more fine grained lock granularity at the memory level. They inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.  Replication Logical shards are placed per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica’s data is distinct to the racks that locate all other replicas’ data.\nReplication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.\nReplica Each replica has its own assignment of a single logical shard per virtual shard.\nConceptually it can be defined as:\nReplica { id uint32 shards []Shard } Shard state Each shard can be conceptually defined as:\nShard { id uint32 assignments []ShardAssignment } ShardAssignment { host Host state ShardState } enum ShardState { INITIALIZING, AVAILABLE, LEAVING } Shard assignment The assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.\nFor a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard. That means if there is a given shard with a host assigned as LEAVING and another host assigned as INITIALIZING for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica. Currently however only AVAILABLE shards count towards consistency, the work to group the LEAVING and INITIALIZING shards together when calculating a write success/error is not complete, see issue 417.\nIt is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the INITIALIZING state and to transition the state to AVAILABLE once bootstrapped by calling the cluster management APIs when done. Using a compare and set this atomically removes the LEAVING shard still assigned to the node that previously owned it and transitions the shard state on the new node from INITIALIZING state to AVAILABLE.\nNodes will not start serving reads for the new shard until it is AVAILABLE, meaning not until they have bootstrapped data for those shards.\nCluster operations Node add When a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes. The shards assigned to the new node will become INITIALIZING, the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available. The shards that will be removed from the existing nodes are marked as LEAVING.\nNode down A node needs to be explicitly taken out of the cluster. If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns. During this time it will rely on reads from other replicas to continue uninterrupted operation.\nNode remove When a node is removed the shards it owns are assigned to existing nodes in the cluster. Remaining servers discover they are now in possession of shards that are INITIALIZING and need to be bootstrapped and will begin bootstrapping the data using all replicas available.\n"
},
{
	"uri": "/m3db/architecture/consistencylevels/",
	"title": "Consistency Levels",
	"tags": [],
	"description": "",
	"content": "M3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.\nWrite consistency levels  One: Corresponds to a single node succeeding for an operation to succeed. Majority: Corresponds to the majority of nodes succeeding for an operation to succeed. All: Corresponds to all nodes succeeding for an operation to succeed.  Read consistency levels  One: Corresponds to reading from a single node to designate success. UnstrictMajority: Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes. Majority: Corresponds to reading from the majority of nodes to designate success. All: Corresponds to reading from all of the nodes to designate success.  Connect consistency levels Connect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.\n Any: Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed. None: Corresponds to connecting to no nodes for all shards and as such will always succeed. One: Corresponds to connecting to a single node for all shards. Majority: Corresponds to connecting to the majority of nodes for all shards. All: Corresponds to connecting to all of the nodes for all shards.  "
},
{
	"uri": "/m3query/architecture/functions/",
	"title": "Function Processing",
	"tags": [],
	"description": "",
	"content": "Supported Functions    M3QL Prometheus Graphite     abs/absolute abs() absolute(seriesList)   alias [alias]  alias(seriesList, newName)   aliasByTags [tag]  aliasByTags(seriesList, *tags)   aliasByBucket/aliasByHistogramBucket [tag]     anomalies [flags]     asPercent / asPercent(seriesList, total=None, *nodes)   avg/averageSeries [tag] avg() averageSeries(*seriesLists)   changed  changed(seriesList)   constantLine [value]  constantLine(value)   count count() countSeries(*seriesLists)   derivative  derivative(seriesList)   diff - diffSeries(*seriesLists)   divideSeries / divideSeries(dividendSeriesList, divisorSeries)   eq/== [value] == removeBelowValue(seriesList, n)/removeAboveValue(seriesList, n)   ne/!= [value] != removeBelowValue(seriesList, n)/removeAboveValue(seriesList, n)   excludeByTag [tag, pattern]  exclude(seriesList, pattern)   execute/exec [fetch]     fallbackSeries [replacement]  fallbackSeries(seriesList, fallback)   fetch     ge/=\u0026gt; [value] \u0026gt;= removeBelowValue(seriesList, n)   gt/\u0026gt; [value] \u0026gt; removeBelowValue(seriesList, n)   head [limit] topk() highest(seriesList, n=1, func=\u0026lsquo;average\u0026rsquo;)   histogramCDF [idTag, rangeTag, value]     histogramPercentile [idTag, rangeTag, percentileValue]     identity [name]  identity(name)   integral  integral(seriesList)   intersect [tags] and/or    isNonNull  isNonNull(seriesList)   jainCP     keepLastValue  keepLastValue(seriesList, limit=inf)   le/\u0026lt;= [value] \u0026lt;= removeAboveValue(seriesList, n)   logarithm ln() logarithm(seriesList, base=10)   lt/\u0026lt; [value] \u0026lt; removeAboveValue(seriesList, n)   max/maxSeries [tag] max() maxSeries(*seriesLists)   min/minSeries [tag] min() minSeries(*seriesLists)   moving [interval, func] _over_time() movingMax, movingMin, movingMedian, movingAverage, etc.   multiply/multiplySeries [tag] * multiplySeries(*seriesLists)   nonNegativeDerivative [maxValue]  nonNegativeDerivative(seriesList, maxValue=None)   nPercentile [percentile]  nPercentile(seriesList, n)   offset [amount]  offset(seriesList, factor)   percentileOfSeries [n, true/false, tag]  percentileOfSeries(seriesList, n, interpolate=False)   perSecond rate() perSecond(seriesList, maxValue=None)   promHistogramPercentile [percentileValue]     range [tag]  rangeOfSeries(*seriesLists)   removeAbovePercentile [percentile]  removeAbovePercentile(seriesList, n)   removeBelowPercentile [percentile]  removeBelowPercentile(seriesList, n)   removeAboveValue [value]  removeAboveValue(seriesList, n)   removeBelowValue [value]  removeBelowValue(seriesList, n)   removeEmpty  removeEmptySeries(seriesList, xFilesFactor=None)   scale [factor]  scale(seriesList, factor)   scaleToSeconds [seconds]  scaleToSeconds(seriesList, seconds)   setDiff [tags]     showAnomalyThresholds [level, model]     showTags [true/false, tagName(s)]     sort/sortSeries [avg, current, max, stddev, sum] sort() sortBy(seriesList, func=\u0026lsquo;average\u0026rsquo;, reverse=False)   stdev [points, windowTolerance] stddev() stdev(seriesList, points, windowTolerance=0.1)   sqrt/squareRoot sqrt() squareRoot(seriesList)   summarize [interval, func, alignToFrom]  summarize(seriesList, intervalString, func=\u0026lsquo;sum\u0026rsquo;, alignToFrom=False)   sum/sumSeries [tag] sum() sumSeries(*seriesLists)   sustain [duration]     sustainedAbove \u0026amp; sustainedBelow     tail [limit] bottomk() lowest(seriesList, n=1, func=\u0026lsquo;average\u0026rsquo;)   timeshift [duration]  timeShift(seriesList, timeShift, resetEnd=True, alignDST=False)   timestamp timestamp()    transformNull [value]  transformNull(seriesList, default=0, referenceSeries=None)    "
},
{
	"uri": "/integrations/grafana/",
	"title": "Grafana",
	"tags": [],
	"description": "",
	"content": "M3 supports a variety of Grafana integrations.\nPrometheus / Graphite Sources M3Coordinator can function as a datasource for Prometheus as well as Graphite. See the Prometheus integration and Graphite integration documents respectively for more information.\nPre-configured Prometheus Dashboards All M3 applications expose Prometheus metrics on port 7203 by default as described in the Prometheus integration guide, so if you\u0026rsquo;re already monitoring your M3 stack with Prometheus and Grafana you can use our pre-configured dashboards.\nM3DB Prometheus / Grafana dashboard\nM3Coordinator Prometheus / Grafana dashboard: TODO\nAlternatively, you can obtain the JSON for our most up-to-date dashboards from our git repo directly.\n"
},
{
	"uri": "/how_to/kubernetes/",
	"title": "M3DB on Kubernetes",
	"tags": [],
	"description": "",
	"content": "Please note: If possible PLEASE USE THE OPERATOR to deploy to Kubernetes if you can. It is a considerably more streamlined setup.\nThe operator leverages custom resource definitions (CRDs) to automatically handle operations such as managing cluster topology.\nThe guide below provides static manifests to bootstrap a cluster on Kubernetes and should be considered as a guide to running M3 on Kubernetes, if and only if you have significant custom requirements not satisfied by the operator.\nPrerequisites M3DB performs better when it has access to fast disks. Every incoming write is written to a commit log, which at high volumes of writes can be sensitive to spikes in disk latency. Additionally the random seeks into files when loading cold files benefit from lower random read latency.\nBecause of this, the included manifests reference a StorageClass named fast. Manifests are provided to provide such a StorageClass on AWS / Azure / GCP using the respective cloud provider\u0026rsquo;s premium disk class.\nIf you do not already have a StorageClass named fast, create one using one of the provided manifests:\n# AWS EBS (class io1) kubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-aws.yaml # Azure premium LRS kubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-azure.yaml # GCE Persistent SSD kubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-gcp.yaml If you wish to use your cloud provider\u0026rsquo;s default remote disk, or another disk class entirely, you\u0026rsquo;ll have to modify them manifests.\nIf your Kubernetes cluster spans multiple availability zones, it\u0026rsquo;s important to specify a Volume Binding Mode of WaitForFirstConsumer in your StorageClass to delay the binding of the PersistentVolume until the Pod is created.\nKernel Configuration We provide a Kubernetes daemonset that can make setting host-level sysctls easier. Please see the kernel docs for more.\nNote that our default StatefulSet spec will give the M3DB container CAP_SYS_RESOURCE so it may raise its file limits. Uncomment the securityContext on the m3db container in the StatefulSet if running with a Pod Security Policy or similar enforcement mechanism that prevents adding capabilities to containers.\nDeploying Apply the following manifest to create your cluster:\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/bundle.yaml Applying this bundle will create the following resources:\n An m3db Namespace for all M3DB-related resources. A 3-node etcd cluster in the form of a StatefulSet backed by persistent remote SSDs. This cluster stores the DB topology and other runtime configuration data. A 3-node M3DB cluster in the form of a StatefulSet. Headless services for the etcd and m3db StatefulSets to provide stable DNS hostnames per-pod.  Wait until all created pods are listed as ready:\n$ kubectl -n m3db get po NAME READY STATUS RESTARTS AGE etcd-0 1/1 Running 0 22m etcd-1 1/1 Running 0 22m etcd-2 1/1 Running 0 22m m3dbnode-0 1/1 Running 0 22m m3dbnode-1 1/1 Running 0 22m m3dbnode-2 1/1 Running 0 22m You can now proceed to initialize a namespace and placement for the cluster the same as you would for our other how-to guides:\n# Open a local connection to the coordinator service: $ kubectl -n m3db port-forward svc/m3coordinator 7201 Forwarding from 127.0.0.1:7201 -\u0026gt; 7201 Forwarding from [::1]:7201 -\u0026gt; 7201 # Create an initial cluster topology curl -sSf -X POST localhost:7201/api/v1/placement/init -d \u0026#39;{ \u0026#34;num_shards\u0026#34;: 1024, \u0026#34;replication_factor\u0026#34;: 3, \u0026#34;instances\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;m3dbnode-0\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;pod0\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;endpoint\u0026#34;: \u0026#34;m3dbnode-0.m3dbnode:9000\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;m3dbnode-0.m3dbnode\u0026#34;, \u0026#34;port\u0026#34;: 9000 }, { \u0026#34;id\u0026#34;: \u0026#34;m3dbnode-1\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;pod1\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;endpoint\u0026#34;: \u0026#34;m3dbnode-1.m3dbnode:9000\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;m3dbnode-1.m3dbnode\u0026#34;, \u0026#34;port\u0026#34;: 9000 }, { \u0026#34;id\u0026#34;: \u0026#34;m3dbnode-2\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;pod2\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;endpoint\u0026#34;: \u0026#34;m3dbnode-2.m3dbnode:9000\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;m3dbnode-2.m3dbnode\u0026#34;, \u0026#34;port\u0026#34;: 9000 } ] }\u0026#39; # Create a namespace to hold your metrics curl -X POST localhost:7201/api/v1/namespace -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;bootstrapEnabled\u0026#34;: true, \u0026#34;flushEnabled\u0026#34;: true, \u0026#34;writesToCommitLog\u0026#34;: true, \u0026#34;cleanupEnabled\u0026#34;: true, \u0026#34;snapshotEnabled\u0026#34;: true, \u0026#34;repairEnabled\u0026#34;: false, \u0026#34;retentionOptions\u0026#34;: { \u0026#34;retentionPeriodDuration\u0026#34;: \u0026#34;720h\u0026#34;, \u0026#34;blockSizeDuration\u0026#34;: \u0026#34;12h\u0026#34;, \u0026#34;bufferFutureDuration\u0026#34;: \u0026#34;1h\u0026#34;, \u0026#34;bufferPastDuration\u0026#34;: \u0026#34;1h\u0026#34;, \u0026#34;blockDataExpiry\u0026#34;: true, \u0026#34;blockDataExpiryAfterNotAccessPeriodDuration\u0026#34;: \u0026#34;5m\u0026#34; }, \u0026#34;indexOptions\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;blockSizeDuration\u0026#34;: \u0026#34;12h\u0026#34; } } }\u0026#39; Shortly after you should see your nodes finish bootstrapping:\n$ kubectl -n m3db logs -f m3dbnode-0 21:36:54.831698[I] cluster database initializing topology 21:36:54.831732[I] cluster database resolving topology 21:37:22.821740[I] resolving namespaces with namespace watch 21:37:22.821813[I] updating database namespaces [{adds [metrics]} {updates []} {removals []}] 21:37:23.008109[I] node tchannelthrift: listening on 0.0.0.0:9000 21:37:23.008384[I] cluster tchannelthrift: listening on 0.0.0.0:9001 21:37:23.217090[I] node httpjson: listening on 0.0.0.0:9002 21:37:23.217240[I] cluster httpjson: listening on 0.0.0.0:9003 21:37:23.217526[I] bootstrapping shards for range starting [{run bootstrap-data} {bootstrapper filesystem} ... ... 21:37:23.239534[I] bootstrap data fetched now initializing shards with series blocks [{namespace metrics} {numShards 256} {numSeries 0}] 21:37:23.240778[I] bootstrap finished [{namespace metrics} {duration 23.325194ms}] 21:37:23.240856[I] bootstrapped 21:37:29.733025[I] successfully updated topology to 3 hosts You can now write and read metrics using the API on the DB nodes:\n$ kubectl -n m3db port-forward svc/m3dbnode 9003 Forwarding from 127.0.0.1:9003 -\u0026gt; 9003 Forwarding from [::1]:9003 -\u0026gt; 9003 curl -sSf -X POST localhost:9003/writetagged -d \u0026#39;{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new_york\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;endpoint\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/request\u0026#34; } ], \u0026#34;datapoint\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#39;\u0026#34;$(date \u0026#34;+%s\u0026#34;)\u0026#34;\u0026#39;, \u0026#34;value\u0026#34;: 42.123456789 } }\u0026#39; $ curl -sSf -X POST http://localhost:9003/query -d \u0026#39;{ \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;regexp\u0026#34;: \u0026#34;.*\u0026#34; } }, \u0026#34;rangeStart\u0026#34;: 0, \u0026#34;rangeEnd\u0026#34;: \u0026#39;\u0026#34;$(date \u0026#34;+%s\u0026#34;)\u0026#34;\u0026#39; }\u0026#39; | jq . { \u0026#34;results\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;new_york\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;endpoint\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/request\u0026#34; } ], \u0026#34;datapoints\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: 1527630053, \u0026#34;value\u0026#34;: 42.123456789 } ] } ], \u0026#34;exhaustive\u0026#34;: true } Adding nodes You can easily scale your M3DB cluster by scaling the StatefulSet and informing the cluster topology of the change:\nkubectl -n m3db scale --replicas=4 statefulset/m3dbnode Once the pod is ready you can modify the cluster topology:\nkubectl -n m3db port-forward svc/m3coordinator 7201 Forwarding from 127.0.0.1:7201 -\u0026gt; 7201 Forwarding from [::1]:7201 -\u0026gt; 7201 curl -sSf -X POST localhost:7201/api/v1/placement -d \u0026#39;{ \u0026#34;instances\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;m3dbnode-3\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;pod3\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;endpoint\u0026#34;: \u0026#34;m3dbnode-3.m3dbnode:9000\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;m3dbnode-3.m3dbnode\u0026#34;, \u0026#34;port\u0026#34;: 9000 } ] }\u0026#39; Integrations Prometheus As mentioned in our integrations guide, M3DB can be used as a remote read/write endpoint for Prometheus.\nIf you run Prometheus on your Kubernetes cluster you can easily point it at M3DB in your Prometheus server config:\nremote_read: - url: \u0026#34;http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/read\u0026#34; # To test reading even when local Prometheus has the data read_recent: true remote_write: - url: \u0026#34;http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/write\u0026#34; # To differentiate between local and remote storage we will add a storage label write_relabel_configs: - target_label: metrics_storage replacement: m3db_remote Scheduling In some cases, you might prefer M3DB to run on certain nodes in your cluster. For example: if your cluster is comprised of different instance types and some have more memory than others then you\u0026rsquo;d like M3DB to run on those nodes if possible. To accommodate this, the pods created by the StatefulSets use pod affinities and tolerations to prefer to run on certain nodes. Specifically:\n The pods tolerate the taint \u0026quot;dedicated-m3db\u0026quot; to run on nodes that are specifically dedicated to m3db if you so choose. Via nodeAffinity the pods prefer to run on nodes with the label m3db.io/dedicated-m3db=\u0026quot;true\u0026quot;.  "
},
{
	"uri": "/m3db/",
	"title": "M3DB, a distributed time series database",
	"tags": [],
	"description": "",
	"content": "About M3DB, inspired by Gorilla and Cassandra, is a distributed time series database released as open source by Uber Technologies. It can be used for storing realtime metrics at long retention.\nHere are some attributes of the project:\n Distributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently Cluster management built on top of etcd Built-in synchronous replication with configurable durability and read consistency (one, majority, all, etc) M3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy Arbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write Configurable out of order writes, currently limited to the size of the configured time window\u0026rsquo;s block size  Current Limitations Due to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.\nThe project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours). As such out of order writes are limited to the size of a single compressed time series window. Consequently backfilling large amounts of data is not currently possible.\nThe project has also optimized the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.\n"
},
{
	"uri": "/overview/media/",
	"title": "Media",
	"tags": [],
	"description": "",
	"content": "Blogs   M3: Uber’s Open Source, Large-scale Metrics Platform for Prometheus By Rob Skillington - Aug 7, 2018.\n  Building a Query Engine for High Cardinality Time Series Data By Nikunj Aggarwal and Ben Raskin - Dec 10, 2018.\n  M3 Community Meetups Recordings of all past meetups can be found on a Vimeo M3 Community Meetings folder.\n  June 2020 Meetup.\n  July 2020 Meetup and LinkedIn presentation.\n  August 2020 Meetup and Walmart presentation.\n  Recorded Talks   OpenObservability Talks: Long-Term Metrics with M3 and Prometheus By Matt Schallert. Aug 27, 2020.\n  CNCF Webinar: Maximizing M3 – Pushing performance boundaries in a distributed metrics engine By Ryan Allen - Aug 6, 2020.\n  OSCON 2019: Large-Scale Automated Storage on Kubernetes By Matt Schallert - Jul 18, 2019. Slides\n  How to get the 30,000 ft view, 1 ft view and everything in between without breaking the bank By Martin Mao - June 5, 2019. Slides\n  M3 and Prometheus, Monitoring at Planet Scale for Everyone By Rob Skillington - May 22, 2019. Video\n  Building Operators at Uber By Matt Schallert \u0026amp; Paul Schooss - Mar 11, 2019.\n  M3 and a new age of metrics and monitoring in an increasingly complex world By Rob Skillington - Feb 3, 2019.\n  KubeCon Seattle 2018 Keynote: Smooth Operator♪: Large Scale Automated Storage with Kubernetes By Celina Ward \u0026amp; Matt Schallert - Dec 13, 2018.\n  How to query millions of time series efficiently By Martin Mao - Dec 10, 2018. Slides\n  Learnings, patterns and Uber’s metrics platform M3, open sourced as a Prometheus long term storage backend By Rob Skillington - Nov 5, 2018. Slides\n  Adventures in building a high-volume Time-Series Database By Richard Artoul \u0026amp; Prateek Rungta - Nov 4, 2018.\n  PromCon 2018 Lightning Talk: M3 with Prometheus by Nikunj Aggarwal - Aug 9, 2018.\n  PromCon 2018 Panel Discussion: Prometheus Long-Term Storage Approaches including highlights of the M3 stack by Nikunj Aggarwal - Aug 9, 2018.\n  Putting billions of time series to work at Uber with autonomous monitoring By Prateek Rungta - Jun 6, 2018. Slides\n  "
},
{
	"uri": "/operational_guide/upgrading_m3/",
	"title": "Upgrading M3",
	"tags": [],
	"description": "",
	"content": "Overview This guide explains how to upgrade M3 from one version to another (e.g. from 0.14.0 to 0.15.0). This includes upgrading:\n m3dbnode m3coordinator m3query m3aggregator  m3dbnode Graphs to monitor While upgrading M3DB nodes, it\u0026rsquo;s important to monitor the status of bootstrapping the individual nodes. This can be monitored using the M3DB Node Details dashboard. Typically, the Bootstrapped graph under Background Tasks and the graphs within the CPU and Memory Utilization give a good understanding of how well bootstrapping is going.\nKubernetes If running M3DB on Kubernetes, upgrade by completing the following steps.\n  Identify the version of m3dbnode to upgrade to on Quay.\n  Replace the Docker image in the StatefulSet manifest (or m3db-operator manifest) to be the new version of m3dbnode.\n  spec: image: quay.io/m3db/m3dbnode:$VERSION Once updated, apply the updated manifest and a rolling restart will be performed. You must wait until the StatefulSet is entirely upgraded and bootstrapped (as per the M3DB Node Details dashboard) before proceeding to the next StatefulSet otherwise multiple replicas will be unavailable at once.  kubectl apply -f \u0026lt;m3dbnode_manifest\u0026gt; Downgrading The upgrading steps above can also be used to downgrade M3DB. However, it is important to refer to the release notes to make sure that versions are backwards compatible.\nm3coordinator m3coordinator can be upgraded using similar steps as m3dbnode, however, the images can be found here instead.\nm3query m3query can be upgraded using similar steps as m3dbnode, however, the images can be found here instead.\nm3aggregator m3aggregator can be upgraded using similar steps as m3dbnode, however, the images can be found here instead.\nNon-Kubernetes It is very important that for each replica set, only one node gets upgraded at a time. However, multiple nodes can be upgraded across replica sets.\n Download new binary (linux example below).  wget \u0026#34;https://github.com/m3db/m3/releases/download/v$VERSION/m3_$VERSION_linux_amd64.tar.gz\u0026#34; \u0026amp;\u0026amp; tar xvzf m3_$VERSION_linux_amd64.tar.gz \u0026amp;\u0026amp; rm m3_$VERSION_linux_amd64.tar.gz Stop and upgrade one M3DB node at a time per replica set using the systemd unit.  # stop m3dbnode sudo systemctl stop m3dbnode # start m3dbnode with the new binary (which should be placed in the path specified in the systemd unit) sudo systemctl start m3dbnode Note: If unable to stop m3dbnode using systemctl, use pkill instead.\n# stop m3dbnode pkill m3dbnode # start m3dbnode with new binary ./m3_$VERSION_linux_amd64/m3dbnode -f \u0026lt;config-name.yml\u0026gt; Confirm m3dbnode has finished bootstrapping.  20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}] 20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000 20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001 20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002 20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003 20:10:13.747763[I] bootstrapping shards for range starting ... ... 20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}] 20:10:13.758001[I] bootstrapped 20:10:14.764771[I] successfully updated topology to 3 hosts Repeat steps 2 and 3 until all nodes have been upgraded.  "
},
{
	"uri": "/style-guide/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "MarkdownHTMLJSONThis is some markdown.\ncode HTML Include code from elsewhere. { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;PodTemplate\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34; }, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34; }, \u0026#34;generateName\u0026#34;: \u0026#34;nginx-\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;dockerfile/nginx\u0026#34;, \u0026#34;ports\u0026#34;: [{\u0026#34;containerPort\u0026#34;: 80}] }] } } }  $(function(){$(\"#tab_with_md\").tabs();}); m3I am a test term.  func GetTitleFunc(style string) func(s string) string { switch strings.ToLower(style) {  case \u0026#34;go\u0026#34;: return strings.Title case \u0026#34;chicago\u0026#34;: return transform.NewTitleConverter(transform.ChicagoStyle) default: return transform.NewTitleConverter(transform.APStyle) } "
},
{
	"uri": "/integrations/influx/",
	"title": "InfluxDB",
	"tags": [],
	"description": "",
	"content": "This document is a getting started guide to integrating InfluxDB data pipelines with M3.\nWriting metrics using InfluxDB line protocol To write metrics to M3 using the InfluxDB line protocol, simply form the request as you typically would line separated and POST the body to /api/v1/influxdb/write on the coordinator. Note that timestamp is in nanoseconds from Unix epoch.\nThis example writes two metrics weather_temperature and weather_wind using the current time in nanoseconds as the timestamp:\ncurl -i -X POST \u0026#34;http://localhost:7201/api/v1/influxdb/write\u0026#34; --data-binary \u0026#34;weather,location=us-midwest temperature=82,wind=42 $(expr $(date +%s) \\* 1000000000)\u0026#34; Querying for metrics After successfully written you can query for these metrics using PromQL. All measurements are translated into metric names by concatenating the name with the measurement name.\nThe previous example forms the two following Prometheus time series:\nweather_temperature{location=\u0026quot;us-midwest\u0026quot;} 82 weather_wind{location=\u0026quot;us-midwest\u0026quot;} 42 All metric names and labels are rewritten to contain only alphanumeric characters. Any non-alphanumeric characters are rewritten with an underscore.\n"
},
{
	"uri": "/m3coordinator/",
	"title": "M3 Coordinator, API for reading/writing metrics and management",
	"tags": [],
	"description": "",
	"content": "M3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and downstream systems, such as M3DB.\nIt also provides management APIs to setup and configure different parts of M3.\nThe coordinator is generally a bridge for read and writing different types of metrics formats and a management layer for M3.\nNote: M3DB by default includes the M3 Coordinator accessible on port 7201. For production deployments it is recommended to deploy it as a dedicated service to ensure you can scale the write coordination role separately and independently to database nodes as an isolated application separate from the M3DB database role.\n"
},
{
	"uri": "/operational_guide/resource_limits/",
	"title": "Resource Limits and Preventing Abusive Reads/Writes",
	"tags": [],
	"description": "",
	"content": "This operational guide provides an overview of how to set resource limits on M3 components to prevent abusive reads/writes impacting availability or performance of M3 in a production environment.\nM3DB Configuring limits The best way to get started protecting M3DB nodes is to set a few limits on the top level limits config stanza for M3DB.\nWhen using M3DB for metrics workloads, queries arrive as a set of matchers that select time series based on certain dimensions. The primary mechanism to protect against these matchers matching huge amounts of data in an unbounded way is to set a maximum limit for the amount of time series blocks allowed to be matched and consequently read in a given time window. This can be done using maxRecentlyQueriedSeriesBlocks to set a maximum value and lookback time window to determine the duration over which the max limit is enforced.\nYou can use the Prometheus query rate(query_stats_total_docs_per_block[1m]) to determine how many time series blocks are queried per second by your cluster today to determine what is a sane value to set this to. Make sure to multiply that number by the lookback period to get your desired max value. For instance, if the query shows that you frequently query 10,000 time series blocks per second safely with your deployment and you want to use the default lookback of 5s then you would multiply 10,000 by 5 to get 50,000 as a max value with a 5s lookback.\nAnnotated configuration limits: # If set, will enforce a maximum cap on time series blocks matched for # queries searching time series by dimensions. maxRecentlyQueriedSeriesBlocks: # Value sets the maximum time series blocks matched, use your block  # settings to understand how many datapoints that may actually translate  # to (e.g. 2 hour blocks for unaggregated data with 30s scrape interval # will translate to 240 datapoints per single time series block matched). value: 0 # Lookback sets the time window that this limit is enforced over, every  # lookback period the global count is reset to zero and when the limit  # is reached it will reject any further time series blocks being matched  # and read until the lookback period resets. lookback: 5s # If set then will limit the number of parallel write batch requests to the  # database and return errors if hit. maxOutstandingWriteRequests: 0 # If set then will limit the number of parallel read requests to the  # database and return errors if hit.  # Note since reads can be so variable in terms of how expensive they are # it is not always very useful to use this config to prevent resource  # exhaustion from reads. maxOutstandingReadRequests: 0 M3 Query and M3 Coordinator Deployment Protecting queries impacting your ingestion of metrics for metrics workloads can first and foremost be done by deploying M3 Query and M3 Coordinator independently. That is, for writes to M3 use a dedicated deployment of M3 Coordinator instances, and then for queries to M3 use a dedicated deployment of M3 Query instances.\nThis ensures when M3 Query instances become busy and are starved of resources serving an unexpected query load, they will not interrupt the flow of metrics being ingested to M3.\nConfiguring limits To protect against individual queries using too many resources, you can specify some sane limits in the M3 Query (and consequently M3 Coordinator) configuration file under the top level limits config stanza.\nThere are two types of limits:\n Per query time series limit Per query time series * blocks limit (docs limit)  When either of these limits are hit, you can define the behavior you would like, either to return an error when this limit is hit, or to return a partial result with the response header M3-Results-Limited detailing the limit that was hit and a warning included in the response body.\nAnnotated configuration limits: # If set will override default limits set per query. perQuery: # If set limits the number of time series returned for any given  # individual storage node per query, before returning result to query  # service. maxFetchedSeries: 0 # If set limits the number of index documents matched for any given  # individual storage node per query, before returning result to query  # service. # This equates to the number of time series * number of blocks, so for  # 100 time series matching 4 hours of data for a namespace using a 2 hour  # block size, that would result in matching 200 index documents. maxFetchedDocs: 0 # If true this results in causing a query error if the query exceeds  # the series or blocks limit for any given individual storage node per query. requireExhaustive: false # If set this limits the max number of datapoints allowed to be used by a # given query. This is applied at the query service after the result has  # been returned by a storage node. maxFetchedDatapoints: 0 # If set will override default limits set globally. global: # If set this limits the max number of datapoints allowed to be used by all # queries at any point in time, this is applied at the query service after  # the result has been returned by a storage node. maxFetchedDatapoints: 0 Headers The following headers can also be used to override configured limits on a per request basis (to allow for different limits dependent on caller):\n M3-Limit-Max-Series:\nIf this header is set it will override any configured per query time series limit. If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.\n M3-Limit-Max-Docs:\nIf this header is set it will override any configured per query time series * blocks limit (docs limit). If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.\n M3-Limit-Require-Exhaustive:\nIf this header is set it will override any configured require exhaustive setting. If \u0026ldquo;true\u0026rdquo; it will return an error if query hits a configured limit (such as series or docs limit) instead of a partial result. Otherwise if \u0026ldquo;false\u0026rdquo; it will return a partial result of the time series already matched with the response header M3-Results-Limited detailing the limit that was hit and a warning included in the response body.\n  "
},
{
	"uri": "/overview/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": "This roadmap is open for suggestions and currently just a small snapshot of what is coming up.\nShort:\n Add diagrams of what using M3 looks like (broken down by use case) Improve operational guides for the aggregator Add tutorials for a variety of use cases Add design documentation of reverse index Add design documentation of aggregator  Medium:\n Plan what a v1.0 release looks like  "
},
{
	"uri": "/how_to/query/",
	"title": "Setting up m3query",
	"tags": [],
	"description": "",
	"content": "m3query is used to query data that is stored in M3DB. For instance, if you are using the Prometheus remote write endpoint with m3coordinator, you can use m3query instead of the Prometheus remote read endpoint. By doing so, you get all of the benefits of m3query\u0026rsquo;s engine such as block processing. Furthermore, since m3query provides a Prometheus compatible API, you can use 3rd party graphing and alerting solutions like Grafana.\nConfiguration Before setting up m3query, make sure that you have at least one M3DB node running. In order to start m3query, you need to configure a yaml file, that will be used to connect to M3DB. Here is a link to a sample config file that is used for an embedded etcd cluster within M3DB.\nRunning You can run m3query by either building and running the binary yourself:\nmake m3query ./bin/m3query -f ./src/query/config/m3query-local-etcd.yml Or you can run it with Docker using the Docker file located at $GOPATH/src/github.com/m3db/m3/docker/m3query/Dockerfile.\nNamespaces All namespaces that you wish to query from must be configured when setting up M3DB. If you wish to add or change an existing namespace, please follow the namespace operational guide here.\netcd The configuration file linked above uses an embedded etcd cluster, which is fine for development purposes. However, if you wish to use this in production, you will want an external etcd cluster.\nAggregation You will notice that in the setup linked above, M3DB has just one unaggregated namespace configured. If you want aggregated metrics, you will need to set up an aggregated namespace in M3DB and in the m3query configuration. It is important to note that all writes go to all namespaces so as long as you include all namespaces in your query config, you will be querying all namespaces. Aggregation is done strictly by the query service. For example if you have an aggregated namespace setup in M3DB named metrics_10s_48h, you can add the following to the query config:\n- namespace: metrics_10s_48h type: aggregated retention: 48h resolution: 10s Disabling automatic aggregation If you run Statsite, m3agg, or some other aggregation tier, you will want to set the all flag under downsample to false. Otherwise, you will be aggregating metrics that have already been aggregated.\n- namespace: metrics_10s_48h type: aggregated retention: 48h resolution: 10s downsample: all: false ID generation The default generation scheme for IDs, legacy, is unfortunately prone to collisions, but remains the default for backwards compatibility reasons. It is suggested to set the ID generation scheme to one of either quoted or prepend_meta. quoted generation scheme yields the most human-readable IDs, whereas prepend_meta is better for more compact IDs, or if tags are expected to contain non-ASCII characters. To set the ID generation scheme, add the following to your m3coordinator configuration yaml file:\ntagOptions: idScheme: \u0026lt;name\u0026gt; As an example of how these schemes generate IDs, consider a series with the following 4 tags, [{\u0026quot;t1\u0026quot;:v1}, {t2:\u0026quot;v2\u0026quot;}, {t3:v3}, {t4:v4}]. The following is an example of how different schemes will generate IDs.\nlegacy: \u0026#34;t1\u0026#34;=v1,t2=\u0026#34;v2\u0026#34;,t3=v3,t4=v4, prepend_meta: 4,2,2,4,2,2,2,2!\u0026#34;t1\u0026#34;v1t2\u0026#34;v2\u0026#34;t3v3t4v4 quoted: {\\\u0026#34;t1\\\u0026#34;=\u0026#34;v1\u0026#34;,t2=\u0026#34;\\\u0026#34;v2\\\u0026#34;\u0026#34;,t3=\u0026#34;v3\u0026#34;,t4=\u0026#34;v4\u0026#34;} If there is a chance that your metric tags will contain \u0026ldquo;control\u0026rdquo; characters, specifically , and =, it is highly recommended that one of either the quoted or prepend_meta schemes are specified, as the legacy scheme may cause ID collisions. As a general guideline, we suggest quoted, as it mirrors the more familiar Prometheus style IDs.\nWe technically have a fourth ID generation scheme that is used for Graphite IDs, but it is exclusive to the Graphite ingestion path and is not selectable as a general scheme.\nWARNING: Once a scheme is selected, be very careful about changing it. If changed, all incoming metrics will resolve to a new ID, effectively doubling the metric cardinality until all of the older-style metric IDs fall out of retention.\nMigration We recently updated our ID generation scheme in m3coordinator to avoid the collision issues discussed above. To ease migration, we\u0026rsquo;re temporarily enforcing that an ID generation scheme be explicitly provided in the m3coordinator configuration files.\nIf you have been running m3query or m3coordinator already, you may want to counterintuitively select the collision-prone legacy scheme, as all the IDs for all of your current metrics would have already been generated with this scheme, and choosing another will effectively double your index size. If the twofold increase in cardinality is an acceptable increase (and unfortunately, this is likely to mean doubled cardinality until your longest retention cluster rotates out), it\u0026rsquo;s suggested to choose a collision-resistant scheme instead.\nAn example of a configuration file for a standalone m3query instance with the ID generation scheme can be found here. If you\u0026rsquo;re running m3query or m3coordinator embedded, these configuration options should be nested under the coordinator: heading, as seen here.\nIf none of these options work for you, or you would like further clarification, please stop by our Slack and we\u0026rsquo;ll be happy to help you.\nGrafana You can also set up m3query as a datasource in Grafana. To do this, add a new datasource with a type of Prometheus. The URL should point to the host/port running m3query. By default, m3query runs on port 7201.\n"
},
{
	"uri": "/m3db/architecture/storage/",
	"title": "Storage",
	"tags": [],
	"description": "",
	"content": "Overview The primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.\nThey are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to. If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)\nFileSets A fileset has the following files:\n Info file: Stores the block time window start and size and other important metadata about the fileset volume. Summaries file: Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up. Index file: Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval. Data file: Stores the series compressed data streams. Bloom filter file: Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume. Digests file: Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification. Checkpoint file: Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.   ┌───────────────────────┐ ┌─────────────────────┐ ┌─────────────────────┐ │ Index File │ │ Info File │ │ Summaries File │ │ (sorted by ID) │ ├─────────────────────┤ │ (sorted by ID) │ ├───────────────────────┤ │- Block Start │ ├─────────────────────┤ ┌─\u0026gt;│- Idx │ │- Block Size │ │- Idx │ │ │- ID │ │- Entries (Num) │ │- ID │ │ │- Size │ │- Major Version │ │- Index Entry Offset ├──┘ │- Checksum │ │- Summaries (Num) │ └─────────────────────┘ │- Data Entry Offset ├──┐ │- BloomFilter (K/M) │ │- Encoded Tags │ │ │- Snapshot Time │ │- Index Entry Checksum │ │ │- Type (Flush/Snap) │ └───────────────────────┘ │ │- Snapshot ID │ │ │- Volume Index │ │ │- Minor Version │ │ └─────────────────────┘ │ │ ┌─────────────────────┐ ┌─────────────────────────────┘ ┌─────────────────────┐ │ Bloom Filter File │ │ │ Digests File │ ├─────────────────────┤ │ ┌─────────────────────┐ ├─────────────────────┤ │- Bitset │ │ │ Data File │ │- Info file digest │ └─────────────────────┘ │ ├─────────────────────┤ │- Summaries digest │ │ │List of: │ │- Index digest │ └─\u0026gt;│ - Marker (16 bytes)│ │- Data digest │ │ - ID │ │- Bloom filter digest│ │ - Data (size bytes)│ └─────────────────────┘ └─────────────────────┘ ┌─────────────────────┐ │ Checkpoint File │ ├─────────────────────┤ │- Digests digest │ └─────────────────────┘ In the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.\nFileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.\n"
},
{
	"uri": "/m3db/architecture/commitlogs/",
	"title": "Commit Logs And Snapshot Files",
	"tags": [],
	"description": "",
	"content": "Overview M3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)\nIntegrity Levels There are two integrity levels available for commit logs:\n Synchronous: write operations must wait until it has finished writing an entry in the commit log to complete. Behind: write operations must finish enqueueing an entry to the commit log write queue to complete.  Depending on the data loss requirements users can choose either integrity level.\nProperties Commit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.\nStructure Commit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it\u0026rsquo;s the first time a log entry for a given series appears.\nThe structures can be conceptually described as:\nCommitLogInfo { start int64 duration int64 index int64 } CommitLog { created int64 index uint64 metadata bytes timestamp int64 value float64 unit uint32 annotation bytes } CommitLogMetadata { id bytes namespace bytes shard uint32 } Compaction / Snapshotting Commit log files are compacted via the snapshotting proccess which (if enabled at the namespace level) will snapshot all data in memory into compressed files which have the same structure as the fileset files but are stored in a different location. Once these snapshot files are created, then all the commit log files whose data are captured by the snapshot files can be deleted. This can result in significant disk savings for M3DB nodes running with large block sizes and high write volume where the size of the (uncompressed) commit logs can quickly get out of hand.\nIn addition, since the snapshot files are already compressed, bootstrapping from them is much faster than bootstrapping from raw commit log files because the individual data points don\u0026rsquo;t need to be decoded and then M3TSZ encoded. The M3DB node just needs to read the raw bytes off disk and load them into memory.\nCleanup Commit log files are automatically deleted once all the data they contain has been flushed to disk as immutable compressed filesets or all the data they contain has been captured by a compressed snapshot file. Similarly, snapshot files are deleted once all the data they contain has been flushed to disk as filesets.\n"
},
{
	"uri": "/m3query/",
	"title": "M3 Query, a statelees query server for M3DB and Prometheus",
	"tags": [],
	"description": "",
	"content": "M3 Query is a service that exposes all metrics query endpoints along with metrics time series metadata APIs that return dimensions and labels of metrics that reside in a M3DB cluster.\nNote: M3 Coordinator, and by proxy M3DB, by default includes the M3 Query endpoints accessible on port 7201. For production deployments it is recommended to deploy it as a dedicated service to ensure you can scale the memory heavy query role separately from the metrics ingestion write path of writes through M3 Coordinator to M3DB database role nodes. This allows excessive queries to primarily affect the dedicated M3 Query service instead of interrupting service to the write ingestion pipeline.\n"
},
{
	"uri": "/how_to/aggregator/",
	"title": "Setting up M3Aggregator",
	"tags": [],
	"description": "",
	"content": "Introduction m3aggregator is used to cluster stateful downsampling and rollup of metrics before they are store in M3DB. The M3 Coordinator also performs this role but is not cluster aware. This means metrics will not get aggregated properly if you send metrics in round robin fashion to multiple M3 Coordinators for the same metrics ingestion source (e.g. Prometheus server).\nSimilar to M3DB, m3aggregator supports clustering and replication by default. This means that metrics are correctly routed to the instance(s) responsible for aggregating each metric and multiple m3aggregator replicas can be configured such that there are no single points of failure for aggregation.\nConfiguration Before setting up m3aggregator, make sure that you have at least one M3DB node running and a dedicated m3coordinator setup.\nWe highly recommend running with at least a replication factor 2 for a m3aggregator deployment. If you run with replication factor 1 then when you restart an aggregator it will temporarily interrupt good the stream of aggregated metrics and there will be some data loss.\nTopology Initializing aggregator topology You can setup a m3aggregator topology by issuing a request to your coordinator (be sure to use your own hostnames, number of shards and replication factor):\ncurl -vvvsSf -H \u0026#34;Cluster-Environment-Name: namespace/m3db-cluster-name\u0026#34; -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/services/m3aggregator/placement/init -d \u0026#39;{ \u0026#34;num_shards\u0026#34;: 64, \u0026#34;replication_factor\u0026#34;: 2, \u0026#34;instances\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;m3aggregator01:6000\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;availability-zone-a\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;endpoint\u0026#34;: \u0026#34;m3aggregator01:6000\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;m3aggregator01\u0026#34;, \u0026#34;port\u0026#34;: 6000 }, { \u0026#34;id\u0026#34;: \u0026#34;m3aggregator02:6000\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;availability-zone-b\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;weight\u0026#34;: 100, \u0026#34;endpoint\u0026#34;: \u0026#34;m3aggregator02:6000\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;m3aggregator02\u0026#34;, \u0026#34;port\u0026#34;: 6000 } ] }\u0026#39; Initializing m3msg topic for m3aggregator to receive from m3coordinators to aggregate metrics Now we must setup a topic for the m3aggregator to receive unaggregated metrics from m3coordinator instances:\ncurl -vvvsSf -H \u0026#34;Cluster-Environment-Name: namespace/m3db-cluster-name\u0026#34; -H \u0026#34;Topic-Name: aggregator_ingest\u0026#34; -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic/init -d \u0026#39;{ \u0026#34;numberOfShards\u0026#34;: 64 }\u0026#39; Add m3aggregagtor consumer group to ingest topic Add the m3aggregator placement to receive traffic from the topic (make sure to set message TTL to match your desired maximum in memory retry message buffer):\ncurl -vvvsSf -H \u0026#34;Cluster-Environment-Name: namespace/m3db-cluster-name\u0026#34; -H \u0026#34;Topic-Name: aggregator_ingest\u0026#34; -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic -d \u0026#39;{ \u0026#34;consumerService\u0026#34;: { \u0026#34;serviceId\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;m3aggregator\u0026#34;, \u0026#34;environment\u0026#34;: \u0026#34;namespace/m3db-cluster-name\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34; }, \u0026#34;consumptionType\u0026#34;: \u0026#34;REPLICATED\u0026#34;, \u0026#34;messageTtlNanos\u0026#34;: \u0026#34;300000000000\u0026#34; } }\u0026#39; Note: 300000000000 nanoseconds is a TTL of 5 minutes for messages to rebuffer for retry.\nInitializing m3msg topic for m3coordinator to receive from m3aggregator to write to M3DB Now we must setup a topic for the m3coordinator to receive aggregated metrics from m3aggregator instances to write to M3DB:\ncurl -vvvsSf -H \u0026#34;Cluster-Environment-Name: namespace/m3db-cluster-name\u0026#34; -H \u0026#34;Topic-Name: aggregated_metrics\u0026#34; -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic/init -d \u0026#39;{ \u0026#34;numberOfShards\u0026#34;: 64 }\u0026#39; Initializing m3coordinator topology Then m3coordinator instances need to be configured to receive traffic for this topic (note ingest at port 7507 must match the configured port for your m3coordinator ingest server, see config at bottom of this guide):\ncurl -vvvsSf -H \u0026#34;Cluster-Environment-Name: namespace/m3db-cluster-name\u0026#34; -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/services/m3coordinator/placement/init -d \u0026#39;{ \u0026#34;instances\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;m3coordinator01\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;m3coordinator01:7507\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;m3coordinator01\u0026#34;, \u0026#34;port\u0026#34;: 7507 } ] }\u0026#39; Note: When you add or remove m3coordinator instances they must be added to this placement.\nAdd m3coordinator consumer group to outbound topic Add the m3coordinator placement to receive traffic from the topic (make sure to set message TTL to match your desired maximum in memory retry message buffer):\ncurl -vvvsSf -H \u0026#34;Cluster-Environment-Name: namespace/m3db-cluster-name\u0026#34; -H \u0026#34;Topic-Name: aggregated_metrics\u0026#34; -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic -d \u0026#39;{ \u0026#34;consumerService\u0026#34;: { \u0026#34;serviceId\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;m3coordinator\u0026#34;, \u0026#34;environment\u0026#34;: \u0026#34;namespace/m3db-cluster-name\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;embedded\u0026#34; }, \u0026#34;consumptionType\u0026#34;: \u0026#34;SHARED\u0026#34;, \u0026#34;messageTtlNanos\u0026#34;: \u0026#34;300000000000\u0026#34; } }\u0026#39; Note: 300000000000 nanoseconds is a TTL of 5 minutes for messages to rebuffer for retry.\nRunning Dedicated Coordinator Metrics will still arrive at the m3coordinator, they simply need to be forwarded to an m3aggregator. The m3coordinator then also needs to receive metrics that have been aggregated from the m3aggregator and store them in M3DB, so running an ingestion server should be configured.\nHere is the config you should add to your m3coordinator:\n# This is for sending metrics to the remote m3aggregators downsample: remoteAggregator: client: type: m3msg m3msg: producer: writer: topicName: aggregator_ingest topicServiceOverride: zone: embedded environment: namespace/m3db-cluster-name placement: isStaged: true placementServiceOverride: namespaces: placement: /placement connection: numConnections: 4 messagePool: size: 16384 watermark: low: 0.2 high: 0.5 # This is for configuring the ingestion server that will receive metrics from the m3aggregators on port 7507 ingest: ingester: workerPoolSize: 10000 opPool: size: 10000 retry: maxRetries: 3 jitter: true logSampleRate: 0.01 m3msg: server: listenAddress: \u0026#34;0.0.0.0:7507\u0026#34; retry: maxBackoff: 10s jitter: true M3 Aggregator You can run m3aggregator by either building and running the binary yourself:\nmake m3aggregator ./bin/m3aggregator -f ./src/aggregator/config/m3aggregator.yml Or you can run it with Docker using the Docker file located at docker/m3aggregator/Dockerfile or the publicly provided image quay.io/m3db/m3aggregator:latest.\nYou can use a config like so, making note of the topics used such as aggregator_ingest and aggregated_metrics and the corresponding environment namespace/m3db-cluster-name:\nlogging: level: info metrics: scope: prefix: m3aggregator prometheus: onError: none handlerPath: /metrics listenAddress: 0.0.0.0:6002 timerType: histogram sanitization: prometheus samplingRate: 1.0 extended: none m3msg: server: listenAddress: 0.0.0.0:6000 retry: maxBackoff: 10s jitter: true consumer: messagePool: size: 16384 watermark: low: 0.2 high: 0.5 http: listenAddress: 0.0.0.0:6001 readTimeout: 60s writeTimeout: 60s kvClient: etcd: env: namespace/m3db-cluster-name zone: embedded service: m3aggregator cacheDir: /var/lib/m3kv etcdClusters: - zone: embedded endpoints: - dbnode01:2379 runtimeOptions: kvConfig: environment: namespace/m3db-cluster-name zone: embedded writeValuesPerMetricLimitPerSecondKey: write-values-per-metric-limit-per-second writeValuesPerMetricLimitPerSecond: 0 writeNewMetricLimitClusterPerSecondKey: write-new-metric-limit-cluster-per-second writeNewMetricLimitClusterPerSecond: 0 writeNewMetricNoLimitWarmupDuration: 0 aggregator: hostID: resolver: environment envVarName: M3AGGREGATOR_HOST_ID instanceID: type: host_id verboseErrors: true metricPrefix: \u0026#34;\u0026#34; counterPrefix: \u0026#34;\u0026#34; timerPrefix: \u0026#34;\u0026#34; gaugePrefix: \u0026#34;\u0026#34; aggregationTypes: counterTransformFnType: empty timerTransformFnType: suffix gaugeTransformFnType: empty aggregationTypesPool: size: 1024 quantilesPool: buckets: - count: 256 capacity: 4 - count: 128 capacity: 8 stream: eps: 0.001 capacity: 32 streamPool: size: 4096 samplePool: size: 4096 floatsPool: buckets: - count: 4096 capacity: 16 - count: 2048 capacity: 32 - count: 1024 capacity: 64 client: type: m3msg m3msg: producer: writer: topicName: aggregator_ingest topicServiceOverride: zone: embedded environment: namespace/m3db-cluster-name placement: isStaged: true placementServiceOverride: namespaces: placement: /placement messagePool: size: 16384 watermark: low: 0.2 high: 0.5 placementManager: kvConfig: namespace: /placement environment: namespace/m3db-cluster-name zone: embedded placementWatcher: key: m3aggregator initWatchTimeout: 10s hashType: murmur32 bufferDurationBeforeShardCutover: 10m bufferDurationAfterShardCutoff: 10m bufferDurationForFutureTimedMetric: 10m # Allow test to write into future. resignTimeout: 1m flushTimesManager: kvConfig: environment: namespace/m3db-cluster-name zone: embedded flushTimesKeyFmt: shardset/%d/flush flushTimesPersistRetrier: initialBackoff: 100ms backoffFactor: 2.0 maxBackoff: 2s maxRetries: 3 electionManager: election: leaderTimeout: 10s resignTimeout: 10s ttlSeconds: 10 serviceID: name: m3aggregator environment: namespace/m3db-cluster-name zone: embedded electionKeyFmt: shardset/%d/lock campaignRetrier: initialBackoff: 100ms backoffFactor: 2.0 maxBackoff: 2s forever: true jitter: true changeRetrier: initialBackoff: 100ms backoffFactor: 2.0 maxBackoff: 5s forever: true jitter: true resignRetrier: initialBackoff: 100ms backoffFactor: 2.0 maxBackoff: 5s forever: true jitter: true campaignStateCheckInterval: 1s shardCutoffCheckOffset: 30s flushManager: checkEvery: 1s jitterEnabled: true maxJitters: - flushInterval: 5s maxJitterPercent: 1.0 - flushInterval: 10s maxJitterPercent: 0.5 - flushInterval: 1m maxJitterPercent: 0.5 - flushInterval: 10m maxJitterPercent: 0.5 - flushInterval: 1h maxJitterPercent: 0.25 numWorkersPerCPU: 0.5 flushTimesPersistEvery: 10s maxBufferSize: 5m forcedFlushWindowSize: 10s flush: handlers: - dynamicBackend: name: m3msg hashType: murmur32 producer: writer: topicName: aggregated_metrics topicServiceOverride: zone: embedded environment: namespace/m3db-cluster-name messagePool: size: 16384 watermark: low: 0.2 high: 0.5 passthrough: enabled: true forwarding: maxConstDelay: 5m # Need to add some buffer window, since timed metrics by default are delayed by 1min. entryTTL: 1h entryCheckInterval: 10m maxTimerBatchSizePerWrite: 140 defaultStoragePolicies: [] maxNumCachedSourceSets: 2 discardNaNAggregatedValues: true entryPool: size: 4096 counterElemPool: size: 4096 timerElemPool: size: 4096 gaugeElemPool: size: 4096 Usage Send metrics as usual to your m3coordinator instances in round robin fashion (or any other load balancing strategy), the metrics will be forwarded to the m3aggregator instances, then once aggregated they will be returned to the m3coordinator instances to write to M3DB.\n"
},
{
	"uri": "/glossary/",
	"title": "Standardized Glossary",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/operational_guide/availability_consistency_durability/",
	"title": "Tuning Availability, Consistency, and Durability",
	"tags": [],
	"description": "",
	"content": "M3DB is designed as a High Availability HA system because it doesn\u0026rsquo;t use a consensus protocol like Raft or Paxos to enforce strong consensus and consistency guarantees. However, even within the category of HA systems, there is a broad spectrum of consistency and durability guarantees that a database can provide. To address as many use cases as possible, M3DB can be tuned to achieve the desired balance between performance, availability, durability, and consistency.\nGenerally speaking, the default and example configuration for M3DB favors performance and availability, as that is well-suited for M3DB\u0026rsquo;s most common metrics and Observability use cases. To instead favor consistency and durability, consider tuning values as described in the \u0026ldquo;Tuning for Consistency and Durability\u0026rdquo; section. Database operators who are using M3DB for workloads that require stricter consistency and durability guarantees should consider tuning the default configuration to better suit their use case.\nThe rest of this document describes the various configuration options that are available to M3DB operators to make such tradeoffs. While reading it, we recommend referring to the default configuration file (which has every possible configuration value set) to see how the described values fit into M3DB\u0026rsquo;s configuration as a whole.\nTuning for Performance and Availability Client Write and Read consistency We recommend running the client with writeConsistencyLevel set to majority and readConsistencyLevel set to unstrict_majority. This means that all write must be acknowledged by a quorums of nodes in order to be considered succesful, and that reads will attempt to achieve quorum, but will return the data from a single node if they are unable to achieve quorum. This ensures that reads will normally ensure consistency, but degraded conditions will cause reads to fail outright as long as at least a single node can satisfy the request.\nYou can read about the consistency levels in more detail in the Consistency Levels section\nCommitlog Configuration We recommend running M3DB with an asynchronous commitlog. This means that writes will be reported as successful by the client, though the data may not have been flushed to disk yet.\nFor example, consider the default configuration:\ncommitlog: flushMaxBytes: 524288 flushEvery: 1s queue: calculationType: fixed size: 2097152 This configuration states that the commitlog should be flushed whenever either of the following is true:\n 524288 or more bytes have been written since the last time M3DB flushed the commitlog. One or more seconds has elapsed since the last time M3DB flushed the commitlog.  In addition, the configuration also states that M3DB should allow up to 2097152 writes to be buffered in the commitlog queue before the database node will begin rejecting incoming writes so it can attempt to drain the queue and catch up. Increasing the size of this queue can often increase the write throughput of an M3DB node at the cost of potentially losing more data if the node experiences a sudden failure like a hard crash or power loss.\nWriting New Series Asynchronously The default M3DB YAML configuration will contain the following as a top-level key under the db section:\nwriteNewSeriesAsync: true This instructs M3DB to handle writes for new timeseries (for a given time block) asynchronously. Creating a new timeseries in memory is much more expensive than simply appending a new write to an existing series, so the default configuration of creating them asynchronously improves M3DBs write throughput significantly when many new series are being created all at once.\nHowever, since new time series are created asynchronously, it\u0026rsquo;s possible that there may be a brief delay inbetween when a write is acknowledged by the client and when that series becomes available for subsequent reads.\nM3DB also allows operators to rate limit the number of new series that can be created per second via the following configuration:\nwriteNewSeriesLimitPerSecond: 1048576 This value can be set much lower than the default value for workloads in which a significant increase in cardinality usually indicates a misbehaving caller.\nIgnoring Corrupt Commitlogs on Bootstrap If M3DB is shut down gracefully (i.e via SIGTERM), it will ensure that all pending writes are flushed to the commitlog on disk before the process exists. However, in situations where the process crashed/exited unexpectedly or the node itself experienced a sudden failure, the tail end of the commitlog may be corrupt. In such situations, M3DB will read as much of the commitlog as possible in an attempt to recover the maximum amount of data. However, it then needs to make a decision: it can either (a) come up successfully and tolerate an ostensibly minor amount of data or loss, or (b) attempt to stream the missing data from its peers. This behavior is controlled by the following default configuration:\nbootstrap: commitlog: returnUnfulfilledForCorruptCommitLogFiles: false In the situation where only a single node fails, the optimal outcome is for the node to attempt to repair itself from one of its peers. However, if a quorum of nodes fail and encounter corrupt commitlog files, they will deadlock while attempting to stream data from each other, as no nodes will be able to make progress due to a lack of quorum. This issue requires an operator with significant M3DB operational experience to manually bootstrap the cluster; thus the official recommendation is to set returnUnfulfilledForCorruptCommitLogFiles: false to avoid this issue altogether. In most cases, a small amount of data loss is preferable to a quorum of nodes that crash and fail to start back up automatically.\nTuning for Consistency and Durability Client Write and Read consistency The most important thing to understand is that if you want to guarantee that you will be able to read the result of every successful write, then both writes and reads must be done with majority consistency. This means that both writes and reads will fail if a quorum of nodes are unavailable for a given shard. You can read about the consistency levels in more detail in the Consistency Levels section\nCommitlog Configuration M3DB supports running the commitlog synchronously such that every write is flushed to disk and fsync\u0026rsquo;d before the client receives a successful acknowledgement, but this is not currently exposed to users in the YAML configuration and generally leads to a massive performance degradation. We only recommend operating M3DB this way for workloads where data consistency and durability is strictly required, and even then there may be better alternatives such as running M3DB with the bootstrapping configuration: filesystem,peers,uninitialized_topology as described in our bootstrapping operational guide.\nWriting New Series Asynchronously If you want to guarantee that M3DB will immediately allow you to read data for writes that have been acknowledged by the client, including the situation where the previous write was for a brand new timeseries, then you will need to change the default M3DB configuration to set writeNewSeriesAsync: false as a top-level key under the db section:\nwriteNewSeriesAsync: false This instructs M3DB to handle writes for new timeseries (for a given time block) synchronously. Creating a new timeseries in memory is much more expensive than simply appending a new write to an existing series, so this configuration could have an adverse effect on performance when many new timeseries are being inserted into M3DB concurrently.\nSince this operation is so expensive, M3DB allows operator to rate limit the number of new series that can be created per second via the following configuration (also a top-level key under the db section):\nwriteNewSeriesLimitPerSecond: 1048576 Ignoring Corrupt Commitlogs on Bootstrap As described in the \u0026ldquo;Tuning for Performance and Availability\u0026rdquo; section, we recommend configuring M3DB to ignore corrupt commitlog files on bootstrap. However, if you want to avoid any amount of inconsistency or data loss, no matter how minor, then you should configure M3DB to return unfulfilled when the commitlog bootstrapper encounters corrupt commitlog files. You can do so by modifying your configuration to look like this:\nbootstrap: commitlog: returnUnfulfilledForCorruptCommitLogFiles: true This will force your M3DB nodes to attempt to repair corrupted commitlog files on bootstrap by streaming the data from their peers. In most situations this will be transparent to the operator and the M3DB node will finish bootstrapping without trouble. However, in the scenario where a quorum of nodes for a given shard failed in unison, the nodes will deadlock while attempting to stream data from each other, as no nodes will be able to make progress due to a lack of quorum. This issue requires an operator with significant M3DB operational experience to manually bootstrap the cluster; thus the official recommendation is to avoid configuring M3DB in this way unless data consistency and durability are of utmost importance.\n"
},
{
	"uri": "/how_to/",
	"title": "How To Guides",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/m3db/architecture/peer_streaming/",
	"title": "Peer Streaming",
	"tags": [],
	"description": "",
	"content": "Client Peer streaming is managed by the M3DB client. It fetches all blocks from peers for a specified time range for bootstrapping purposes. It performs the following steps:\n Fetch all metadata for blocks from all peers who own the specified shard Compares metadata from different peers and determines the best peer(s) from which to stream the actual data Streams the block data from peers  Steps 1, 2 and 3 all happen concurrently. As metadata streams in, we begin determining which peer is the best source to stream a given block\u0026rsquo;s data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata. If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from. If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.\nIn terms of error handling, the client will respect the consistency level specified for bootstrap. This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata. For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \u0026ldquo;none\u0026rdquo; is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers. Fetching block data as well will indefinitely retry until the consistency level is achieved.\nThe client supports dynamically changing the bootstrap consistency level, which is helpful in disaster scenarios where the consistency level cannot be achieved. To break the indefinite streaming attempt an operator can change the consistency level to \u0026ldquo;none\u0026rdquo; and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.\nThe diagram below depicts the control flow and concurrency (goroutines and channels) in detail:\n ┌───────────────────────────────────────────────┐ │ │ │ FetchBootstrapBlocksFromPeers │ │ │ └───────────────────────────────────────────────┘ │ │ ┌────────────────────┘ │ ▼ ┌───────────────────────────────┐ │ Main routine │ │ │ │ 1) Create metadataCh │────────────────┐ │ 2) Spin up background routine │ │ └───────────────────────────────┘ Create with metadataCh │ │ │ ▼ │ ┌───────────────────────────────┐ │ │ │ │ │ Background routine │ │ │ │ │ └───────────────────────────────┘ │ │ │ For each peer │ │ │ ┌────────────────┼─────────────────┐ │ │ │ │ │ │ │ │ │ ▼ ▼ ▼ │ ┌───────────────────────────────────────────┐ │ │ StreamBlocksMetadataFromPeer │ │ │ │ │ │ Stream paginated blocks metadata from a │ │ │ peer while pageToken != nil │ │ │ │ │ │ For each blocks' metadata --\u0026gt; put │ │ │ metadata into metadataCh │ │ └───────────────────────────────────────────┘ ▼ ┌───────────────────────────────────────────┐ │ StreamBlocksFromPeers │ │ │ │ 1) Create a background goroutine (details │ │ to the right) │ │ │ │ 2) Create a queue per-peer which each have│ │ their own internal goroutine and will │ │ stream blocks back per-series from a │──────────┐ │ specific peer │ │ │ │ │ │ 3) Loop through the enqueCh and pick an │ Creates with metadataCh │appropriate peer(s) for each series (based │ and enqueueCh │on whether all the peers have the same data│ │ │ or not) and then put that into the queue │ │ │for that peer so the data will be streamed │ │ └───────────────────────────────────────────┘ │ │ ▼ │ ┌──────────────────────────────────────────────────────────┐ │ │ streamAndGroupCollectedBlocksMetadata (injected via │ │ │ streamMetadataFn variable) │ │ │ │ │ │ Loop through the metadataCh aggregating blocks metadata │ │ │per series/block combination from different peers until we│ │ │ have them from all peers for a series/block metadata │ │ │ combination and then \u0026quot;submit\u0026quot; them to the enqueueCh │ │ │ │ │ │At the end, flush any remaining series/block combinations │ │ │(that we received from less than N peers) into the enqueCh│ │ │ as well. │ │ └──────────────────────────────────────────────────────────┘ │ For each peer │ ┌────────────┼─────────────┐ │ │ │ │ │ │ ▼ ▼ ▼ ┌─────────────────────────────────────────────────────────────┐ │ newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer) │ │ │ │For each peer we're creating a new peerBlocksQueue which will│ │ stream data blocks from a specific peer (using the │ │ streamBlocksBatchFromPeer function) and add them to the │ │ blocksResult │ │ │ └─────────────────────────────────────────────────────────────┘  "
},
{
	"uri": "/operational_guide/placement/",
	"title": "Placement",
	"tags": [],
	"description": "",
	"content": "Note: The words placement and topology are used interchangeably throughout the M3DB documentation and codebase.\nA M3DB cluster has exactly one Placement. That placement maps the cluster\u0026rsquo;s shard replicas to nodes. A cluster also has 0 or more namespaces (analogous to tables in other databases), and each node serves every namespace for the shards it owns. In other words, if the cluster topology states that node A owns shards 1, 2, and 3 then node A will own shards 1, 2, 3 for all configured namespaces in the cluster.\nM3DB stores its placement (mapping of which NODES are responsible for which shards) in etcd. There are three possible states that each node/shard pair can be in:\n Initializing Available Leaving  Note that these states are not a reflection of the current status of an M3DB node, but an indication of whether a given node has ever successfully bootstrapped and taken ownership of a given shard (achieved goal state). For example, in a new cluster all the nodes will begin with all of their shards in the Initializing state. Once all the nodes finish bootstrapping, they will mark all of their shards as Available. If all the M3DB nodes are stopped at the same time, the cluster placement will still show all of the shards for all of the nodes as Available.\nInitializing State The Initializing state is the state in which all new node/shard combinations begin. For example, upon creating a new placement all the node/shard pairs will begin in the Initializing state and only once they have successfully bootstrapped will they transition to the Available state.\nThe Initializing state is not limited to new placement, however, as it can also occur during placement changes. For example, during a node add/replace the new node will begin with all of its shards in the Initializing state until it can stream the data it is missing from its peers. During a node removal, all of the nodes who receive new shards (as a result of taking over the responsibilities of the node that is leaving) will begin with those shards marked as Initializing until they can stream in the data from the node leaving the cluster, or one of its peers.\nAvailable State Once a node with a shard in the Initializing state successfully bootstraps all of the data for that shard, it will mark that shard as Available (for the single node) in the cluster placement.\nLeaving State The Leaving state indicates that a node has been marked for removal from the cluster. The purpose of this state is to allow the node to remain in the cluster long enough for the nodes that are taking over its responsibilities to stream data from it.\nSample Cluster State Transitions - Node Add Node adds are performed by adding the new node to the placement. Some portion of the existing shards will be assigned to the new node based on its weight, and they will begin in the Initializing state. Similarly, the shards will be marked as Leaving on the node that are destined to lose ownership of them. Once the new node finishes bootstrapping the shards, it will update the placement to indicate that the shards it owns are Available and that the Leaving node should no longer own that shard in the placement.\nReplication factor: 3 ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ Node A │ │ Node B │ │ Node C │ │ Node D │ ┌──────────────────────────┬─────┴─────────────────┴─────┬────┴─────────────────┴────┬───┴─────────────────┴───┬───┴─────────────────┴───┐ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │ ┌──────────────────────┐│ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ Shard 1: Available │ │ │ Shard 1: Available │ │ │ Shard 1: Available ││ │ │ 1) Initial Placement │ │ Shard 2: Available │ │ │ Shard 2: Available │ │ │ Shard 2: Available ││ │ │ │ │ Shard 3: Available │ │ │ Shard 3: Available │ │ │ Shard 3: Available ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │ └──────────────────────┘│ │ ├──────────────────────────┼─────────────────────────────┼───────────────────────────┼─────────────────────────┼─────────────────────────┤ │ │ │ │ │ │ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │ ┌──────────────────────┐│┌──────────────────────┐ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ Shard 1: Leaving │ │ │ Shard 1: Available │ │ │ Shard 1: Available │││Shard 1: Initializing │ │ │ 2) Begin Node Add │ │ Shard 2: Available │ │ │ Shard 2: Leaving │ │ │ Shard 2: Available │││Shard 2: Initializing │ │ │ │ │ Shard 3: Available │ │ │ Shard 3: Available │ │ │ Shard 3: Leaving │││Shard 3: Initializing │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │ └──────────────────────┘│└──────────────────────┘ │ │ │ │ │ │ │ ├──────────────────────────┼─────────────────────────────┼───────────────────────────┼─────────────────────────┼─────────────────────────┤ │ │ │ │ │ │ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │ ┌──────────────────────┐│┌──────────────────────┐ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ Shard 2: Available │ │ │ Shard 1: Available │ │ │ Shard 1: Available │││ Shard 1: Available │ │ │ 3) Complete Node Add │ │ Shard 3: Available │ │ │ Shard 3: Available │ │ │ Shard 2: Available │││ Shard 2: Available │ │ │ │ │ │ │ │ │ │ │ │││ Shard 3: Available │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │ └──────────────────────┘│└──────────────────────┘ │ │ │ │ │ │ │ └──────────────────────────┴─────────────────────────────┴───────────────────────────┴─────────────────────────┴─────────────────────────┘ Sample Cluster State Transitions - Node Remove Node removes are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as Leaving and those shards are distributed to the remaining nodes (based on their weight) and assigned a state of Initializing. Once the existing nodes that are taking ownership of the leaving nodes shards finish bootstrapping, they will update the placement to indicate that the shards that they just acquired are Available and that the leaving node should no longer own those shards in the placement.\nReplication factor: 3 ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ Node A │ │ Node B │ │ Node C │ │ Node D │ ┌──────────────────────────┬─────┴─────────────────┴─────┬────┴─────────────────┴────┬───┴─────────────────┴───┬───┴─────────────────┴───┐ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │ ┌──────────────────────┐│┌──────────────────────┐ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ Shard 2: Available │ │ │ Shard 1: Available │ │ │ Shard 1: Available │││ Shard 1: Available │ │ │ 1) Initial Placement │ │ Shard 3: Available │ │ │ Shard 3: Available │ │ │ Shard 2: Available │││ Shard 2: Available │ │ │ │ │ │ │ │ │ │ │ │││ Shard 3: Available │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ │ │ │ │ │ │ │ │││ │ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │ └──────────────────────┘│└──────────────────────┘ │ ├──────────────────────────┼─────────────────────────────┼───────────────────────────┼─────────────────────────┼─────────────────────────┤ │ │ │ │ │ │ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │┌───────────────────────┐│┌──────────────────────┐ │ │ │ │ │ │ │ │ ││ │││ │ │ │ │ │ │ │ │ │ ││ │││ │ │ │ │ │ Shard 1: Initializing │ │ │ Shard 1: Available │ ││ Shard 1: Available │││ Shard 1: Leaving │ │ │ 2) Begin Node Remove │ │ Shard 2: Available │ │ │ Shard 2: Initializing│ ││ Shard 2: Available │││ Shard 2: Leaving │ │ │ │ │ Shard 3: Available │ │ │ Shard 3: Available │ ││ Shard 3: Initializing│││ Shard 3: Leaving │ │ │ │ │ │ │ │ │ ││ │││ │ │ │ │ │ │ │ │ │ ││ │││ │ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │└───────────────────────┘│└──────────────────────┘ │ │ │ │ │ │ │ ├──────────────────────────┼─────────────────────────────┼───────────────────────────┼─────────────────────────┼─────────────────────────┤ │ │ │ │ │ │ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │ ┌──────────────────────┐│ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ Shard 1: Avaiable │ │ │ Shard 1: Available │ │ │ Shard 1: Available ││ │ │ 3) Complete Node Remove │ │ Shard 2: Available │ │ │ Shard 2: Available │ │ │ Shard 2: Available ││ │ │ │ │ Shard 3: Available │ │ │ Shard 3: Available │ │ │ Shard 3: Available ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │ └──────────────────────┘│ │ │ │ │ │ │ │ └──────────────────────────┴─────────────────────────────┴───────────────────────────┴─────────────────────────┴─────────────────────────┘ Sample Cluster State Transitions - Node Replace Node replaces are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as Leaving and those shards are all added to the node that is being added and assigned a state of Initializing. Once the replacement node finishes bootstrapping, it will update the placement to indicate that the shards that it acquired are Available and that the leaving node should no longer own those shards in the placement.\nReplication factor: 3 ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ Node A │ │ Node B │ │ Node C │ │ Node D │ ┌──────────────────────────┬─────┴─────────────────┴─────┬────┴─────────────────┴────┬───┴─────────────────┴───┬───┴─────────────────┴───┐ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │ ┌──────────────────────┐│ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ Shard 1: Available │ │ │ Shard 1: Available │ │ │ Shard 1: Available ││ │ │ 1) Initial Placement │ │ Shard 2: Available │ │ │ Shard 2: Available │ │ │ Shard 2: Available ││ │ │ │ │ Shard 3: Available │ │ │ Shard 3: Available │ │ │ Shard 3: Available ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │ └──────────────────────┘│ │ ├──────────────────────────┼─────────────────────────────┼───────────────────────────┼─────────────────────────┼─────────────────────────┤ │ │ │ │ │ │ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │┌───────────────────────┐│┌──────────────────────┐ │ │ │ │ │ │ │ │ ││ │││ │ │ │ │ │ │ │ │ │ ││ │││ │ │ │ │ │ Shard 1: Available │ │ │ Shard 1: Available │ ││ Shard 1: Leaving │││Shard 1: Initializing │ │ │ 2) Begin Node Replace │ │ Shard 2: Available │ │ │ Shard 2: Available │ ││ Shard 2: Leaving │││Shard 2: Initializing │ │ │ │ │ Shard 3: Available │ │ │ Shard 3: Available │ ││ Shard 3: Leaving │││Shard 3: Initializing │ │ │ │ │ │ │ │ │ ││ │││ │ │ │ │ │ │ │ │ │ ││ │││ │ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │└───────────────────────┘│└──────────────────────┘ │ │ │ │ │ │ │ ├──────────────────────────┼─────────────────────────────┼───────────────────────────┼─────────────────────────┼─────────────────────────┤ │ │ │ │ │ │ │ │ ┌─────────────────────────┐ │ ┌───────────────────────┐ │ │┌──────────────────────┐ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ Shard 1: Avaiable │ │ │ Shard 1: Available │ │ ││ Shard 1: Available │ │ │ 3) Complete Node Replace│ │ Shard 2: Available │ │ │ Shard 2: Available │ │ ││ Shard 2: Available │ │ │ │ │ Shard 3: Available │ │ │ Shard 3: Available │ │ ││ Shard 3: Available │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ │ └─────────────────────────┘ │ └───────────────────────┘ │ │└──────────────────────┘ │ │ │ │ │ │ │ └──────────────────────────┴─────────────────────────────┴───────────────────────────┴─────────────────────────┴─────────────────────────┘ Cluster State Transitions - Placement Updates Initiation The diagram below depicts the sequence of events that happen during a node replace and illustrates which entity is performing the placement update (in etcd) at each step.\n┌────────────────────────────────┐ │ Node A │ │ │ │ Shard 1: Available │ │ Shard 2: Available │ Operator performs node replace by │ Shard 3: Available │ updating placement in etcd such │ │ that shards on node A are marked └────────────────────────────────┤ Leaving and shards on node B are │ marked Initializing └─────────────────────────────────┐ │ │ │ │ │ ▼ ┌────────────────────────────────┐ │ Node A │ │ │ │ Shard 1: Leaving │ │ Shard 2: Leaving │ │ Shard 3: Leaving │ │ │ └────────────────────────────────┘ ┌────────────────────────────────┐ │ Node B │ │ │ │ Shard 1: Initializing │ ┌────────────────────────────────┐ │ Shard 2: Initializing │ │ │ │ Shard 3: Initializing │ │ │ │ │ │ Node A │ └────────────────────────────────┘ │ │ │ │ │ │ │ │ │ └────────────────────────────────┘ │ │ ┌────────────────────────────────┐ │ │ Node B │ │ │ │ │ │ Shard 1: Available │ Node B completes bootstrapping and │ Shard 2: Available │◀────updates placement (via etcd) to │ Shard 3: Available │ indicate shard state is Available and │ │ that Node A should no longer own any shards └────────────────────────────────┘ "
},
{
	"uri": "/how_to/use_as_tsdb/",
	"title": "Using M3DB as a general purpose time series database",
	"tags": [],
	"description": "",
	"content": "Overview M3 has native integrations that make it particularly easy to use it as a metrics storage for Prometheus and Graphite. M3DB can also be used as a general purpose distributed time series database by itself.\nData Model IDs and Tags M3DB\u0026rsquo;s data model allows multiple namespaces, each of which can be configured and tuned independently.\nEach namespace can also be configured with its own schema (see \u0026ldquo;Schema Modeling\u0026rdquo; section below).\nWithin a namespace, each time series is uniquely identified by an ID which can be any valid string / byte array. In addition, tags can be attached to any series which makes the series queryable using the inverted index.\nM3DB\u0026rsquo;s inverted index supports term (exact match) and regular expression queries over all tag values, and individual tag queries can be arbitrarily combined using AND, OR, and NOT operators.\nFor example, imagine an application that tracks a fleet of vehicles. One potential structure for the time series could be as follows:\n    Timeseries 1 Timeseries 2 Timeseries 3 Timeseries 4     Timeseries ID vehicle_id_1 vehicle_id_2 vehicle_id_3 vehicle_id_4   \u0026ldquo;type\u0026rdquo; tag value sedan bike scooter scooter   \u0026ldquo;city\u0026rdquo; tag value san_francisco san_francisco new_york chicago   \u0026ldquo;version\u0026rdquo; tag value 0_1_0 0_1_0 0_1_1 0_1_2    This would allow users to issue queries that answer questions like:\n \u0026ldquo;What time series IDs exist for any vehicle type operating in San Francisco?\u0026rdquo; \u0026ldquo;What time series IDs exist for scooters that are NOT operating in Chicago?\u0026rdquo; \u0026ldquo;What time series IDs exist where the \u0026ldquo;version\u0026rdquo; tag matches the regular expression: 0_1_[12]\u0026rdquo;  TODO(rartoul): Discuss the ability to perform limited amounts of aggregation queries here as well.\nTODO(rartoul): Discuss ID / tags mutability.\nData Points Each time series in M3DB stores data as a stream of data points in the form of \u0026lt;timestamp, value\u0026gt; tuples. Timestamp resolution can be as granular as individual nanoseconds.\nThe value portion of the tuple is a Protobuf message that matches the configured namespace schema, which requires that all values in the current time series must also match this schema. This limitation may be lifted in the future.\nSchema Modeling Every M3DB namespace can be configured with a Protobuf-defined schema that every value in the time series must conform to\nFor example, continuing with the vehicle fleet tracking example introduced earlier, a schema might look as follows:\nsyntax = \u0026#34;proto3\u0026#34;;message VehicleLocation { double latitude = 1; double longitude = 2; double fuel_percent = 3; string status = 4;}While M3DB strives to support the entire proto3 language spec, only the following features are currently supported:\n Scalar values Nested messages Repeated fields Map fields Reserved fields  The following features are currently not supported:\n Any fields Oneof fields Options of any type Custom field types  Compression While M3DB supports schemas that contain nested messages, repeated fields, and map fields, currently it can only effectively compress top level scalar fields. For example, M3DB can compress every field in the following schema:\nsyntax = \u0026#34;proto3\u0026#34;;message VehicleLocation { double latitude = 1; double longitude = 2; double fuel_percent = 3; string status = 4;}however, it will not apply any form of compression to the attributes field in this schema:\nsyntax = \u0026#34;proto3\u0026#34;;message VehicleLocation { double latitude = 1; double longitude = 2; double fuel_percent = 3; string status = 4; map\u0026lt;string, string\u0026gt; attributes = 5;}While the latter schema is valid, the attributes field will not be compressed; users should weigh the tradeoffs between more expressive schema and better compression for each use case.\nFor more details on the compression scheme and its limitations, review the documentation for M3DB\u0026rsquo;s compressed Protobuf encoding.\nGetting Started M3DB setup For more advanced setups, it\u0026rsquo;s best to follow the guides on how to configure an M3DB cluster manually or using Kubernetes. However, this tutorial will walk you through configuring a single node setup locally for development.\nFirst, run the following command to pull the latest M3DB image:\ndocker pull quay.io/m3db/m3dbnode:latest  Next, run the following command to start the M3DB container:\ndocker run -p 7201:7201 -p 7203:7203 -p 9000:9000 -p 9001:9001 -p 9002:9002 -p 9003:9003 -p 9004:9004 -p 2379:2379 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db -v $(pwd)/src/dbnode/config/m3dbnode-local-etcd-proto.yml:/etc/m3dbnode/m3dbnode.yml -v \u0026lt;PATH_TO_SCHEMA_Protobuf_FILE\u0026gt;:/etc/m3dbnode/default_schema.proto quay.io/m3db/m3dbnode:latest  Breaking that down:\n All the -p flags expose the necessary ports. The -v $(pwd)/m3db_data:/var/lib/m3db section creates a bind mount that enables M3DB to persist data between container restarts. The -v \u0026lt;PATH_TO_YAML_CONFIG_FILE\u0026gt;:/etc/m3dbnode/m3dbnode.yml section mounts the specified configuration file in the container which allows configuration changes by restarting the container (rather than rebuilding it). This example file can be used as a good starting point. It configures the database to have the Protobuf feature enabled and expects one namespace with the name default and a Protobuf message name of VehicleLocation for the schema. You\u0026rsquo;ll need to update that portion of the config if you intend to use a different schema than the example one used throughout this document. Note that hard-coding paths to the schema should only be done for local development and testing. For production use-cases, M3DB supports storing the current schema in etcd so that it can be update dynamically. TODO(rartoul): Document how to do that as well as what kind of schema changes are safe / backwards compatible. The -v \u0026lt;PATH_TO_SCHEMA_Protobuf_FILE\u0026gt;:/etc/m3dbnode/default_schema.proto section mounts the Protobuf file containing the schema in the container, similar to the configuration file this allows the schema to be changed by restarting the container instead of rebuilding it. You can use this example schema as a starting point. Is is also the same example schema that is used by the sample Go program discussed below in the \u0026ldquo;Clients\u0026rdquo; section. Also see the bullet point above about not hard coding schema files in production.  Once the M3DB container has started, issue the following CURL statement to create the default namespace:\ncurl -X POST http://localhost:7201/api/v1/database/create -d \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;namespaceName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;retentionTime\u0026#34;: \u0026#34;4h\u0026#34; }\u0026#39; Note that the retentionTime is set artificially low to conserve resources.\nAfter a few moments, the M3DB container should finish bootstrapping. At this point it should be ready to serve write and read queries.\nClients Note: M3DB only has a Go client; this is unlikely to change in the future due to the fact that the client is \u0026ldquo;fat\u0026rdquo; and contains a substantial amount of logic that would be difficult to port to other languages.\nUsers interested in interacting with M3DB directly from Go applications can reference this runnable example to get an understanding of how to interact with M3DB in Go. Note that the example above uses the same default namespace and VehicleLocation schema used throughout this document so it can be run directly against an M3DB docker container setup using the \u0026ldquo;M3DB setup\u0026rdquo; instructions above.\nM3DB will eventually support other languages by exposing an M3Coordinator endpoint which will allow users to write/read from M3DB directly using GRPC/JSON.\n"
},
{
	"uri": "/m3db/architecture/caching/",
	"title": "Caching",
	"tags": [],
	"description": "",
	"content": "Overview Blocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don\u0026rsquo;t need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \u0026ldquo;cache\u0026rdquo; itself is not a separate datastructure in memory, cached blocks are simply stored in their respective in-memory objects with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.\nFor general purpose workloads, the lru caching policy is reccommended.\nNone Cache Policy The none cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.\nAll Cache Policy The all cache policy is the opposite of the none cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.\nRecently Read Cache Policy The recently_read cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the recently_read cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.\nData eviction from memory is triggered by the \u0026ldquo;ticking\u0026rdquo; process described in the background processes section\nLeast Recently Used (LRU) Cache Policy The lru cache policy uses an lru list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in wired_list.go for implementation details.\n"
},
{
	"uri": "/operational_guide/",
	"title": "Operational Guides",
	"tags": [],
	"description": "",
	"content": "This list of operational guides provide documentation for operating M3.\n"
},
{
	"uri": "/operational_guide/placement_configuration/",
	"title": "Placement Configuration",
	"tags": [],
	"description": "",
	"content": "M3DB was designed from the ground up to be a distributed (clustered) database that is availability zone or rack aware (by using isolation groups). Clusters will seamlessly scale with your data, and you can start with a small number of nodes and grow it to a size of several hundred nodes with no downtime or expensive migrations.\nBefore reading the rest of this document, we recommend familiarizing yourself with the M3DB placement documentation\nNote: The primary limiting factor for the maximum size of an M3DB cluster is the number of shards. Picking an appropriate number of shards is more of an art than a science, but our recommendation is as follows:\nThe number of shards that M3DB uses is configurable and there are a couple of key points to note when deciding the number to use. The more nodes you have, the more shards you want because you want the shards to be evenly distributed amongst your nodes. However, because each shard requires more files to be created, you also don’t want to have too many shards per node. This is due to the fact each bit of data needs to be repartitioned and moved around the cluster (i.e. every bit of data needs to be moved all at once). Below are some guidelines depending on how many nodes you will have in your cluster eventually - you will need to decide the number of shards up front, you cannot change this once the cluster is created.\n   Number of Nodes Number of Shards     3 64   6 128   12 256   24 512   48 1024   128+ 4096    After performing any of the instructions documented below a new placement will automatically be generated to distribute the shards among the M3DB nodes such that the isolation group and replication factor constraints are met.\nIf the constraints cannot be met, because there are not enough nodes to calculate a new placement such that each shard is replicated on the desired number of nodes with none of the nodes owning the same shard existing in the same isolation group, then the operation will fail.\nIn other words, all you have to do is issue the desired instruction and the M3 stack will take care of making sure that your data is distributed with appropriate replication and isolation.\nIn the case of the M3DB nodes, nodes that have received new shards will immediately begin receiving writes (but not serving reads) for the new shards that they are responsible for. They will also begin streaming in all the data for their newly acquired shards from the peers that already have data for those shards. Once the nodes have finished streaming in the data for the shards that they have acquired, they will mark their status for those shards as Available in the placement and begin accepting writes. Simultaneously, the nodes that are losing ownership of any shards will mark their status for those shards as Leaving. Once all the nodes accepting ownership of the new shards have finished streaming data from them, they will relinquish ownership of those shards and remove all the data associated with the shards they lost from memory and from disk.\nM3Coordinator nodes will also pickup the new placement from etcd and alter which M3DB nodes they issue writes and reads to appropriately.\nUnderstanding the Placement Configuration The placement configuration contains a few core values that control how the placement behaves.\nID This is the identifier for a node in the placement and can be any value that uniquely identifies an M3DB node.\nIsolation Group This value controls how nodes that own the same M3DB shards are isolated from each other. For example, in a single datacenter configuration this value could be set to the rack that the M3DB node lives on. As a result, the placement will guarantee that nodes that exist on the same rack do not share any shards, allowing the cluster to survive the failure of an entire rack. Alternatively, if M3DB was deployed in an AWS region, the isolation group could be set to the region\u0026rsquo;s availability zone and that would ensure that the cluster would survive the loss of an entire availability zone.\nZone This value controls what etcd zone the M3DB node belongs to.\nWeight This value should be an integer and controls how the cluster will weigh the number of shards that an individual node will own. If you\u0026rsquo;re running the M3DB cluster on homogenous hardware, then you probably want to assign all M3DB nodes the same weight so that shards are distributed evenly. On the otherhand, if you\u0026rsquo;re running the cluster on heterogenous hardware, then this value should be higher for nodes with higher resources for whatever the limiting factor is in your cluster setup. For example, if disk space (as opposed to memory or CPU) is the limiting factor in how many shards any given node in your cluster can tolerate, then you could assign a higher value to nodes in your cluster that have larger disks and the placement calculations would assign them a higher number of shards.\nEndpoint This value should be in the form of \u0026lt;M3DB_HOST_NAME\u0026gt;:\u0026lt;M3DB_NODE_LISTEN_PORT\u0026gt; and identifies how network requests should be routed to this particular node in the placement.\nHostname This value should be in the form of \u0026lt;M3DB_HOST_NAME\u0026gt; and identifies the address / host name of the M3DB node.\nPort This value should be in the form of \u0026lt;M3DB_NODE_LISTEN_PORT\u0026gt; and identifies the port over which this M3DB node expects to receive traffic (defaults to 9000).\nPlacement Operations NOTE: If you find yourself performing operations on seed nodes, please refer to the seed node-specific sections below before making changes.\nThe instructions below all contain sample curl commands, but you can always review the API documentation by navigating to\nhttp://\u0026lt;M3_COORDINATOR_HOST_NAME\u0026gt;:\u0026lt;CONFIGURED_PORT(default 7201)\u0026gt;/api/v1/openapi or our online API documentation.\nNote: The peers bootstrapper must be configured on all nodes in the M3DB cluster for placement changes to work. The peers bootstrapper is enabled by default, so you only need to worry about this if you modified the default bootstrapping configuration\nAdditionally, the following headers can be used in the placement operations:\n Cluster-Environment-Name:\nThis header is used to specify the cluster environment name. If not set, the default default_env is used.  Cluster-Zone-Name:\nThis header is used to specify the cluster zone name. If not set, the default embedded is used.  Placement Initialization Send a POST request to the /api/v1/services/m3db/placement/init endpoint\ncurl -X POST localhost:7201/api/v1/services/m3db/placement/init -d \u0026#39;{ \u0026#34;num_shards\u0026#34;: \u0026lt;DESIRED_NUMBER_OF_SHARDS\u0026gt;, \u0026#34;replication_factor\u0026#34;: \u0026lt;DESIRED_REPLICATION_FACTOR\u0026gt;(recommended 3), \u0026#34;instances\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;NODE_1_ID\u0026gt;\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;\u0026lt;NODE_1_ISOLATION_GROUP\u0026gt;\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;\u0026lt;ETCD_ZONE\u0026gt;\u0026#34;, \u0026#34;weight\u0026#34;: \u0026lt;NODE_WEIGHT\u0026gt;, \u0026#34;endpoint\u0026#34;: \u0026#34;\u0026lt;NODE_1_HOST_NAME\u0026gt;:\u0026lt;NODE_1_PORT\u0026gt;\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;NODE_1_HOST_NAME\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: \u0026lt;NODE_1_PORT\u0026gt; }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;NODE_2_ID\u0026gt;\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;\u0026lt;NODE_2_ISOLATION_GROUP\u0026gt;\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;\u0026lt;ETCD_ZONE\u0026gt;\u0026#34;, \u0026#34;weight\u0026#34;: \u0026lt;NODE_WEIGHT\u0026gt;, \u0026#34;endpoint\u0026#34;: \u0026#34;\u0026lt;NODE_2_HOST_NAME\u0026gt;:\u0026lt;NODE_2_PORT\u0026gt;\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;NODE_2_HOST_NAME\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: \u0026lt;NODE_2_PORT\u0026gt; }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;NODE_3_ID\u0026gt;\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;\u0026lt;NODE_3_ISOLATION_GROUP\u0026gt;\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;\u0026lt;ETCD_ZONE\u0026gt;\u0026#34;, \u0026#34;weight\u0026#34;: \u0026lt;NODE_WEIGHT\u0026gt;, \u0026#34;endpoint\u0026#34;: \u0026#34;\u0026lt;NODE_3_HOST_NAME\u0026gt;:\u0026lt;NODE_3_PORT\u0026gt;\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;NODE_3_HOST_NAME\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: \u0026lt;NODE_3_PORT\u0026gt; } ] }\u0026#39; Adding a Node Send a POST request to the /api/v1/services/m3db/placement endpoint\ncurl -X POST \u0026lt;M3_COORDINATOR_HOST_NAME\u0026gt;:\u0026lt;M3_COORDINATOR_PORT(default 7201)\u0026gt;/api/v1/services/m3db/placement -d \u0026#39;{ \u0026#34;instances\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;NEW_NODE_ID\u0026gt;\u0026#34;, \u0026#34;isolationGroup\u0026#34;: \u0026#34;\u0026lt;NEW_NODE_ISOLATION_GROUP\u0026gt;\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;\u0026lt;ETCD_ZONE\u0026gt;\u0026#34;, \u0026#34;weight\u0026#34;: \u0026lt;NODE_WEIGHT\u0026gt;, \u0026#34;endpoint\u0026#34;: \u0026#34;\u0026lt;NEW_NODE_HOST_NAME\u0026gt;:\u0026lt;NEW_NODE_PORT\u0026gt;(default 9000)\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;NEW_NODE_HOST_NAME\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: \u0026lt;NEW_NODE_PORT\u0026gt; } ] }\u0026#39; After sending the add command you will need to wait for the M3DB cluster to reach the new desired state. You\u0026rsquo;ll know that this has been achieved when the placement shows that all shards for all hosts are in the Available state.\nRemoving a Node Send a DELETE request to the /api/v1/services/m3db/placement/\u0026lt;NODE_ID\u0026gt; endpoint.\ncurl -X DELETE \u0026lt;M3_COORDINATOR_HOST_NAME\u0026gt;:\u0026lt;M3_COORDINATOR_PORT(default 7201)\u0026gt;/api/v1/services/m3db/placement/\u0026lt;NODE_ID\u0026gt; After sending the delete command you will need to wait for the M3DB cluster to reach the new desired state. You\u0026rsquo;ll know that this has been achieved when the placement shows that all shards for all hosts are in the Available state.\nAdding / Removing Seed Nodes If you find yourself adding or removing etcd seed nodes then we highly recommend setting up an external etcd cluster, as the overhead of operating two stateful systems at once is non-trivial. As this is not a recommended production setup, this section is intentionally brief.\nTo add or remove nodes to the etcd cluster, use etcdctl member add and etcdctl member remove as found in Replacing a Seed Node below. A general rule to keep in mind is that any time the M3DB process starts on a seed node, the list of cluster members in etcdctl member list must match exactly the list in config.\nReplacing a Node NOTE: If using embedded etcd and replacing a seed node, please read the section below.\nSend a POST request to the /api/v1/services/m3db/placement/replace endpoint containing hosts to replace and candidates to replace it with.\ncurl -X POST \u0026lt;M3_COORDINATOR_HOST_NAME\u0026gt;:\u0026lt;M3_COORDINATOR_PORT(default 7201)\u0026gt;/api/v1/services/m3db/placement/replace -d \u0026#39;{ \u0026#34;leavingInstanceIDs\u0026#34;: [\u0026#34;\u0026lt;OLD_NODE_ID\u0026gt;\u0026#34;], \u0026#34;candidates\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;NEW_NODE_ID\u0026gt;\u0026#34;, \u0026#34;isolationGroup\u0026#34;: \u0026#34;\u0026lt;NEW_NODE_ISOLATION_GROUP\u0026gt;\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;\u0026lt;ETCD_ZONE\u0026gt;\u0026#34;, \u0026#34;weight\u0026#34;: \u0026lt;NODE_WEIGHT\u0026gt;, \u0026#34;endpoint\u0026#34;: \u0026#34;\u0026lt;NEW_NODE_HOST_NAME\u0026gt;:\u0026lt;NEW_NODE_PORT\u0026gt;(default 9000)\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;NEW_NODE_HOST_NAME\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: \u0026lt;NEW_NODE_PORT\u0026gt; } ] }\u0026#39; Replacing a Seed Node If you are using the embedded etcd mode (which is only recommended for test purposes) and replacing a seed node then there are a few more steps to be done, as you are essentially doing two replace operations at once (replacing an etcd node and and M3DB node). To perform these steps you will need the etcdctl binary (version 3.2 or later), which can be downloaded from the etcd releases page.\nMany of the instructions here are mentioned in the etcd operational guide, which we recommend reading for more context.\nTo provide some context for the commands below, let\u0026rsquo;s assume your cluster was created with the below configuration, and that you\u0026rsquo;d like to replace host3 with a new host host4, which has IP address 1.1.1.4:\ninitialCluster: - hostID: host1 endpoint: http://1.1.1.1:2380 - hostID: host2 endpoint: http://1.1.1.2:2380 - hostID: host3 endpoint: http://1.1.1.3:2380  On an existing node in the cluster that is not the one you\u0026rsquo;re removing, use etcdctl to remove host3 from the cluster:  $ ETCDCTL_API=3 etcdctl member list 9d29673cf1328d1, started, host1, http://1.1.1.1:2380, http://1.1.1.1:2379 f14613b6c8a336b, started, host2, http://1.1.1.2:2380, http://1.1.1.2:2379 2fd477713daf243, started, host3, http://1.1.1.3:2380, http://1.1.1.3:2379 # \u0026lt;\u0026lt;\u0026lt; INSTANCE WE WANT TO REMOVE $ ETCDCTL_API=3 etcdctl member remove 2fd477713daf243 Removed member 2fd477713daf243 from cluster From the same host, use etcdctl to add host4 to the cluster:  $ ETCDCTL_API=3 etcdctl member add host4 --peer-urls http://1.1.1.4:2380 Before starting M3DB on host4, modify the initial cluster list to indicate host4 has a cluster state of existing. Note: if you had previously started M3DB on this host, you\u0026rsquo;ll have to remove the member subdirectory in $M3DB_DATA_DIR/etcd/.  initialCluster: - hostID: host1 endpoint: http://1.1.1.1:2380 - hostID: host2 endpoint: http://1.1.1.2:2380 - hostID: host4 clusterState: existing endpoint: http://1.1.1.4:2380  Start M3DB on host4.\n  On all other seed nodes, update their initialCluster list to be exactly equal to the list on host4 from step 3. Rolling restart the hosts one at a time, waiting until they indicate they are bootstrapped (indicated in the /health) endpoint before continuing to the next.\n  Follow the steps from Replacing a Seed Node to replace host3 with host4 in the M3DB placement.\n  Setting a new placement (Not Recommended) This endpoint is unsafe since it creates a brand new placement and therefore should be used with extreme caution. Some use cases for using this endpoint include:\n Changing IP addresses of nodes Rebalancing shards  If the placement for M3DB needs to be recreated, the /api/v1/services/m3db/placement/set can be used to do so. Please note, a placement already needs to exist to use this endpoint. If no placement exists, use the Placement Initialization endpoint described above. Also, as mentioned above, this endpoint creates an entirely new placement therefore complete placement information needs to be passed into the body of the request. The recommended way to this is to get the existing placement using /api/v1/placement and modify that (as the placement field) along with two additional fields \u0026ndash; version and confirm. Please see below for a full example:\ncurl -X POST localhost:7201/api/v1/services/m3db/placement/set -d \u0026#39;{ \u0026#34;placement\u0026#34;: { \u0026#34;num_shards\u0026#34;: \u0026lt;DESIRED_NUMBER_OF_SHARDS\u0026gt;, \u0026#34;replication_factor\u0026#34;: \u0026lt;DESIRED_REPLICATION_FACTOR\u0026gt;(recommended 3), \u0026#34;instances\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;NODE_1_ID\u0026gt;\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;\u0026lt;NODE_1_ISOLATION_GROUP\u0026gt;\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;\u0026lt;ETCD_ZONE\u0026gt;\u0026#34;, \u0026#34;weight\u0026#34;: \u0026lt;NODE_WEIGHT\u0026gt;, \u0026#34;endpoint\u0026#34;: \u0026#34;\u0026lt;NODE_1_HOST_NAME\u0026gt;:\u0026lt;NODE_1_PORT\u0026gt;\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;NODE_1_HOST_NAME\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: \u0026lt;NODE_1_PORT\u0026gt; }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;NODE_2_ID\u0026gt;\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;\u0026lt;NODE_2_ISOLATION_GROUP\u0026gt;\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;\u0026lt;ETCD_ZONE\u0026gt;\u0026#34;, \u0026#34;weight\u0026#34;: \u0026lt;NODE_WEIGHT\u0026gt;, \u0026#34;endpoint\u0026#34;: \u0026#34;\u0026lt;NODE_2_HOST_NAME\u0026gt;:\u0026lt;NODE_2_PORT\u0026gt;\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;NODE_2_HOST_NAME\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: \u0026lt;NODE_2_PORT\u0026gt; }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;NODE_3_ID\u0026gt;\u0026#34;, \u0026#34;isolation_group\u0026#34;: \u0026#34;\u0026lt;NODE_3_ISOLATION_GROUP\u0026gt;\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;\u0026lt;ETCD_ZONE\u0026gt;\u0026#34;, \u0026#34;weight\u0026#34;: \u0026lt;NODE_WEIGHT\u0026gt;, \u0026#34;endpoint\u0026#34;: \u0026#34;\u0026lt;NODE_3_HOST_NAME\u0026gt;:\u0026lt;NODE_3_PORT\u0026gt;\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;NODE_3_HOST_NAME\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: \u0026lt;NODE_3_PORT\u0026gt; } ] }, \u0026#34;version\u0026#34;: \u0026lt;version\u0026gt;, \u0026#34;confirm\u0026#34;: \u0026lt;true/false\u0026gt; }\u0026#39; Note: The set endpoint can also be used to set the placements in M3Aggregator and M3Coordinator using the following endpoints, respectively:\n/api/v1/m3aggregator/set /api/v1/m3coordinator/set "
},
{
	"uri": "/integrations/",
	"title": "Integrations",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/operational_guide/namespace_configuration/",
	"title": "Namespace Configuration",
	"tags": [],
	"description": "",
	"content": "Namespaces in M3DB are analogous to tables in other databases. Each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize. For more information about namespaces and the technical details of their implementation, read our storage engine documentation.\nNamespace Operations The operations below include sample cURLs, but you can always review the API documentation by navigating to\nhttp://\u0026lt;M3_COORDINATOR_HOST_NAME\u0026gt;:\u0026lt;CONFIGURED_PORT(default 7201)\u0026gt;/api/v1/openapi or our online API documentation.\nAdditionally, the following headers can be used in the namespace operations:\n Cluster-Environment-Name:\nThis header is used to specify the cluster environment name. If not set, the default default_env is used.  Cluster-Zone-Name:\nThis header is used to specify the cluster zone name. If not set, the default embedded is used.  Adding a Namespace Recommended (Easy way) The recommended way to add a namespace to M3DB is to use our api/v1/database/namespace/create endpoint. This API abstracts over a lot of the complexity of configuring a namespace and requires only two pieces of configuration to be provided: the name of the namespace, as well as its retention.\nFor example, the following cURL:\ncurl -X POST \u0026lt;M3_COORDINATOR_IP_ADDRESS\u0026gt;:\u0026lt;CONFIGURED_PORT(default 7201)\u0026gt;/api/v1/database/namespace/create -d \u0026#39;{ \u0026#34;namespaceName\u0026#34;: \u0026#34;default_unaggregated\u0026#34;, \u0026#34;retentionTime\u0026#34;: \u0026#34;24h\u0026#34; }\u0026#39; will create a namespace called default_unaggregated with a retention of 24 hours. All of the other namespace options will either use reasonable default values or be calculated based on the provided retentionTime.\nAdding a namespace does not require restarting M3DB, but will require modifying the M3Coordinator configuration to include the new namespace, and then restarting it.\nIf you feel the need to configure the namespace options yourself (for performance or other reasons), read the Advanced section below.\nAdvanced (Hard Way) The \u0026ldquo;advanced\u0026rdquo; API allows you to configure every aspect of the namespace that you\u0026rsquo;re adding which can sometimes be helpful for development, debugging, and tuning clusters for maximum performance. Adding a namespace is a simple as using the POST api/v1/namespace API on an M3Coordinator instance.\ncurl -X POST \u0026lt;M3_COORDINATOR_IP_ADDRESS\u0026gt;:\u0026lt;CONFIGURED_PORT(default 7201)\u0026gt;/api/v1/namespace -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;default_unaggregated\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;bootstrapEnabled\u0026#34;: true, \u0026#34;flushEnabled\u0026#34;: true, \u0026#34;writesToCommitLog\u0026#34;: true, \u0026#34;cleanupEnabled\u0026#34;: true, \u0026#34;snapshotEnabled\u0026#34;: true, \u0026#34;repairEnabled\u0026#34;: false, \u0026#34;retentionOptions\u0026#34;: { \u0026#34;retentionPeriod\u0026#34;: \u0026#34;2d\u0026#34;, \u0026#34;blockSize\u0026#34;: \u0026#34;2h\u0026#34;, \u0026#34;bufferFuture\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;bufferPast\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;blockDataExpiry\u0026#34;: true, \u0026#34;blockDataExpiryAfterNotAccessedPeriod\u0026#34;: \u0026#34;5m\u0026#34; }, \u0026#34;indexOptions\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;blockSize\u0026#34;: \u0026#34;2h\u0026#34; } } }\u0026#39; Adding a namespace does not require restarting M3DB, but will require modifying the M3Coordinator configuration to include the new namespace, and then restarting it.\nDeleting a Namespace Deleting a namespace is a simple as using the DELETE /api/v1/namespace API on an M3Coordinator instance.\ncurl -X DELETE \u0026lt;M3_COORDINATOR_IP_ADDRESS\u0026gt;:\u0026lt;CONFIGURED_PORT(default 7201)\u0026gt;/api/v1/namespace/\u0026lt;NAMESPACE_NAME\u0026gt;\nNote that deleting a namespace will not have any effect on the M3DB nodes until they are all restarted. In addition, the namespace will need to be removed from the M3Coordinator configuration and then the M3Coordinator node will need to be restarted.\nModifying a Namespace There is currently no atomic namespace modification endpoint. Instead, you will need to delete a namespace and then add it back again with the same name, but modified settings. Review the individual namespace settings above to determine whether or not a given setting is safe to modify. For example, it is never safe to modify the blockSize of a namespace.\nAlso, be very careful not to restart the M3DB nodes after deleting the namespace, but before adding it back. If you do this, the M3DB nodes may detect the existing data files on disk and delete them since they are not configured to retain that namespace.\nViewing a Namespace In order to view a namespace and its attributes, use the GET /api/v1/namespace API on a M3Coordinator instance. Additionally, for readability/debugging purposes, you can add the debug=true parameter to the URL to view block sizes, buffer sizes, etc. in duration format as opposed to nanoseconds (default).\nNamespace Attributes bootstrapEnabled This controls whether M3DB will attempt to bootstrap the namespace on startup. This value should always be set to true unless you have a very good reason to change it as setting it to false can cause data loss when restarting nodes.\nCan be modified without creating a new namespace: yes\nflushEnabled This controls whether M3DB will periodically flush blocks to disk once they become immutable. This value should always be set to true unless you have a very good reason to change it as setting it to false will cause increased memory utilization and potential data loss when restarting nodes.\nCan be modified without creating a new namespace: yes\nwritesToCommitlog This controls whether M3DB will includes writes to this namespace in the commitlog. This value should always be set to true unless you have a very good reason to change it as setting it to false will cause potential data loss when restarting nodes.\nCan be modified without creating a new namespace: yes\nsnapshotEnabled This controls whether M3DB will periodically write out snapshot files for this namespace which act as compacted commitlog files. This value should always be set to true unless you have a very good reason to change it as setting it to false will increasing bootstrapping times (reading commitlog files is slower than reading snapshot files) and increase disk utilization (snapshot files are compressed but commitlog files are uncompressed).\nCan be modified without creating a new namespace: yes\nrepairEnabled If enabled, the M3DB nodes will attempt to compare the data they own with the data of their peers and emit metrics about any discrepancies. This feature is experimental and we do not recommend enabling it under any circumstances.\nretentionOptions retentionPeriod This controls the duration of time that M3DB will retain data for the namespace. For example, if this is set to 30 days, then data within this namespace will be available for querying up to 30 days after it is written. Note that this retention operates at the block level, not the write level, so its possible for individual datapoints to only be available for less than the specified retention. For example, if the blockSize was set to 24 hour and the retention was set to 30 days then a write that arrived at the very end of a 24 hour block would only be available for 29 days, but the node itself would always support querying the last 30 days worth of data.\nCan be modified without creating a new namespace: yes\nblockSize This is the most important value to consider when tuning the performance of an M3DB namespace. Read the storage engine documentation for more details, but the basic idea is that larger blockSizes will use more memory, but achieve higher compression. Similarly, smaller blockSizes will use less memory, but have worse compression. In testing, good compression occurs with blocksizes containing around 720 samples per timeseries.\nCan be modified without creating a new namespace: no\nBelow are recommendations for block size based on resolution:\n   Resolution Block Size     5s 60m   15s 3h   30s 6h   1m 12h   5m 60h    bufferFuture and bufferPast These values control how far into the future and the past (compared to the system time on an M3DB node) writes for the namespace will be accepted. For example, consider the following configuration:\nbufferPast: 10m bufferFuture: 20m currentSystemTime: 2:35:00PM Now consider the following writes (all of which arrive at 2:35:00PM system time, but include datapoints with the specified timestamps):\n2:25:00PM - Accepted, within the 10m bufferPast 2:24:59PM - Rejected, outside the 10m bufferPast 2:55:00PM - Accepted, within the 20m bufferFuture 2:55:01PM - Rejected, outside the 20m bufferFuture While it may be tempting to configure bufferPast and bufferFuture to very large values to prevent writes from being rejected, this may cause performance issues. M3DB is a timeseries database that is optimized for realtime data. Out of order writes, as well as writes for times that are very far into the future or past are much more expensive and will cause additional CPU / memory pressure. In addition, M3DB cannot evict a block from memory until it is no longer mutable and large bufferPast and bufferFuture values effectively increase the amount of time that a block is mutable for which means that it must be kept in memory for a longer period of time.\nCan be modified without creating a new namespace: yes\nIndex Options enabled Whether to use the built-in indexing. Must be true.\nCan be modified without creating a new namespace: no\nblockSize The size of blocks (in duration) that the index uses. Should match the databases blocksize for optimal memory usage.\nCan be modified without creating a new namespace: no\n"
},
{
	"uri": "/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Some common problems and resolutions\nPorts 9001-9004 aren\u0026rsquo;t open after starting m3db. These ports will not open until a namespace and placement have been created and the nodes have bootstrapped.\nBootstrapping is slow Double check your configuration against the bootstrapping guide. The nodes will log what bootstrapper they are using and what time range they are using it for.\nIf you\u0026rsquo;re using the commitlog bootstrapper, and it seems to be slow, ensure that snapshotting is enabled for your namespace. Enabling snapshotting will require a node restart to take effect.\nIf an m3db node hasn\u0026rsquo;t been able to snapshot for awhile, or is stuck in the commitlog bootstrapping phase for a long time due to accumulating a large number of commitlogs, consider using the peers bootstrapper. In situations where a large number of commitlogs need to be read, the peers bootstrapper will outperform the commitlog bootstrapper (faster and less memory usage) due to the fact that it will receive already-compressed data from its peers. Keep in mind that this will only work with a replication factor of 3 or larger and if the nodes peers are healthy and bootstrapped. Review the bootstrapping guide for more information.\nNodes a crashing with memory allocation errors, but there\u0026rsquo;s plenty of available memory Ensure you\u0026rsquo;ve set vm.max_map_count to something like 262,144 using sysctl. Find out more in the Clustering the Hard Way document.\nWhat to do if my M3DB node is OOM’ing?  Ensure that you are not co-locating coordinator, etcd or query nodes with your M3DB nodes. Colocation or embedded mode is fine for a development environment, but highly discouraged in production. Check to make sure you are running adequate block sizes based on the retention of your namespace. See namespace configuration for more information. Ensure that you use at most 50-60% memory utilization in the normal running state. You want to ensure enough overhead to handle bursts of metrics, especially ones with new IDs as those will take more memory initially. High cardinality metrics can also lead to OOMs especially if you are not adequately provisioned. If you have many unique timeseries such as ones containing UUIDs or timestamps as tag values, you should consider mitigating their cardinality.  Using the /debug/dump API The /debug/dump API returns a number of helpful debugging outputs. Currently, we support the following:\n CPU profile: determines where a program spends its time while actively consuming CPU cycles (as opposed to while sleeping or waiting for I/O). Currently set to take a 5 second profile. Heap profile: reports memory allocation samples; used to monitor current and historical memory usage, and to check for memory leaks. Goroutines profile: reports the stack traces of all current goroutines. Host profile: returns data about the underlying host such as PID, working directory, etc. Namespace: returns information about the namespaces setup in M3DB Placement: returns information about the placement setup in M3DB  This endpoint can be used on both the db nodes as well as the coordinator/query nodes. However, namespace and placement info are only available on the coordinator debug endpoint.\nTo use this, simply run the following on either the M3DB debug listen port or the regular port on M3Coordinator.\ncurl \u0026lt;m3db_or_m3coordinator_ip\u0026gt;:\u0026lt;port\u0026gt;/debug/dump \u0026gt; \u0026lt;tmp_zip_file.zip\u0026gt; # unzip the file unzip \u0026lt;tmp_zip_file.zip\u0026gt;  Now, you will have the following files, which you can use for troubleshooting using the below commands:\ncpuSource\ngo tool pprof -http=:16000 cpuSource  heapSource\ngo tool pprof -http=:16000 heapSource  goroutineProfile\nless goroutineProfile  hostSource\nless hostSource | jq .  namespaceSource\nless namespaceSource | jq .  placementSource\nless placementSource | jq . "
},
{
	"uri": "/operational_guide/bootstrapping_crash_recovery/",
	"title": "Bootstrapping &amp; Crash Recovery",
	"tags": [],
	"description": "",
	"content": "Introduction We recommend reading the placement operational guide before reading the rest of this document.\nWhen an M3DB node is turned on (goes through a placement change) it needs to go through a bootstrapping process to determine the integrity of data that it has, replay writes from the commit log, and/or stream missing data from its peers. In most cases, as long as you\u0026rsquo;re running with the default and recommended bootstrapper configuration of: filesystem,commitlog,peers,uninitialized_topology then you should not need to worry about the bootstrapping process at all and M3DB will take care of doing the right thing such that you don\u0026rsquo;t lose data and consistency guarantees are met. Note that the order of the configured bootstrappers does matter.\nGenerally speaking, we recommend that operators do not modify the bootstrappers configuration, but in the rare case that you to, this document is designed to help you understand the implications of doing so.\nM3DB currently supports 5 different bootstrappers:\n filesystem commitlog peers uninitialized_topology noop-all  When the bootstrapping process begins, M3DB nodes need to determine two things:\n What shards the bootstrapping node should bootstrap, which can be determined from the cluster placement. What time-ranges the bootstrapping node needs to bootstrap those shards for, which can be determined from the namespace retention.  For example, imagine a M3DB node that is responsible for shards 1, 5, 13, and 25 according to the cluster placement. In addition, it has a single namespace called \u0026ldquo;metrics\u0026rdquo; with a retention of 48 hours. When the M3DB node is started, the node will determine that it needs to bootstrap shards 1, 5, 13, and 25 for the time range starting at the current time and ending 48 hours ago. In order to obtain all this data, it will run the configured bootstrappers in the specified order. Every bootstrapper will notify the bootstrapping process of which shard/ranges it was able to bootstrap and the bootstrapping process will continue working its way through the list of bootstrappers until all the shards/ranges required have been marked as fulfilled. Otherwise the M3DB node will fail to start.\nBootstrappers Filesystem Bootstrapper The filesystem bootstrapper\u0026rsquo;s responsibility is to determine which immutable Fileset files exist on disk, and if so, mark them as fulfilled. The filesystem bootstrapper achieves this by scanning M3DB\u0026rsquo;s directory structure and determining which Fileset files exist on disk. Unlike the other bootstrappers, the filesystem bootstrapper does not need to load any data into memory, it simply verifies the checksums of the data on disk and other components of the M3DB node will handle reading (and caching) the data dynamically once it begins to serve reads.\nCommitlog Bootstrapper The commitlog bootstrapper\u0026rsquo;s responsibility is to read the commitlog and snapshot (compacted commitlogs) files on disk and recover any data that has not yet been written out as an immutable Fileset file. Unlike the filesystem bootstrapper, the commit log bootstrapper cannot simply check which files are on disk in order to determine if it can satisfy a bootstrap request. Instead, the commitlog bootstrapper determines whether it can satisfy a bootstrap request using a simple heuristic.\nOn a shard-by-shard basis, the commitlog bootstrapper will consult the cluster placement to see if the node it is running on has ever achieved the Available status for the specified shard. If so, then the commit log bootstrapper should have all the data since the last Fileset file was flushed and will return that it can satisfy any time range for that shard. In other words, the commit log bootstrapper is all-or-nothing for a given shard: it will either return that it can satisfy any time range for a given shard or none at all. In addition, the commitlog bootstrapper assumes it is running after the filesystem bootstrapper. M3DB will not allow you to run with a configuration where the filesystem bootstrapper is placed after the commitlog bootstrapper, but it will allow you to run the commitlog bootstrapper without the filesystem bootstrapper which can result in loss of data, depending on the workload.\nPeers Bootstrapper The peers bootstrapper\u0026rsquo;s responsibility is to stream in data for shard/ranges from other M3DB nodes (peers) in the cluster. This bootstrapper is only useful in M3DB clusters with more than a single node and where the replication factor is set to a value larger than 1. The peers bootstrapper will determine whether or not it can satisfy a bootstrap request on a shard-by-shard basis by consulting the cluster placement and determining if there are enough peers to satisfy the bootstrap request. For example, imagine the following M3DB placement where node A is trying to perform a peer bootstrap:\n ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ Node A │ │ Node B │ │ Node C │ ────┴─────────────────┴──────────┴─────────────────┴────────┴─────────────────┴─── ┌─────────────────────────┐ ┌───────────────────────┐ ┌──────────────────────┐ │ │ │ │ │ │ │ │ │ │ │ │ │ Shard 1: Initializing │ │ Shard 1: Initializing │ │ Shard 1: Available │ │ Shard 2: Initializing │ │ Shard 2: Initializing │ │ Shard 2: Available │ │ Shard 3: Initializing │ │ Shard 3: Initializing │ │ Shard 3: Available │ │ │ │ │ │ │ │ │ │ │ │ │ └─────────────────────────┘ └───────────────────────┘ └──────────────────────┘ In this case, the peers bootstrapper running on node A will not be able to fullfill any requests because node B is in the Initializing state for all of its shards and cannot fulfill bootstrap requests. This means that node A\u0026rsquo;s peers bootstrapper cannot meet its default consistency level of majority for bootstrapping (1 \u0026lt; 2 which is majority with a replication factor of 3). On the other hand, node A would be able to peer bootstrap its shards in the following placement because its peers (nodes B/C) have sufficient replicas of the shards it needs in the Available state:\n ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ Node A │ │ Node B │ │ Node C │ ────┴─────────────────┴──────────┴─────────────────┴────────┴─────────────────┴─── ┌─────────────────────────┐ ┌───────────────────────┐ ┌──────────────────────┐ │ │ │ │ │ │ │ │ │ │ │ │ │ Shard 1: Initializing │ │ Shard 1: Available │ │ Shard 1: Available │ │ Shard 2: Initializing │ │ Shard 2: Available │ │ Shard 2: Available │ │ Shard 3: Initializing │ │ Shard 3: Available │ │ Shard 3: Available │ │ │ │ │ │ │ │ │ │ │ │ │ └─────────────────────────┘ └───────────────────────┘ └──────────────────────┘ Note that a bootstrap consistency level of majority is the default value, but can be modified by changing the value of the key m3db.client.bootstrap-consistency-level in etcd to one of: none, one, unstrict_majority (attempt to read from majority, but settle for less if any errors occur), majority (strict majority), and all. For example, if an entire cluster with a replication factor of 3 was restarted simultaneously, all the nodes would get stuck in an infinite loop trying to peer bootstrap from each other and not achieving majority until an operator modified this value. Note that this can happen even if all the shards were in the Available state because M3DB nodes will reject all read requests for a shard until they have bootstrapped that shard (which has to happen everytime the node is restarted).\nNote: Any bootstrappers configuration that does not include the peers bootstrapper will be unable to handle dynamic placement changes of any kind.\nUninitialized Topology Bootstrapper The purpose of the uninitialized_topology bootstrapper is to succeed bootstraps for all time ranges for shards that have never been completely bootstrapped (at a cluster level). This allows us to run the default bootstrapper configuration of: filesystem,commitlog,peers,uninitialized_topology such that the filesystem and commitlog bootstrappers are used by default in node restarts, the peers bootstrapper is used for node adds/removes/replaces, and bootstraps still succeed for brand new placement where both the commitlog and peers bootstrappers will be unable to succeed any bootstraps. In other words, the uninitialized_topology bootstrapper allows us to place the commitlog bootstrapper before the peers bootstrapper and still succeed bootstraps with brand new placements without resorting to using the noop-all bootstrapper which suceeds bootstraps for all shard/time-ranges regardless of the status of the placement.\nThe uninitialized_topology bootstrapper determines whether a placement is \u0026ldquo;new\u0026rdquo; for a given shard by counting the number of nodes in the Initializing state and Leaving states and there are more Initializing than Leaving, then it succeeds the bootstrap because that means the placement has never reached a state where all nodes are Available.\nNo Operational All Bootstrapper The noop-all bootstrapper succeeds all bootstraps regardless of requests shards/time ranges.\nBootstrappers Configuration Now that we\u0026rsquo;ve gone over the various bootstrappers, let\u0026rsquo;s consider how M3DB will behave in different configurations. Note that we include uninitialized_topology at the end of all the lists of bootstrappers because its required to get a new placement up and running in the first place, but is not required after that (although leaving it in has no detrimental effects). Also note that any configuration that does not include the peers bootstrapper will not be able to handle dynamic placement changes like node adds/removes/replaces.\nfilesystem,commitlog,peers,uninitialized_topology (default) This is the default bootstrappers configuration for M3DB and will behave \u0026ldquo;as expected\u0026rdquo; in the sense that it will maintain M3DB\u0026rsquo;s consistency guarantees at all times, handle node adds/replaces/removes correctly, and still work with brand new placements / topologies. This is the only configuration that we recommend using in production.\nIn the general case, the node will use only the filesystem and commitlog bootstrappers on node startup. However, in the case of a node add/remove/replace, the commitlog bootstrapper will detect that it is unable to fulfill the bootstrap request (because the node has never reached the Available state) and defer to the peers bootstrapper to stream in the data.\nAdditionally, if it is a brand new placement where even the peers bootstrapper cannot fulfill the bootstrap, this will be detected by the uninitialized_topology bootstrapper which will succeed the bootstrap.\nfilesystem,peers,uninitialized_topology Everytime a node is restarted it will attempt to stream in all of the the data for any blocks that it has never flushed, which is generally the currently active block and possibly the previous block as well. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of any unflushed blocks. This mode can lead to violations of M3DB\u0026rsquo;s consistency guarantees due to the fact that commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see peers bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.\npeers,uninitialized_topology Every time a node is restarted, it will attempt to stream in all of the data that it is responsible for from its peers, completely ignoring the immutable Fileset files it already has on disk. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of all data on an individual node. This mode can lead to violations of M3DB\u0026rsquo;s consistency guarantees due to the fact that the commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see peers bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.\nInvalid bootstrappers configuration For the sake of completeness, we\u0026rsquo;ve included a short discussion below of some bootstrapping configurations that we consider \u0026ldquo;invalid\u0026rdquo; in that they are likely to lose data / violate M3DB\u0026rsquo;s consistency guarantees and/or not handle placement changes in a correct way.\nfilesystem,commitlog,uninitialized_topology This bootstrapping configuration will work just fine if nodes are never added/replaced/removed, but will fail when attempting a node add/replace/remove.\nfilesystem,uninitialized_topology Every time a node is restarted it will utilize the immutable Fileset files its already written out to disk, but any data that it had received since it wrote out the last set of immutable files will be lost.\ncommitlog,uninitialized_topology Every time a node is restarted it will read all the commit log and snapshot files it has on disk, but it will ignore all the data in the immutable Fileset files that it has already written.\nCrash Recovery NOTE: These steps should not be necessary in most cases, especially if using the default bootstrappers configuration of filesystem,commitlog,peers,uninitialized_topology. However in the case the configuration is non-default or the cluster has been down for a prolonged period of time these steps may be necessary. A good indicator would be log messages related to failing to bootstrap from peers due to consistency issues.\nM3DB may require manual intervention to recover in the event of a prolonged loss of quorum. This is because the Peers Boostrapper must read from a majority of nodes owning a shard to bootstrap.\nTo relax this bootstrapping constraint, a value stored in etcd must be modified that corresponds to the m3db.client.bootstrap-consistency-level runtime flag. Until the coordinator supports an API for this, this must be done manually. The M3 contributors are aware of how cumbersome this is and are working on this API.\nTo update this value in etcd, first determine the environment the M3DB node is using. For example in this configuration, it is default_env. If using the M3DB Operator, the value will be $KUBE_NAMESPACE/$CLUSTER_NAME, where $KUBE_NAMESPACE is the name of the Kubernetes namespace the cluster is located in and $CLUSTER_NAME is the name you have assigned the cluster (such as default/my-test-cluster).\nThe following base64-encoded string represents a Protobuf-serialized message containing the string unstrict_majority: ChF1bnN0cmljdF9tYWpvcml0eQ==. Decode this string and place it in the following etcd key, where $ENV is the value determined above:\n_kv/$ENV/m3db.client.bootstrap-consistency-level Note that on MacOS, base64 requires the -D flag to decode, whereas elsewhere it is likely -d. Also note the use of echo -n to ensure removal of newlines if your shell does not support the \u0026lt;\u0026lt;\u0026lt;STRING pattern.\nOnce the cluster is recovered, the value should be deleted from etcd to revert to normal behavior using etcdctl del.\nExamples:\n# On Linux, using a recent bash, update the value for env=default_env \u0026lt;\u0026lt;\u0026lt;ChF1bnN0cmljdF9tYWpvcml0eQ== base64 -d | env ETCDCTL_API=3 etcdctl put _kv/default_env/m3db.client.bootstrap-consistency-level # On Linux, using a limited shell, update the value for env=default_env echo -n \u0026#34;ChF1bnN0cmljdF9tYWpvcml0eQ==\u0026#34; | base64 -d | env ETCDCTL_API=3 etcdctl put _kv/default_env/m3db.client.bootstrap-consistency-level # On MacOS, update the value for a cluster \u0026#34;test_cluster\u0026#34; in Kubernetes namespace \u0026#34;m3db\u0026#34; echo -n \u0026#34;ChF1bnN0cmljdF9tYWpvcml0eQ==\u0026#34; | base64 -D | kubectl exec -i $ETCD_POD -- env ETCDCTL_API=3 etcdctl put _kv/m3db/test_cluster/m3db.client.bootstrap-consistency-level # Delete the key to restore normal behavior env ETCDCTL_API=3 etcdctl del _kv/default_env/m3db.client.bootstrap-consistency-level "
},
{
	"uri": "/faqs/",
	"title": "FAQs",
	"tags": [],
	"description": "",
	"content": "  Is there a way to disable M3DB embedded etcd and just use an external etcd cluster? Yes, you can definitely do that. It\u0026rsquo;s all just about setting the etcd endpoints in config as etcd hosts instead of M3DB hosts. See these docs for more information on configuring an external etcd cluster.\n  Is there a client that lets me send metrics to m3coordinator without going through Prometheus? Yes, you can use the Prometheus remote write client.\n  Why does my dbnode keep OOM’ing? Refer to the troubleshooting guide.\n  Do you support PromQL? Yes, M3Query and M3Coordinator both support PromQL.\n  Do you support Graphite? Yes, M3Query and M3Coordinator both support Graphite.\n  Does M3DB store both data and (tag) metadata on the same node? Yes it stores the data (i.e. the timeseries datapoints) as well as the tags since it has an embedded index. Make sure you have IndexEnabled set to true in your namespace configuration\n  How are writes handled and how is the data kept consistent within M3DB? M3 uses quorum/majority consistency to ensure data is written to replicas in a way that can be read back consistently. For example, if you have a replication factor of 3 and you set your write and read consistencies to quorum, then all writes will only succeed if they make it to at least 2 of the 3 replicas, and reads will only succeed if they get results back from at least 2 of the 3 replicas\n  Do I need to restart M3DB if I add a namespace? If you’re adding namespaces, the m3dbnode process will pickup the new namespace without a restart.\n  Do I need to restart M3DB if I change or delete a namespace? If you’re removing or modifying an existing namespace, you’ll need to restart the m3dbnode process in order to complete the namespace deletion/modification process. It is recommended to restart one node at a time and wait for a node to be completely bootstrapped before restarting another node.\n  How do I set up aggregation in the coordinator? Refer to the Aggregation section of the M3Query how-to guide.\n  How do I set up aggregation using a separate aggregation tier? See this WIP documentation.\n  Can you delete metrics from M3DB? Not yet, but that functionality is currently being worked on.\n  How can you tell if a node is snapshotting? You can check if your nodes are snapshotting by looking at the Background tasks tab in the M3DB Grafana dashboard.\n  How do you list all available API endpoints? See M3DB OpenAPI.\n  What is the recommended way to upgrade my M3 stack? See the Upgrading M3 guide.\n  When graphing my Prometheus data in Grafana, I see gaps. How do I resolve this? This is due to M3 having a concept of null datapoints whereas Prometheus does not. To resolve this, change Stacking \u0026amp; Null value to Connected under the Visualization tab of your graph.\n  I am receiving the error \u0026quot;could not create etcd watch\u0026quot;,\u0026quot;error\u0026quot;:\u0026quot;etcd watch create timed out after 10s for key: _sd.placement/default_env/m3db\u0026quot; This is due to the fact that M3DB, M3Coordinator, etc. could not connect to the etcd server. Make sure that the endpoints listed under in the following config section are correct AND the correct configuration file is being used.\n  etcdClusters: - zone: embedded endpoints: - HOST1_STATIC_IP_ADDRESS:2379 - HOST2_STATIC_IP_ADDRESS:2379 - HOST3_STATIC_IP_ADDRESS:2379   How can I get a heap dump, cpu profile, etc. See our docs on the /debug/dump api\n  How much memory utilization should I run M3DB at? We recommend not going above 50%.\n  What is the recommended hardware to run on? TBA\n  What is the recommended way to create a new namespace? Refer to the Namespace configuration guide.\n  How can I see the cardinality of my metrics? Currently, the best way is to go to the M3DB Node Details Dashboard and look at the Ticking panel. However, this is not entirely accurate because of the way data is stored in M3DB \u0026ndash; time series are stored inside time-based blocks that you configure. In actuality, the Ticking graph shows you how many unique series there are for the most recent block that has persisted. In the future, we plan to introduce easier ways to determine the number of unique time series.\n  "
},
{
	"uri": "/operational_guide/kernel_configuration/",
	"title": "Docker &amp; Kernel Configuration",
	"tags": [],
	"description": "",
	"content": "This document lists the Kernel tweaks M3DB needs to run well. If you are running on Kubernetes, you may use our sysctl-setter DaemonSet that will set these values for you. Please read the comment in that manifest to understand the implications of applying it.\nRunning with Docker When running M3DB inside Docker, it is recommended to add the SYS_RESOURCE capability to the container (using the --cap-add argument to docker run) so that it can raise its file limits:\ndocker run --cap-add SYS_RESOURCE quay.io/m3/m3dbnode:latest If M3DB is being run as a non-root user, M3\u0026rsquo;s setcap images are required:\ndocker run --cap-add SYS_RESOURCE -u 1000:1000 quay.io/m3/m3dbnode:latest-setcap More information on Docker\u0026rsquo;s capability settings can be found here.\nvm.max_map_count M3DB uses a lot of mmap-ed files for performance, as a result, you might need to bump vm.max_map_count. We suggest setting this value to 3000000, so you don’t have to come back and debug issues later.\nOn Linux, you can increase the limits by running the following command as root:\nsysctl -w vm.max_map_count=3000000 To set this value permanently, update the vm.max_map_count setting in /etc/sysctl.conf.\nvm.swappiness vm.swappiness controls how much the virtual memory subsystem will try to swap to disk. By default, the kernel configures this value to 60, and will try to swap out items in memory even when there is plenty of RAM available to the system.\nWe recommend sizing clusters such that M3DB is running on a substrate (hosts/containers) such that no-swapping is necessary, i.e. the process is only using 30-50% of the maximum available memory. And therefore recommend setting the value of vm.swappiness to 1. This tells the kernel to swap as little as possible, without altogether disabling swapping.\nOn Linux, you can configure this by running the following as root:\nsysctl -w vm.swappiness=1 To set this value permanently, update the vm.swappiness setting in /etc/sysctl.conf.\nrlimits M3DB also can use a high number of files and we suggest setting a high max open number of files due to per partition fileset volumes.\nYou may need to override the system and process-level limits set by the kernel with the following commands. To check the existing values run:\nsysctl -n fs.file-max and\nsysctl -n fs.nr_open to see the kernel and process limits respectively. If either of the values are less than three million (our minimum recommended value), then you can update them with the following commands:\nsysctl -w fs.file-max=3000000 sysctl -w fs.nr_open=3000000 To set these values permanently, update the fs.file-max and fs.nr_open settings in /etc/sysctl.conf.\nAlternatively, if you wish to have M3DB run under systemd you can use our service example which will set sane defaults. Keep in mind that you\u0026rsquo;ll still need to configure the kernel and process limits because systemd will not allow a process to exceed them and will silently fallback to a default value which could cause M3DB to crash due to hitting the file descriptor limit. Also note that systemd has a system.conf file and a user.conf file which may contain limits that the service-specific configuration files cannot override. Be sure to check that those files aren\u0026rsquo;t configured with values lower than the value you configure at the service level.\nBefore running the process make sure the limits are set, if running manually you can raise the limit for the current user with ulimit -n 3000000.\nAutomatic Limit Raising During startup, M3DB will attempt to raise its open file limit to the current value of fs.nr_open. This is a benign operation; if it fails M3DB, will simply emit a warning.\n"
},
{
	"uri": "/operational_guide/etcd/",
	"title": "etcd",
	"tags": [],
	"description": "",
	"content": "The M3 stack leverages etcd as a distributed key-value storage to:\n Update cluster configuration in realtime Manage placements for our distributed / sharded tiers like M3DB and M3Aggregator Perform leader-election in M3Aggregator  and much more!\nOverview M3DB ships with support for running embedded etcd (called seed nodes), and while this is convenient for testing and development, we don\u0026rsquo;t recommend running with this setup in production.\nBoth M3 and etcd are complex distributed systems, and trying to operate both within the same binary is challenging and dangerous for production workloads.\nInstead, we recommend running an external etcd cluster that is isolated from the M3 stack so that performing operations like node adds, removes, and replaces are easier.\nWhile M3 relies on etcd to provide strong consistency, the operations we use it for are all low-throughput so you should be able to operate a very low maintenance etcd cluster. A 3-node setup for high availability should be more than sufficient for most workloads.\nConfiguring an External etcd Cluster M3DB Most of our documentation demonstrates how to run M3DB with embedded etcd nodes. Once you\u0026rsquo;re ready to switch to an external etcd cluster, all you need to do is modify the M3DB config to remove the seedNodes field entirely and then change the endpoints under etcdClusters to point to your external etcd nodes instead of the M3DB seed nodes.\nFor example this portion of the config\nconfig: service: env: default_env zone: embedded service: m3db cacheDir: /var/lib/m3kv etcdClusters: - zone: embedded endpoints: - http://m3db_seed1:2379 - http://m3db_seed2:2379 - http://m3db_seed3:2379 seedNodes: initialCluster: - hostID: m3db_seed1 endpoint: http://m3db_seed1:2380 - hostID: m3db_seed2 endpoint: http://m3db_seed2:2380 - hostID: m3db_seed3 endpoint: http://m3db_seed3:2380 would become\nconfig: service: env: default_env zone: embedded service: m3db cacheDir: /var/lib/m3kv etcdClusters: - zone: embedded endpoints: - http://external_etcd1:2379 - http://external_etcd2:2379 - http://external_etcd3:2379 Note: M3DB placements and namespaces are stored in etcd so if you want to switch to an external etcd cluster you\u0026rsquo;ll need to recreate all your placements and namespaces. You can do this manually or use etcdctl\u0026rsquo;s Mirror Maker functionality.\nM3Coordinator M3Coordinator does not run embedded etcd, so configuring it to use an external etcd cluster is simple. Just replace the endpoints under etcdClusters in the YAML config to point to your external etcd nodes instead of the M3DB seed nodes. See the M3DB example above for a detailed before/after comparison of the YAML config.\netcd Operations Embedded etcd If you\u0026rsquo;re running M3DB seed nodes with embedded etcd (which we do not recommend for production workloads) and need to perform a node add/replace/remove then follow our placement configuration guide and pay special attention to follow the special instructions for seed nodes.\nExternal etcd Just follow the instructions in the etcd docs.\n"
},
{
	"uri": "/operational_guide/mapping_rollup/",
	"title": "Mapping Rules",
	"tags": [],
	"description": "",
	"content": "Mapping Rules Mapping rules are used to configure the storage policy for metrics. The storage policy determines how long to store metrics for and at what resolution to keep them at. For example, a storage policy of 1m:48h tells M3 to keep the metrics for 48hrs at a 1min resolution. Mapping rules can be configured in the m3coordinator configuration file under the downsample \u0026gt; rules \u0026gt; mappingRules stanza. We will use the following as an example.\ndownsample: rules: mappingRules: - name: \u0026#34;mysql metrics\u0026#34; filter: \u0026#34;app:mysql*\u0026#34; aggregations: [\u0026#34;Last\u0026#34;] storagePolicies: - resolution: 1m retention: 48h - name: \u0026#34;nginx metrics\u0026#34; filter: \u0026#34;app:nginx*\u0026#34; aggregations: [\u0026#34;Last\u0026#34;] storagePolicies: - resolution: 30s retention: 24h - resolution: 1m retention: 48h Here, we have two mapping rules configured \u0026ndash; one for mysql metrics and one for nginx metrics. The filter determines what metrics each rule applies to. The mysql metrics rule will apply to any metrics where the app tag contains mysql* as the value (* being a wildcard). Similarly, the nginx metrics rule will apply to all metrics where the app tag contains nginx* as the value.\nThe aggregations field determines what functions to apply to the datapoints within a resolution tile. For example, if an application emits a metric every 10sec and the resolution for that metrics\u0026rsquo;s storage policy is 1min, M3 will need to combine 6 datapoints. If the aggregations policy is Last, M3 will take the last value in that 1min bucket. aggregations can be one of the following:\nLast Min Max Mean Median Count Sum SumSq Stdev P10 P20 P30 P40 P50 P60 P70 P80 P90 P95 P99 P999 P9999 Lastly, the storagePolicies field determines which namespaces to store the metrics in. For example, the mysql metrics will be sent to the 1m:48h namespace, while the nginx metrics will be sent to both the 1m:48h and 30s:24h namespaces.\nNote: the namespaces listed under the storagePolicies stanza must exist in M3DB.\nRollup Rules Rollup rules are used to rollup metrics and aggregate in different ways by arbitrary dimensions before they are stored.\nHere\u0026rsquo;s an example of creating a new monotonic counter called http_request_rollup_no_pod_bucket from a set of histogram metrics originally called http_request_bucket:\ndownsample: rules: rollupRules: - name: \u0026#34;http_request latency by route and git_sha without pod\u0026#34; filter: \u0026#34;__name__:http_request_bucket k8s_pod:* le:* git_sha:* route:*\u0026#34; transforms: - transform: type: \u0026#34;Increase\u0026#34; - rollup: metricName: \u0026#34;http_request_rollup_no_pod_bucket\u0026#34; groupBy: [\u0026#34;le\u0026#34;, \u0026#34;git_sha\u0026#34;, \u0026#34;route\u0026#34;, \u0026#34;status_code\u0026#34;, \u0026#34;region\u0026#34;] aggregations: [\u0026#34;Sum\u0026#34;] - transform: type: \u0026#34;Add\u0026#34; storagePolicies: - resolution: 30s retention: 720h Note: only metrics that contain all of the group_by tags will be rolled up. For example, in the above config, only http_request_bucket metrics that have all of the group_by labels present will be rolled up into the new metric http_request_rollup_no_pod_bucket.\nWhile the above example can be used to create a new rolled up metric, often times the goal of rollup rules is to eliminate the underlaying, raw metrics. In order to do this, a mappingRule will need to be added like in the following example (using the metric above as an example) with drop set to true. Additionally, if all of the underlaying metrics are being dropped, there is no need to change the metric name (e.g. in the rollupRule, the metricName field can be equal to the existing metric) \u0026ndash; see below for an example.\ndownsample: rules: mappingRules: - name: \u0026#34;http_request latency by route and git_sha drop raw\u0026#34; filter: \u0026#34;__name__:http_request_bucket k8s_pod:* le:* git_sha:* route:*\u0026#34; drop: true rollupRules: - name: \u0026#34;http_request latency by route and git_sha without pod\u0026#34; filter: \u0026#34;__name__:http_request_bucket k8s_pod:* le:* git_sha:* route:*\u0026#34; transforms: - transform: type: \u0026#34;Increase\u0026#34; - rollup: metricName: \u0026#34;http_request_bucket\u0026#34; # metric name doesn\u0026#39;t change groupBy: [\u0026#34;le\u0026#34;, \u0026#34;git_sha\u0026#34;, \u0026#34;route\u0026#34;, \u0026#34;status_code\u0026#34;, \u0026#34;region\u0026#34;] aggregations: [\u0026#34;Sum\u0026#34;] - transform: type: \u0026#34;Add\u0026#34; storagePolicies: - resolution: 30s retention: 720h Note: In order to store rolled up metrics in an unaggregated namespace, a matching aggregated namespace must be added to the coordinator config. For example, if in the above rule, the 720h namespace under storagePolicies is unaggregated, the following will need to be added to the coordinator config.\n- namespace: default resolution: 30s retention: 720h type: aggregated downsample: all: false "
},
{
	"uri": "/operational_guide/repairs/",
	"title": "Background Repairs",
	"tags": [],
	"description": "",
	"content": "Note: This feature is in beta and only available for use with M3DB when run with the inverted index off. It can be run with the inverted index on however metrics will not be re-indexed if they are repaired so will be invisible to that node for queries.\nOverview Background repairs enable M3DB to eventually reach a consistent state such that all nodes have identical view. An M3DB cluster can be configured to repair itself in the background. If background repairs are enabled, M3DB nodes will continuously scan the metadata of other nodes. If a mismatch is detected, affected nodes will perform a repair such that each node in the cluster eventually settles on a consistent view of the data.\nA repair is performed individually by each node when it detects a mismatch between its metadata and the metadata of its peers. Each node will stream the data for the relevant series, merge the data from its peers with its own, and then write out the resulting merged dataset to disk to make the repair durable. In other words, there is no coordination between individual nodes during the repair process, each node is detecting mismatches on its own and performing a \u0026ldquo;best effort\u0026rdquo; repair by merging all available data from all peers into a new stream.\nConfiguration The feature can be enabled by adding the following configuration to m3dbnode.yml under the db section:\ndb: ... (other configuration) repair: enabled: true By default M3DB will limit the amount of repaired data that can be held in memory at once to 2GiB. This is intended to prevent the M3DB nodes from streaming data from their peers too quickly and running out of memory. Once the 2GiB limit is hit the repair process will throttle itself until some of the streamed data has been flushed to disk (and as a result can be evicted from memory). This limit can be overriden with the following configuration:\ndb: ... (other configuration) limits: maxOutstandingRepairedBytes: 2147483648 # 2GiB In addition, the following two optional fields can also be configured:\ndb: ... (other configuration) repair: enabled: true throttle: 10s checkInterval: 10s The throttle field controls how long the M3DB node will pause between repairing each shard/blockStart combination and the checkInterval field controls how often M3DB will run the scheduling/prioritization algorithm that determines which blocks to repair next. In most situations, operators should omit these fields and rely on the default values.\nCaveats and Limitations  Background repairs do not currently support M3DB\u0026rsquo;s inverted index; as a result, it can only be used for clusters / namespaces where the indexing feature is disabled. Background repairs will wait until (block start + block size + buffer past) has elapsed before attempting to repair a block. For example, if M3DB is configured with a 2 hour block size and a 20 minute buffer past that M3DB will not attempt to repair the 12PM-\u0026gt;2PM block until at least 2:20PM. This limitation is in place primarily to reduce \u0026ldquo;churn\u0026rdquo; caused by repairing mutable data that is actively being modified. Note: This limitation has no impact or negative interaction with M3DB\u0026rsquo;s cold writes feature. In other words, even though it may take some time before a block becomes available for repairs, M3DB will repair the same block repeatedly until it falls out of retention so mismatches between nodes that were caused by \u0026ldquo;cold\u0026rdquo; writes will still eventually be repaired.  "
},
{
	"uri": "/operational_guide/replication_between_clusters/",
	"title": "Replication between clusters",
	"tags": [],
	"description": "",
	"content": "M3DB clusters can be configured to passively replicate data from other clusters. This feature is most commonly used when operators wish to run two (or more) regional clusters that function independently while passively replicating data from the other cluster in an eventually consistent manner.\nThe cross-cluster replication feature is built on-top of the background repairs feature. As a result, it has all the same caveats and limitations. Specifically, it does not currently work with clusters that use M3DB\u0026rsquo;s indexing feature and the replication delay between two clusters will be at least (block size + bufferPast) for data written at the beginning of a block for a given namespace. For use-cases where a large replication delay is unacceptable, the current recommendation is to dual-write to both clusters in parallel and then rely upon the cross-cluster replication feature to repair any discrepancies between the clusters caused by failed dual-writes. This recommendation is likely to change in the future once support for low-latency replication is added to M3DB in the form of commitlog tailing.\nWhile cross-cluster replication is built on top of the background repairs feature, background repairs do not need to be enabled for cross-cluster replication to be enabled. In other words, clusters can be configured such that:\n Background repairs (within a cluster) are disabled and replication is also disabled. Background repairs (within a cluster) are enabled, but replication is disabled. Background repairs (within a cluster) are disabled, but replication is enabled. Background repairs (within a cluster) are enabled and replication is also enabled.  Configuration Important: All M3DB clusters involved in the cross-cluster replication process must be configured such that they have the exact same:\n Number of shards Replication factor Namespace configuration  The replication feature can be enabled by adding the following configuration to m3dbnode.yml under the db section:\ndb: ... (other configuration) replication: clusters: - name: \u0026#34;some-other-cluster\u0026#34; repairEnabled: true client: config: service: env: \u0026lt;ETCD_ENV\u0026gt; zone: \u0026lt;ETCD_ZONE\u0026gt; service: \u0026lt;ETCD_SERVICE\u0026gt; cacheDir: /var/lib/m3kv etcdClusters: - zone: \u0026lt;ETCD_ZONE\u0026gt; endpoints: - \u0026lt;ETCD_ENDPOINT_01_HOST\u0026gt;:\u0026lt;ETCD_ENDPOINT_01_PORT\u0026gt; Note that the repairEnabled field in the configuration above is independent of the enabled field under the repairs section. For example, the example above will enable replication of data from some-other-cluster but will not perform background repairs within the cluster the M3DB node belongs to.\nHowever, the following configuration:\ndb: ... (other configuration) repair: enabled: true replication: clusters: - name: \u0026#34;some-other-cluster\u0026#34; repairEnabled: true client: config: service: env: \u0026lt;ETCD_ENV\u0026gt; zone: \u0026lt;ETCD_ZONE\u0026gt; service: \u0026lt;ETCD_SERVICE\u0026gt; cacheDir: /var/lib/m3kv etcdClusters: - zone: \u0026lt;ETCD_ZONE\u0026gt; endpoints: - \u0026lt;ETCD_ENDPOINT_01_HOST\u0026gt;:\u0026lt;ETCD_ENDPOINT_01_PORT\u0026gt; would enable both replication of data from some-other-cluster as well as background repairs within the cluster that the M3DB node belongs to.\n"
},
{
	"uri": "/case_studies/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Case Studies "
},
{
	"uri": "/community/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Community How to contact the M3 team? Feel free to contact us through any of the following channels:\n Posting on the M3 Google group Opening issues on the M3 GitHub page Chatting us on the official Slack  GitHub/OSS Our official GitHub page can be found here.\n"
},
{
	"uri": "/ecosystem/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Ecosystem "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]