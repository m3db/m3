{
    "docs": [
        {
            "location": "/", 
            "text": "M3\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nAbout\n\n\nM3 is a metrics platform that is built on M3DB, a distributed timeseries database. The project monorepo can be found \nhere\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#m3", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "M3"
        }, 
        {
            "location": "/#about", 
            "text": "M3 is a metrics platform that is built on M3DB, a distributed timeseries database. The project monorepo can be found  here .", 
            "title": "About"
        }, 
        {
            "location": "/introduction/components/components/", 
            "text": "Components\n\n\nM3 Coordinator\n\n\nM3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and M3DB. It is a bridge that users can deploy to access the benefits of M3DB such as long term storage and multi-DC setup with other monitoring systems, such as Prometheus. See \nthis presentation\n for more on long term storage in Prometheus.\n\n\nM3DB\n\n\nM3DB is a distributed time series database that provides scalable storage and a reverse index of time series. It is optimized as a cost effective and reliable realtime and long term retention metrics store and index.  For more details, see the \nM3DB documentation\n.\n\n\nM3 Query\n\n\nM3 Query is a service that houses a distributed query engine for querying both realtime and historical metrics, supporting several different query languages. It is designed to support both low latency realtime queries and queries that can take longer to execute, aggregating over much larger datasets, for analytical use cases.  For more details, see the \nquery engine documentation\n.\n\n\nM3 Aggregator\n\n\nM3 Aggregator is a service that runs as a dedicated metrics aggregator and provides stream based downsampling, based on dynamic rules stored in etcd. It uses leader election and aggregation window tracking, leveraging etcd to manage this state, to reliably emit at-least-once aggregations for downsampled metrics to long term storage. This provides cost effective and reliable downsampling \n roll up of metrics. These features also reside in the M3 Coordinator, however the dedicated aggregator is sharded and replicated, whereas the M3 Coordinator is not and requires care to deploy and run in a highly available way. There is work remaining to make the aggregator more accessible to users without requiring them to write their own compatible producer and consumer.", 
            "title": "Components"
        }, 
        {
            "location": "/introduction/components/components/#components", 
            "text": "", 
            "title": "Components"
        }, 
        {
            "location": "/introduction/components/components/#m3-coordinator", 
            "text": "M3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and M3DB. It is a bridge that users can deploy to access the benefits of M3DB such as long term storage and multi-DC setup with other monitoring systems, such as Prometheus. See  this presentation  for more on long term storage in Prometheus.", 
            "title": "M3 Coordinator"
        }, 
        {
            "location": "/introduction/components/components/#m3db", 
            "text": "M3DB is a distributed time series database that provides scalable storage and a reverse index of time series. It is optimized as a cost effective and reliable realtime and long term retention metrics store and index.  For more details, see the  M3DB documentation .", 
            "title": "M3DB"
        }, 
        {
            "location": "/introduction/components/components/#m3-query", 
            "text": "M3 Query is a service that houses a distributed query engine for querying both realtime and historical metrics, supporting several different query languages. It is designed to support both low latency realtime queries and queries that can take longer to execute, aggregating over much larger datasets, for analytical use cases.  For more details, see the  query engine documentation .", 
            "title": "M3 Query"
        }, 
        {
            "location": "/introduction/components/components/#m3-aggregator", 
            "text": "M3 Aggregator is a service that runs as a dedicated metrics aggregator and provides stream based downsampling, based on dynamic rules stored in etcd. It uses leader election and aggregation window tracking, leveraging etcd to manage this state, to reliably emit at-least-once aggregations for downsampled metrics to long term storage. This provides cost effective and reliable downsampling   roll up of metrics. These features also reside in the M3 Coordinator, however the dedicated aggregator is sharded and replicated, whereas the M3 Coordinator is not and requires care to deploy and run in a highly available way. There is work remaining to make the aggregator more accessible to users without requiring them to write their own compatible producer and consumer.", 
            "title": "M3 Aggregator"
        }, 
        {
            "location": "/introduction/motivation/motivation/", 
            "text": "Motivation\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nWe decided to open source the M3 platform as a scalable remote storage backend for Prometheus so that others may attempt to reuse our work and avoid building yet another scalable metrics platform. As documentation for Prometheus states, it is limited by single nodes in its scalability and durability. The M3 platform aims to provide a turnkey, scalable, and configurable multi-tenant store for Prometheus and other standard metrics schemas.", 
            "title": "Motivation"
        }, 
        {
            "location": "/introduction/motivation/motivation/#motivation", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.  We decided to open source the M3 platform as a scalable remote storage backend for Prometheus so that others may attempt to reuse our work and avoid building yet another scalable metrics platform. As documentation for Prometheus states, it is limited by single nodes in its scalability and durability. The M3 platform aims to provide a turnkey, scalable, and configurable multi-tenant store for Prometheus and other standard metrics schemas.", 
            "title": "Motivation"
        }, 
        {
            "location": "/m3db/", 
            "text": "M3DB, a distributed time series database\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nAbout\n\n\nM3DB, inspired by \nGorilla\n and \nCassandra\n, is a distributed time series database released as open source by \nUber Technologies\n. It can be used for storing realtime metrics at long retention.\n\n\nHere are some attributes of the project:\n\n\n\n\nDistributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently\n\n\nCluster management built on top of \netcd\n\n\nBuilt-in synchronous replication with configurable durability and read consistency (one, majority, all, etc)\n\n\nM3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy\n\n\nArbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write\n\n\nConfigurable out of order writes, currently limited to the size of the configured time window's block size\n\n\n\n\nCurrent Limitations\n\n\nDue to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.\n\n\nThe project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours).  As such out of order writes are limited to the size of a single compressed time series window.  Consequently backfilling large amounts of data is not currently possible.\n\n\nThe project has also optimized for the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.", 
            "title": "Introduction"
        }, 
        {
            "location": "/m3db/#m3db-a-distributed-time-series-database", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "M3DB, a distributed time series database"
        }, 
        {
            "location": "/m3db/#about", 
            "text": "M3DB, inspired by  Gorilla  and  Cassandra , is a distributed time series database released as open source by  Uber Technologies . It can be used for storing realtime metrics at long retention.  Here are some attributes of the project:   Distributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently  Cluster management built on top of  etcd  Built-in synchronous replication with configurable durability and read consistency (one, majority, all, etc)  M3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy  Arbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write  Configurable out of order writes, currently limited to the size of the configured time window's block size", 
            "title": "About"
        }, 
        {
            "location": "/m3db/#current-limitations", 
            "text": "Due to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.  The project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours).  As such out of order writes are limited to the size of a single compressed time series window.  Consequently backfilling large amounts of data is not currently possible.  The project has also optimized for the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.", 
            "title": "Current Limitations"
        }, 
        {
            "location": "/m3db/architecture/", 
            "text": "Architecture\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nM3DB is written entirely in Go and does not have any required dependencies. For larger deployments, one may use an etcd cluster to manage M3DB cluster membership and topology definition.\n\n\nHigh Level Goals\n\n\nSome of the high level goals for the project are defined as:\n\n\n\n\n\n\nMonitoring support:\n M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.\n\n\n\n\n\n\nHighly configurable:\n Provide a high level of configuration to support a wide set of use cases and runtime environments.\n\n\n\n\n\n\nVariable durability:\n Providing variable durability guarantees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarantees that data was replicated to a quorum of nodes and that the data was durable if desired.", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/#architecture", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "Architecture"
        }, 
        {
            "location": "/m3db/architecture/#overview", 
            "text": "M3DB is written entirely in Go and does not have any required dependencies. For larger deployments, one may use an etcd cluster to manage M3DB cluster membership and topology definition.", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/#high-level-goals", 
            "text": "Some of the high level goals for the project are defined as:    Monitoring support:  M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.    Highly configurable:  Provide a high level of configuration to support a wide set of use cases and runtime environments.    Variable durability:  Providing variable durability guarantees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarantees that data was replicated to a quorum of nodes and that the data was durable if desired.", 
            "title": "High Level Goals"
        }, 
        {
            "location": "/m3db/architecture/engine/", 
            "text": "Storage Engine Overview\n\n\nM3DB is a time series database that was primarily designed to be horizontally scalable and handle a large volume of monitoring time series data.\n\n\nTime Series Compression (M3TSZ)\n\n\nOne of M3DB's biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. This high compression ratio is implemented via the M3TSZ algorithm, a variant of the streaming time series compression algorithm described in \nFacebook's Gorilla paper\n with a few small differences.\n\n\nThe compression ratio will vary depending on the workload and configuration, but we found that with M3TSZ we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber's production workloads. This was a 40% improvement over standard TSZ which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.\n\n\nArchitecture\n\n\nM3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.\n\n\nIn-Memory Object Layout\n\n\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           Database            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n   \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          Namespace 1          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n   \u2502     \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502      \u2502\n   \u2502     \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  Shard 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Series 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [2PM - 4PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [4PM - 6PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   Blocks   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502     Block [6PM - 8PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502 Active Buffers (encoders)  \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n   \u2502                                                                 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nThe in-memory portion of M3DB is implemented via a hierarchy of objects:\n\n\n\n\n\n\nA \ndatabase\n of which there is only one per M3DB process.\n\n\n\n\n\n\nA \ndatabase\n \"owns\" numerous namespaces, and each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize (which we will discuss in more detail later). \nNamespaces\n are similar to tables in other databases.\n\n\n\n\n\n\nShards\n which are owned by \nnamespaces\n. \nShards\n are effectively the same as \"virtual shards\" in Cassandra in that they provide arbitrary distribution of time series data via a simple hash of the series ID.\n\n\n\n\n\n\nSeries\n which are owned by \nshards\n. A \nseries\n is generally what comes to mind when you think of \"time series\" data. Ex. The CPU level for a single host in a datacenter over a period of time could be represented as a series with id \"\n.system.cpu.utilization\" and a vector of tuples in the form of (TIMESTAMP, CPU_LEVEL). In other words, if you were rendering a graph a series would represent a single line on that graph. Note that the previous example is only a logical illustration and does not represent the way that M3DB actually stores data.\n\n\n\n\n\n\nBlocks\n belong to a series and are central to M3DB's design. A \nblock\n is simply a smaller wrapper object around a sealed (no longer writable) stream of compressed time series data. The compression comes with a few caveats though, namely that you cannot read individual datapoints in a compressed block. In other words, in order to read a single datapoint you must decompress the entire block up to the datapoint that you're trying to read.\n\n\n\n\n\n\nIf M3DB kept everything in memory (and in fact, early versions of it did), than you could conceptually think of it as being a composed from a hierarchy of maps:\n\n\ndatabase_obect      =\n map\n\nnamespace_object    =\n map\n\nshard_object        =\n map\n\nseries_object       =\n map\n\nseries_object       =\n map\n (This map should only have one or two entries)\n\n\nPersistent storage\n\n\nWhile in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy then it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.\n\n\nIn addition, with large volumes of data it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \"write-once, read-never\" pattern where less than a few percent of all the data that's stored is ever read. With that type of workload, its wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.\n\n\nLike most other databases, M3DB takes a two-pronged approach to persistant storage that involves combining a commitlog (for disaster recovery) with periodic snapshotting (for efficient retrieval):\n\n\n\n\nAll writes are persisted to a \ncommitlog\n (the commitlog can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commitlog is completely uncompressed and exists only to recover \"unflushed\" data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request.\n\n\nPeriodically (based on the configured blocksize) all \"active\" blocks are \"sealed\" (marked as immutable) and flushed to disk as \n\"fileset\" files\n. These files are highly compressed and can be indexed into via their complementary index files. Check out the \nflushing section\n to learn more about the background flushing process.\n\n\n\n\nThe blocksize parameter is the most important variable that needs to be tuned for your particular workload. A small blocksize will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and your data will take up more space on disk.\n\n\nIf the database is stopped for any reason in-between \"flushes\" (writing fileset files out to disk), then when the node is started back up those writes will need to be recovered by reading the commitlog or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than 1).\n\n\nWhile the \nfileset files\n are designed to support efficient data retrieval via the series primary key (the ID), there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB support various \ncaching policies\n which can significantly improve the performance of reads by caching data in memory.\n\n\nWrite Path\n\n\nWe now have enough context of M3DB's architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the \nwriteBatchRaw\n endpoint on M3DB's embedded thrift server. The write itself will contain the following information:\n\n\n\n\nThe namespace\n\n\nThe series ID (byte blob)\n\n\nThe timestamp\n\n\nThe value itself\n\n\n\n\nM3DB will consult the database object to check if the namespace exists, and if it does,then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then it will lookup the series' corresponding encoder and encode the datapoint into the compressed stream. If the encoder doesn't exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some special logic for handling out-of-order writes which is discussed in the \nmerging all encoders section\n.\n\n\nAt the same time, the write will be appended to the commitlog queue (and depending on the commitlog configuration immediately fsync'd to disk or batched together with other writes and flushed out all at once).\n\n\nThe write will exist only in this \"active buffer\" and the commitlog until the block ends and is flushed to disk, at which point the write will exist in a fileset file for efficient storage and retrieval later and the commitlog entry can be garbage collected.\n\n\nNote:\n Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured \nconsistency level\n.\n\n\nRead Path\n\n\nA read begins when an M3DB client calls the \nFetchBatchResult\n or \nFetchBlocksRawResult\n endpoint on M3DB's embedded thrift server. The read request will contain the following information:\n\n\n\n\nThe namespace\n\n\nThe series ID (byte blob)\n\n\nThe period of time being requested (start and end)\n\n\n\n\nM3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:\n\n\n\n\nDoes the series exist? and if it does\n\n\nDoes the data exist in an \"active buffer\" (actively being compressed by an encoder), cached in-memory, on disk, or some combination of all three?\n\n\n\n\nDetermining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn't, then M3DB consults an in-memory bloom filter(s) for that shard / block start combination(s) to determine if the series exists on disk.\n\n\nIf the series exists, then for every block that the request spans, M3DB needs to consolidate data from the active buffers, in-memory cache, and fileset files (disk).\n\n\nLets imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a blocksize of 2 hours (i.e we need to find 3 different blocks.)\n\n\nIf the current time is 8PM, then the location of the requested blocks might be as follows:\n\n\n[2PM - 4PM (FileSet file)]    - Sealed and flushed block that isn't cached\n[4PM - 6PM (In-memory cache)] - Sealed and flush block that is cached\n[6PM - 8PM (active buffer)]   - Hasn't been sealed or flushed yet\n\n\n\n\nThen M3DB will need to consolidate:\n\n\n\n\nThe not-yet-sealed block from the active buffers / encoders (located inside an internal lookup in the Series object) \n[6PM - 8PM]\n\n\nThe in-memory cached block (also located inside an internal lookup in the Series object) \n[4PM - 6PM]\n\n\nThe block from disk (the block retrieve from disk will then be cached according to the current \ncaching policy\n \n[2PM - 4PM]\n\n\n\n\nRetrieving blocks from the active buffers and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:\n\n\n\n\nConsult the in-memory bloom filter to determine if its possible the series exists on disk.\n\n\nIf the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is \nbefore\n the series ID that we're searching for. Review the \nindex_lookup.go\n file for implementation details.\n\n\nJump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we're looking for \nor\n we get far enough in the index file that it becomes clear that the ID we're looking for doesn't exist (this is possible because the index file is sorted by ID)\n\n\nJump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.\n\n\n\n\nOnce M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or not the client returns a success to the caller for the read is dependent on the configured \nconsistency level\n.\n\n\nNote:\n Since M3DB nodes return compressed blocks (the M3DB client decompresses them), it's not possible to return \"partial results\" for a given block. If any portion of a read request spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.\n\n\nBackground processes\n\n\nM3DB has a variety of processes that run in the background during normal operation.\n\n\nTicking\n\n\nThe ticking process runs continously in the background and is responsible for a variety of tasks:\n\n\n\n\nMerging all encoders for a given series / block start combination\n\n\nRemoving expired / flushed series and blocks from memory\n\n\nCleanup of expired data (fileset/commit log) from the filesystem\n\n\n\n\nMerging all encoders\n\n\nM3TSZ is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For monitoring workloads this works very well because every subsequent datapoint is almost always larger than the previous one. However, real world systems are messy and occassionally out of order writes will be received. When this happens, M3DB will allocate a new encoder for the out of order datapoints. The multiple encoders need to be merged before flushing the data to disk, but to prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.\n\n\nRemoving expired / flushed series and blocks from memory\n\n\nDepending on the configured \ncaching policy\n, the \nin-memory object layout\n can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer need to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.\n\n\nFlushing\n\n\nAs discussed in the \narchitecture\n section, writes are actively buffered / compressed in-memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of \nfileset files\n to facilitate efficient storage and retrieval.\n\n\nThis is where the configurable \"blocksize\" comes into play. The blocksize is simply a duration of time that dictates how long active writes will be compressed (in a streaming manner) in memory before being \"sealed\" (marked as immutable) and flushed to disk. Lets use a blocksize of two hours as an example.\n\n\nIf the blocksize is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the \nfileset files\n will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.\n\n\nCaveats / Limitations\n\n\n\n\nM3DB currently supports exact ID based lookups. It does not support tag/secondary indexing. This feature is under development and future versions of M3DB will have support for a built-in reverse index.\n\n\nM3DB does not support updates / deletes. All data written to M3DB is immutable.\n\n\nM3DB does not support writing arbitrarily into the past and future. This is generally fine for monitoring workloads, but can be problematic for traditional \nOLTP\n and \nOLAP\n workloads. Future versions of M3DB will have better support for writes with arbitrary timestamps.\n\n\nM3DB does not support writing datapoints with values other than double-precision floats. Future versions of M3DB will have support for storing arbitrary values.\n\n\nM3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value (Uber has production databases running with retention periods as high as 5 years), its still required and generally speaking M3DB is optimized for workloads with a well-defined \nTTL\n.\n\n\nM3DB does not support either background data repair or Cassandra-style \nread repairs\n. Future versions of M3DB will support automatic repairs of data as an ongoing background process.", 
            "title": "Storage Engine"
        }, 
        {
            "location": "/m3db/architecture/engine/#storage-engine-overview", 
            "text": "M3DB is a time series database that was primarily designed to be horizontally scalable and handle a large volume of monitoring time series data.", 
            "title": "Storage Engine Overview"
        }, 
        {
            "location": "/m3db/architecture/engine/#time-series-compression-m3tsz", 
            "text": "One of M3DB's biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. This high compression ratio is implemented via the M3TSZ algorithm, a variant of the streaming time series compression algorithm described in  Facebook's Gorilla paper  with a few small differences.  The compression ratio will vary depending on the workload and configuration, but we found that with M3TSZ we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber's production workloads. This was a 40% improvement over standard TSZ which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.", 
            "title": "Time Series Compression (M3TSZ)"
        }, 
        {
            "location": "/m3db/architecture/engine/#architecture", 
            "text": "M3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.", 
            "title": "Architecture"
        }, 
        {
            "location": "/m3db/architecture/engine/#in-memory-object-layout", 
            "text": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           Database            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n   \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          Namespace 1          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n   \u2502     \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502      \u2502\n   \u2502     \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  Shard 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Series 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [2PM - 4PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [4PM - 6PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   Blocks   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502     Block [6PM - 8PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502 Active Buffers (encoders)  \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n   \u2502                                                                 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  The in-memory portion of M3DB is implemented via a hierarchy of objects:    A  database  of which there is only one per M3DB process.    A  database  \"owns\" numerous namespaces, and each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize (which we will discuss in more detail later).  Namespaces  are similar to tables in other databases.    Shards  which are owned by  namespaces .  Shards  are effectively the same as \"virtual shards\" in Cassandra in that they provide arbitrary distribution of time series data via a simple hash of the series ID.    Series  which are owned by  shards . A  series  is generally what comes to mind when you think of \"time series\" data. Ex. The CPU level for a single host in a datacenter over a period of time could be represented as a series with id \" .system.cpu.utilization\" and a vector of tuples in the form of (TIMESTAMP, CPU_LEVEL). In other words, if you were rendering a graph a series would represent a single line on that graph. Note that the previous example is only a logical illustration and does not represent the way that M3DB actually stores data.    Blocks  belong to a series and are central to M3DB's design. A  block  is simply a smaller wrapper object around a sealed (no longer writable) stream of compressed time series data. The compression comes with a few caveats though, namely that you cannot read individual datapoints in a compressed block. In other words, in order to read a single datapoint you must decompress the entire block up to the datapoint that you're trying to read.    If M3DB kept everything in memory (and in fact, early versions of it did), than you could conceptually think of it as being a composed from a hierarchy of maps:  database_obect      =  map \nnamespace_object    =  map \nshard_object        =  map \nseries_object       =  map \nseries_object       =  map  (This map should only have one or two entries)", 
            "title": "In-Memory Object Layout"
        }, 
        {
            "location": "/m3db/architecture/engine/#persistent-storage", 
            "text": "While in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy then it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.  In addition, with large volumes of data it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \"write-once, read-never\" pattern where less than a few percent of all the data that's stored is ever read. With that type of workload, its wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.  Like most other databases, M3DB takes a two-pronged approach to persistant storage that involves combining a commitlog (for disaster recovery) with periodic snapshotting (for efficient retrieval):   All writes are persisted to a  commitlog  (the commitlog can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commitlog is completely uncompressed and exists only to recover \"unflushed\" data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request.  Periodically (based on the configured blocksize) all \"active\" blocks are \"sealed\" (marked as immutable) and flushed to disk as  \"fileset\" files . These files are highly compressed and can be indexed into via their complementary index files. Check out the  flushing section  to learn more about the background flushing process.   The blocksize parameter is the most important variable that needs to be tuned for your particular workload. A small blocksize will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and your data will take up more space on disk.  If the database is stopped for any reason in-between \"flushes\" (writing fileset files out to disk), then when the node is started back up those writes will need to be recovered by reading the commitlog or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than 1).  While the  fileset files  are designed to support efficient data retrieval via the series primary key (the ID), there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB support various  caching policies  which can significantly improve the performance of reads by caching data in memory.", 
            "title": "Persistent storage"
        }, 
        {
            "location": "/m3db/architecture/engine/#write-path", 
            "text": "We now have enough context of M3DB's architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the  writeBatchRaw  endpoint on M3DB's embedded thrift server. The write itself will contain the following information:   The namespace  The series ID (byte blob)  The timestamp  The value itself   M3DB will consult the database object to check if the namespace exists, and if it does,then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then it will lookup the series' corresponding encoder and encode the datapoint into the compressed stream. If the encoder doesn't exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some special logic for handling out-of-order writes which is discussed in the  merging all encoders section .  At the same time, the write will be appended to the commitlog queue (and depending on the commitlog configuration immediately fsync'd to disk or batched together with other writes and flushed out all at once).  The write will exist only in this \"active buffer\" and the commitlog until the block ends and is flushed to disk, at which point the write will exist in a fileset file for efficient storage and retrieval later and the commitlog entry can be garbage collected.  Note:  Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured  consistency level .", 
            "title": "Write Path"
        }, 
        {
            "location": "/m3db/architecture/engine/#read-path", 
            "text": "A read begins when an M3DB client calls the  FetchBatchResult  or  FetchBlocksRawResult  endpoint on M3DB's embedded thrift server. The read request will contain the following information:   The namespace  The series ID (byte blob)  The period of time being requested (start and end)   M3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:   Does the series exist? and if it does  Does the data exist in an \"active buffer\" (actively being compressed by an encoder), cached in-memory, on disk, or some combination of all three?   Determining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn't, then M3DB consults an in-memory bloom filter(s) for that shard / block start combination(s) to determine if the series exists on disk.  If the series exists, then for every block that the request spans, M3DB needs to consolidate data from the active buffers, in-memory cache, and fileset files (disk).  Lets imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a blocksize of 2 hours (i.e we need to find 3 different blocks.)  If the current time is 8PM, then the location of the requested blocks might be as follows:  [2PM - 4PM (FileSet file)]    - Sealed and flushed block that isn't cached\n[4PM - 6PM (In-memory cache)] - Sealed and flush block that is cached\n[6PM - 8PM (active buffer)]   - Hasn't been sealed or flushed yet  Then M3DB will need to consolidate:   The not-yet-sealed block from the active buffers / encoders (located inside an internal lookup in the Series object)  [6PM - 8PM]  The in-memory cached block (also located inside an internal lookup in the Series object)  [4PM - 6PM]  The block from disk (the block retrieve from disk will then be cached according to the current  caching policy   [2PM - 4PM]   Retrieving blocks from the active buffers and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:   Consult the in-memory bloom filter to determine if its possible the series exists on disk.  If the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is  before  the series ID that we're searching for. Review the  index_lookup.go  file for implementation details.  Jump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we're looking for  or  we get far enough in the index file that it becomes clear that the ID we're looking for doesn't exist (this is possible because the index file is sorted by ID)  Jump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.   Once M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or not the client returns a success to the caller for the read is dependent on the configured  consistency level .  Note:  Since M3DB nodes return compressed blocks (the M3DB client decompresses them), it's not possible to return \"partial results\" for a given block. If any portion of a read request spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.", 
            "title": "Read Path"
        }, 
        {
            "location": "/m3db/architecture/engine/#background-processes", 
            "text": "M3DB has a variety of processes that run in the background during normal operation.", 
            "title": "Background processes"
        }, 
        {
            "location": "/m3db/architecture/engine/#ticking", 
            "text": "The ticking process runs continously in the background and is responsible for a variety of tasks:   Merging all encoders for a given series / block start combination  Removing expired / flushed series and blocks from memory  Cleanup of expired data (fileset/commit log) from the filesystem", 
            "title": "Ticking"
        }, 
        {
            "location": "/m3db/architecture/engine/#merging-all-encoders", 
            "text": "M3TSZ is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For monitoring workloads this works very well because every subsequent datapoint is almost always larger than the previous one. However, real world systems are messy and occassionally out of order writes will be received. When this happens, M3DB will allocate a new encoder for the out of order datapoints. The multiple encoders need to be merged before flushing the data to disk, but to prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.", 
            "title": "Merging all encoders"
        }, 
        {
            "location": "/m3db/architecture/engine/#removing-expired-flushed-series-and-blocks-from-memory", 
            "text": "Depending on the configured  caching policy , the  in-memory object layout  can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer need to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.", 
            "title": "Removing expired / flushed series and blocks from memory"
        }, 
        {
            "location": "/m3db/architecture/engine/#flushing", 
            "text": "As discussed in the  architecture  section, writes are actively buffered / compressed in-memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of  fileset files  to facilitate efficient storage and retrieval.  This is where the configurable \"blocksize\" comes into play. The blocksize is simply a duration of time that dictates how long active writes will be compressed (in a streaming manner) in memory before being \"sealed\" (marked as immutable) and flushed to disk. Lets use a blocksize of two hours as an example.  If the blocksize is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the  fileset files  will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.", 
            "title": "Flushing"
        }, 
        {
            "location": "/m3db/architecture/engine/#caveats-limitations", 
            "text": "M3DB currently supports exact ID based lookups. It does not support tag/secondary indexing. This feature is under development and future versions of M3DB will have support for a built-in reverse index.  M3DB does not support updates / deletes. All data written to M3DB is immutable.  M3DB does not support writing arbitrarily into the past and future. This is generally fine for monitoring workloads, but can be problematic for traditional  OLTP  and  OLAP  workloads. Future versions of M3DB will have better support for writes with arbitrary timestamps.  M3DB does not support writing datapoints with values other than double-precision floats. Future versions of M3DB will have support for storing arbitrary values.  M3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value (Uber has production databases running with retention periods as high as 5 years), its still required and generally speaking M3DB is optimized for workloads with a well-defined  TTL .  M3DB does not support either background data repair or Cassandra-style  read repairs . Future versions of M3DB will support automatic repairs of data as an ongoing background process.", 
            "title": "Caveats / Limitations"
        }, 
        {
            "location": "/m3db/architecture/sharding/", 
            "text": "Sharding\n\n\nTimeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default \nmurmur3\n is used as the hashing function and 4096 virtual shards are configured.\n\n\nBenefits\n\n\nShards provide a variety of benefits throughout the M3DB stack:\n\n\n\n\nThey make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level.\n\n\nThey provide more fine grained lock granularity at the memory level.\n\n\nThey inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.\n\n\n\n\nReplication\n\n\nLogical shards are placed per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data is distinct to the racks that locate all other replicas\u2019 data.\n\n\nReplication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.\n\n\nReplica\n\n\nEach replica has its own assignment of a single logical shard per virtual shard.\n\n\nConceptually it can be defined as:\n\n\nReplica {\n  id uint32\n  shards []Shard\n}\n\n\n\n\nShard state\n\n\nEach shard can be conceptually defined as:\n\n\nShard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}\n\n\n\n\nShard assignment\n\n\nThe assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.\n\n\nFor a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as \nLEAVING\n and another host assigned as \nINITIALIZING\n for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only \nAVAILABLE\n shards count towards consistency, the work to group the \nLEAVING\n and \nINITIALIZING\n shards together when calculating a write success/error is not complete, see \nissue 417\n.\n\n\nIt is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the \nINITIALIZING\n state and to transition the state to \nAVAILABLE\n once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the \nLEAVING\n shard still assigned to the node that previously owned it and transitions the shard state on the new node from \nINITIALIZING\n state to \nAVAILABLE\n.\n\n\nNodes will not start serving reads for the new shard until it is \nAVAILABLE\n, meaning not until they have bootstrapped data for those shards.\n\n\nCluster operations\n\n\nNode add\n\n\nWhen a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become \nINITIALIZING\n, the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as \nLEAVING\n.\n\n\nNode down\n\n\nA node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.\n\n\nNode remove\n\n\nWhen a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are \nINITIALIZING\n and need to be bootstrapped and will begin bootstrapping the data using all replicas available.", 
            "title": "Sharding and Replication"
        }, 
        {
            "location": "/m3db/architecture/sharding/#sharding", 
            "text": "Timeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default  murmur3  is used as the hashing function and 4096 virtual shards are configured.", 
            "title": "Sharding"
        }, 
        {
            "location": "/m3db/architecture/sharding/#benefits", 
            "text": "Shards provide a variety of benefits throughout the M3DB stack:   They make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level.  They provide more fine grained lock granularity at the memory level.  They inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.", 
            "title": "Benefits"
        }, 
        {
            "location": "/m3db/architecture/sharding/#replication", 
            "text": "Logical shards are placed per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data is distinct to the racks that locate all other replicas\u2019 data.  Replication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.", 
            "title": "Replication"
        }, 
        {
            "location": "/m3db/architecture/sharding/#replica", 
            "text": "Each replica has its own assignment of a single logical shard per virtual shard.  Conceptually it can be defined as:  Replica {\n  id uint32\n  shards []Shard\n}", 
            "title": "Replica"
        }, 
        {
            "location": "/m3db/architecture/sharding/#shard-state", 
            "text": "Each shard can be conceptually defined as:  Shard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}", 
            "title": "Shard state"
        }, 
        {
            "location": "/m3db/architecture/sharding/#shard-assignment", 
            "text": "The assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.  For a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as  LEAVING  and another host assigned as  INITIALIZING  for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only  AVAILABLE  shards count towards consistency, the work to group the  LEAVING  and  INITIALIZING  shards together when calculating a write success/error is not complete, see  issue 417 .  It is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the  INITIALIZING  state and to transition the state to  AVAILABLE  once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the  LEAVING  shard still assigned to the node that previously owned it and transitions the shard state on the new node from  INITIALIZING  state to  AVAILABLE .  Nodes will not start serving reads for the new shard until it is  AVAILABLE , meaning not until they have bootstrapped data for those shards.", 
            "title": "Shard assignment"
        }, 
        {
            "location": "/m3db/architecture/sharding/#cluster-operations", 
            "text": "", 
            "title": "Cluster operations"
        }, 
        {
            "location": "/m3db/architecture/sharding/#node-add", 
            "text": "When a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become  INITIALIZING , the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as  LEAVING .", 
            "title": "Node add"
        }, 
        {
            "location": "/m3db/architecture/sharding/#node-down", 
            "text": "A node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.", 
            "title": "Node down"
        }, 
        {
            "location": "/m3db/architecture/sharding/#node-remove", 
            "text": "When a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are  INITIALIZING  and need to be bootstrapped and will begin bootstrapping the data using all replicas available.", 
            "title": "Node remove"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/", 
            "text": "Consistency Levels\n\n\nM3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.\n\n\nWrite consistency levels\n\n\n\n\n\n\nOne:\n Corresponds to a single node succeeding for an operation to succeed.\n\n\n\n\n\n\nMajority:\n Corresponds to the majority of nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nAll:\n Corresponds to all nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nRead consistency levels\n\n\n\n\n\n\nOne\n: Corresponds to reading from a single node to designate success.\n\n\n\n\n\n\nUnstrictMajority\n: Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.\n\n\n\n\n\n\nMajority\n: Corresponds to reading from the majority of nodes to designate success.\n\n\n\n\n\n\nAll:\n Corresponds to reading from all of the nodes to designate success.\n\n\n\n\n\n\nConnect consistency levels\n\n\nConnect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.\n\n\n\n\n\n\nAny:\n Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.\n\n\n\n\n\n\nNone:\n Corresponds to connecting to no nodes for all shards and as such will always succeed.\n\n\n\n\n\n\nOne:\n Corresponds to connecting to a single node for all shards.\n\n\n\n\n\n\nMajority:\n Corresponds to connecting to the majority of nodes for all shards.\n\n\n\n\n\n\nAll:\n Corresponds to connecting to all of the nodes for all shards.", 
            "title": "Consistency Levels"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/#consistency-levels", 
            "text": "M3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.", 
            "title": "Consistency Levels"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/#write-consistency-levels", 
            "text": "One:  Corresponds to a single node succeeding for an operation to succeed.    Majority:  Corresponds to the majority of nodes succeeding for an operation to succeed.    All:  Corresponds to all nodes succeeding for an operation to succeed.", 
            "title": "Write consistency levels"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/#read-consistency-levels", 
            "text": "One : Corresponds to reading from a single node to designate success.    UnstrictMajority : Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.    Majority : Corresponds to reading from the majority of nodes to designate success.    All:  Corresponds to reading from all of the nodes to designate success.", 
            "title": "Read consistency levels"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/#connect-consistency-levels", 
            "text": "Connect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.    Any:  Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.    None:  Corresponds to connecting to no nodes for all shards and as such will always succeed.    One:  Corresponds to connecting to a single node for all shards.    Majority:  Corresponds to connecting to the majority of nodes for all shards.    All:  Corresponds to connecting to all of the nodes for all shards.", 
            "title": "Connect consistency levels"
        }, 
        {
            "location": "/m3db/architecture/storage/", 
            "text": "Storage\n\n\nOverview\n\n\nThe primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.\n\n\nThey are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)\n\n\nFileSets\n\n\nA fileset has the following files:\n\n\n\n\nInfo file:\n Stores the block time window start and size and other important metadata about the fileset volume.\n\n\nSummaries file:\n Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.\n\n\nIndex file:\n Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval.\n\n\nData file:\n Stores the series compressed data streams.\n\n\nBloom filter file:\n Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.\n\n\nDigests file:\n Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.\n\n\nCheckpoint file:\n Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.\n\n\n\n\n                                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File      \u2502\n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500\n\u2502- Idx                \u2502\n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                 \u2502\n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size               \u2502\n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum           \u2502\n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset  \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2502- Encoded Tags       |  |\n\u2502- Snapshot Time      \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502- Type (Flush/Snap)  \u2502                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n                                                                              \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502\n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502\n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502\n\u2502- Index digest       \u2502                           \u2514\u2500\n\u2502  - Marker (16 bytes)\u2502\n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502\n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checkpoint File   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Digests digest     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.\n\n\nFileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.", 
            "title": "Storage"
        }, 
        {
            "location": "/m3db/architecture/storage/#storage", 
            "text": "", 
            "title": "Storage"
        }, 
        {
            "location": "/m3db/architecture/storage/#overview", 
            "text": "The primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.  They are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/storage/#filesets", 
            "text": "A fileset has the following files:   Info file:  Stores the block time window start and size and other important metadata about the fileset volume.  Summaries file:  Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.  Index file:  Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval.  Data file:  Stores the series compressed data streams.  Bloom filter file:  Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.  Digests file:  Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.  Checkpoint file:  Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File      \u2502\n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500 \u2502- Idx                \u2502\n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                 \u2502\n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size               \u2502\n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum           \u2502\n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset  \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2502- Encoded Tags       |  |\n\u2502- Snapshot Time      \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502- Type (Flush/Snap)  \u2502                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n                                                                              \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502\n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502\n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502\n\u2502- Index digest       \u2502                           \u2514\u2500 \u2502  - Marker (16 bytes)\u2502\n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502\n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checkpoint File   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Digests digest     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.  FileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.", 
            "title": "FileSets"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/", 
            "text": "Commit Logs\n\n\nOverview\n\n\nM3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)\n\n\nIntegrity Levels\n\n\nThere are two integrity levels available for commit logs:\n\n\n\n\nSynchronous:\n write operations must wait until it has finished writing an entry in the commit log to complete.\n\n\nBehind:\n write operations must finish enqueueing an entry to the commit log write queue to complete.\n\n\n\n\nDepending on the data loss requirements users can choose either integrity level.\n\n\nProperties\n\n\nCommit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.\n\n\nStructure\n\n\nCommit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.\n\n\nThe structures can be conceptually described as:\n\n\nCommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}\n\n\n\n\nGarbage Collected\n\n\nCommit logs are garbage collected after all blocks within the retention period in which data inside the commit logs could be applicable have already been flushed to disk as immutable compressed filesets.\n\n\nCompaction\n\n\nThere is currently no compaction process for commitlogs. They are deleted once they fall out of their configurable retention period \nor\n all the \nfileset files\n for that period are flushed.", 
            "title": "Commit Logs"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#commit-logs", 
            "text": "", 
            "title": "Commit Logs"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#overview", 
            "text": "M3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#integrity-levels", 
            "text": "There are two integrity levels available for commit logs:   Synchronous:  write operations must wait until it has finished writing an entry in the commit log to complete.  Behind:  write operations must finish enqueueing an entry to the commit log write queue to complete.   Depending on the data loss requirements users can choose either integrity level.", 
            "title": "Integrity Levels"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#properties", 
            "text": "Commit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.", 
            "title": "Properties"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#structure", 
            "text": "Commit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.  The structures can be conceptually described as:  CommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}", 
            "title": "Structure"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#garbage-collected", 
            "text": "Commit logs are garbage collected after all blocks within the retention period in which data inside the commit logs could be applicable have already been flushed to disk as immutable compressed filesets.", 
            "title": "Garbage Collected"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#compaction", 
            "text": "There is currently no compaction process for commitlogs. They are deleted once they fall out of their configurable retention period  or  all the  fileset files  for that period are flushed.", 
            "title": "Compaction"
        }, 
        {
            "location": "/m3db/architecture/peer_streaming/", 
            "text": "Peer Streaming\n\n\nClient\n\n\nPeer streaming is managed by the M3DB client.  It fetches all blocks from peers for a specified time range for bootstrapping purposes.  It performs the following steps:\n\n\n\n\nFetch all metadata for blocks from all peers who own the specified shard\n\n\nCompares metadata from different peers and determines the best peer(s) from which to stream the actual data\n\n\nStreams the block data from peers\n\n\n\n\nSteps 1, 2 and 3 all happen concurrently.  As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata.  If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from.  If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.\n\n\nIn terms of error handling, the client will respect the consistency level specified for bootstrap.  This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata.  For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \"none\" is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers.  Fetching block data as well will indefinitely retry until the consistency level is achieved.\n\n\nThe client supports dynamically changing the bootstrap consistency level, which is helfpul in disaster scenarios where the consistency level cannot be achieved.  To break the indefinite streaming attempt an operator can change the consistency level to \"none\" and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.\n\n\nThe diagram below depicts the control flow and concurrency (goroutines and channels) in detail:\n\n\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                                               \u2502\n             \u2502         FetchBootstrapBlocksFromPeers         \u2502\n             \u2502                                               \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Main routine          \u2502\n\u2502                               \u2502\n\u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2) Spin up background routine \u2502                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      Create with metadataCh\n                \u2502                                \u2502\n                \u2502                                \u25bc\n                \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                               \u2502\n                \u2502                \u2502      Background routine       \u2502\n                \u2502                \u2502                               \u2502\n                \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                \u2502\n                \u2502                          For each peer\n                \u2502                                \u2502\n                \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u25bc                \u25bc                 \u25bc\n                \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502       StreamBlocksMetadataFromPeer        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502  Stream paginated blocks metadata from a  \u2502\n                \u2502          \u2502        peer while pageToken != nil        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502     For each blocks' metadata --\n put     \u2502\n                \u2502          \u2502         metadata into metadataCh          \u2502\n                \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           StreamBlocksFromPeers           \u2502\n\u2502                                           \u2502\n\u2502 1) Create a background goroutine (details \u2502\n\u2502               to the right)               \u2502\n\u2502                                           \u2502\n\u2502 2) Create a queue per-peer which each have\u2502\n\u2502   their own internal goroutine and will   \u2502\n\u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              specific peer                \u2502          \u2502\n\u2502                                           \u2502          \u2502\n\u2502 3) Loop through the enqueCh and pick an   \u2502 Creates with metadataCh\n\u2502appropriate peer(s) for each series (based \u2502     and enqueueCh\n\u2502on whether all the peers have the same data\u2502          \u2502\n\u2502 or not) and then put that into the queue  \u2502          \u2502\n\u2502for that peer so the data will be streamed \u2502          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n                \u2502                                      \u25bc\n                \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n                \u2502    \u2502                streamMetadataFn variable)                \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n                \u2502    \u2502per series/block combination from different peers until we\u2502\n                \u2502    \u2502   have them from all peers for a series/block metadata   \u2502\n                \u2502    \u2502   combination and then \nsubmit\n them to the enqueueCh    \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502At the end, flush any remaining series/block combinations \u2502\n                \u2502    \u2502(that we received from less than N peers) into the enqueCh\u2502\n                \u2502    \u2502                         as well.                         \u2502\n                \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n          For each peer\n                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502            \u2502             \u2502\n   \u2502            \u2502             \u2502\n   \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Peer Streaming"
        }, 
        {
            "location": "/m3db/architecture/peer_streaming/#peer-streaming", 
            "text": "", 
            "title": "Peer Streaming"
        }, 
        {
            "location": "/m3db/architecture/peer_streaming/#client", 
            "text": "Peer streaming is managed by the M3DB client.  It fetches all blocks from peers for a specified time range for bootstrapping purposes.  It performs the following steps:   Fetch all metadata for blocks from all peers who own the specified shard  Compares metadata from different peers and determines the best peer(s) from which to stream the actual data  Streams the block data from peers   Steps 1, 2 and 3 all happen concurrently.  As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata.  If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from.  If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.  In terms of error handling, the client will respect the consistency level specified for bootstrap.  This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata.  For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \"none\" is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers.  Fetching block data as well will indefinitely retry until the consistency level is achieved.  The client supports dynamically changing the bootstrap consistency level, which is helfpul in disaster scenarios where the consistency level cannot be achieved.  To break the indefinite streaming attempt an operator can change the consistency level to \"none\" and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.  The diagram below depicts the control flow and concurrency (goroutines and channels) in detail:               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                                               \u2502\n             \u2502         FetchBootstrapBlocksFromPeers         \u2502\n             \u2502                                               \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Main routine          \u2502\n\u2502                               \u2502\n\u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2) Spin up background routine \u2502                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      Create with metadataCh\n                \u2502                                \u2502\n                \u2502                                \u25bc\n                \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                               \u2502\n                \u2502                \u2502      Background routine       \u2502\n                \u2502                \u2502                               \u2502\n                \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                \u2502\n                \u2502                          For each peer\n                \u2502                                \u2502\n                \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u25bc                \u25bc                 \u25bc\n                \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502       StreamBlocksMetadataFromPeer        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502  Stream paginated blocks metadata from a  \u2502\n                \u2502          \u2502        peer while pageToken != nil        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502     For each blocks' metadata --  put     \u2502\n                \u2502          \u2502         metadata into metadataCh          \u2502\n                \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           StreamBlocksFromPeers           \u2502\n\u2502                                           \u2502\n\u2502 1) Create a background goroutine (details \u2502\n\u2502               to the right)               \u2502\n\u2502                                           \u2502\n\u2502 2) Create a queue per-peer which each have\u2502\n\u2502   their own internal goroutine and will   \u2502\n\u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              specific peer                \u2502          \u2502\n\u2502                                           \u2502          \u2502\n\u2502 3) Loop through the enqueCh and pick an   \u2502 Creates with metadataCh\n\u2502appropriate peer(s) for each series (based \u2502     and enqueueCh\n\u2502on whether all the peers have the same data\u2502          \u2502\n\u2502 or not) and then put that into the queue  \u2502          \u2502\n\u2502for that peer so the data will be streamed \u2502          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n                \u2502                                      \u25bc\n                \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n                \u2502    \u2502                streamMetadataFn variable)                \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n                \u2502    \u2502per series/block combination from different peers until we\u2502\n                \u2502    \u2502   have them from all peers for a series/block metadata   \u2502\n                \u2502    \u2502   combination and then  submit  them to the enqueueCh    \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502At the end, flush any remaining series/block combinations \u2502\n                \u2502    \u2502(that we received from less than N peers) into the enqueCh\u2502\n                \u2502    \u2502                         as well.                         \u2502\n                \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n          For each peer\n                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502            \u2502             \u2502\n   \u2502            \u2502             \u2502\n   \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Client"
        }, 
        {
            "location": "/m3db/architecture/caching/", 
            "text": "Caching policies\n\n\nOverview\n\n\nBlocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don't need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \"cache\" itself is not a separate datastructure in memory, cached blocks are simply stored in their respective \nin-memory objects\n with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.\n\n\nFor general purpose workloads, the \nlru\n caching policy is reccommended.\n\n\nNone Cache Policy\n\n\nThe \nnone\n cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.\n\n\nAll Cache Policy\n\n\nThe \nall\n cache policy is the opposite of the \nnone\n cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.\n\n\nRecently Read Cache Policy\n\n\nThe \nrecently_read\n cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the \nrecently_read\n cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.\n\n\nData eviction from memory is triggered by the \"ticking\" process described in the \nbackground processes section\n\n\nLeast Recently Used (LRU) Cache Policy\n\n\nThe \nlru\n cache policy uses an \nlru\n list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in \nwired_list.go\n for implementation details.", 
            "title": "Caching"
        }, 
        {
            "location": "/m3db/architecture/caching/#caching-policies", 
            "text": "", 
            "title": "Caching policies"
        }, 
        {
            "location": "/m3db/architecture/caching/#overview", 
            "text": "Blocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don't need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \"cache\" itself is not a separate datastructure in memory, cached blocks are simply stored in their respective  in-memory objects  with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.  For general purpose workloads, the  lru  caching policy is reccommended.", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/caching/#none-cache-policy", 
            "text": "The  none  cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.", 
            "title": "None Cache Policy"
        }, 
        {
            "location": "/m3db/architecture/caching/#all-cache-policy", 
            "text": "The  all  cache policy is the opposite of the  none  cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.", 
            "title": "All Cache Policy"
        }, 
        {
            "location": "/m3db/architecture/caching/#recently-read-cache-policy", 
            "text": "The  recently_read  cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the  recently_read  cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.  Data eviction from memory is triggered by the \"ticking\" process described in the  background processes section", 
            "title": "Recently Read Cache Policy"
        }, 
        {
            "location": "/m3db/architecture/caching/#least-recently-used-lru-cache-policy", 
            "text": "The  lru  cache policy uses an  lru  list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in  wired_list.go  for implementation details.", 
            "title": "Least Recently Used (LRU) Cache Policy"
        }, 
        {
            "location": "/query_engine/", 
            "text": "M3 Query, a distributed query engine for M3DB and Prometheus\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.", 
            "title": "Introduction"
        }, 
        {
            "location": "/query_engine/#m3-query-a-distributed-query-engine-for-m3db-and-prometheus", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "M3 Query, a distributed query engine for M3DB and Prometheus"
        }, 
        {
            "location": "/query_engine/roadmap/", 
            "text": "Roadmap\n\n\nLaunch M3 Coordinator as a bridge for the Read/Write path of M3DB into open source (Q1 2018)\n\n\nV1 (late Q4 2017 - early Q1 2018)\n\n\n\n\nCreate a gRPC/Protobuf service.\n\n\nHandlers for Prometheus remote read/write endpoints.\n\n\nPerform fanout to M3DB nodes.\n\n\nTooling to set up M3 Coordinator alongside Prometheus.\n\n\n\n\nV2 (late Q1 2018)\n\n\n\n\nSupport cross datacenter calls with remote aggregations.\n\n\nBenchmark the performance of the coordinator and M3DB using popular datasets.\n\n\nSupport for multiple M3DB clusters.\n\n\n\n\nM3 Query and optimizations for M3 Coordinator (Q2 2018)\n\n\n\n\nDedicated query engine service, M3 Query.\n\n\nSupport authentication and rate limiting.\n\n\nCost accounting per query and memory management to prevent M3 Query and M3 Coordinator from going OOM.\n\n\nPush computation to storage nodes whenever possible.\n\n\nExecution state manager to keep track of running queries.\n\n\nPort the distributed computation to M3 Query service.\n\n\n\n\nDashboards can directly interact with M3 Query or M3 Coordinator to get data from M3DB (Q3-Q4 2018)\n\n\n\n\nWrite a PromQL parser.\n\n\nWrite the current M3QL interfaces to conform to the common DAG structure.\n\n\nSuggest auto aggregation rules.\n\n\nProvide advanced query tracking to figure out bottlenecks.", 
            "title": "Roadmap"
        }, 
        {
            "location": "/query_engine/roadmap/#roadmap", 
            "text": "", 
            "title": "Roadmap"
        }, 
        {
            "location": "/query_engine/roadmap/#launch-m3-coordinator-as-a-bridge-for-the-readwrite-path-of-m3db-into-open-source-q1-2018", 
            "text": "", 
            "title": "Launch M3 Coordinator as a bridge for the Read/Write path of M3DB into open source (Q1 2018)"
        }, 
        {
            "location": "/query_engine/roadmap/#v1-late-q4-2017-early-q1-2018", 
            "text": "Create a gRPC/Protobuf service.  Handlers for Prometheus remote read/write endpoints.  Perform fanout to M3DB nodes.  Tooling to set up M3 Coordinator alongside Prometheus.", 
            "title": "V1 (late Q4 2017 - early Q1 2018)"
        }, 
        {
            "location": "/query_engine/roadmap/#v2-late-q1-2018", 
            "text": "Support cross datacenter calls with remote aggregations.  Benchmark the performance of the coordinator and M3DB using popular datasets.  Support for multiple M3DB clusters.", 
            "title": "V2 (late Q1 2018)"
        }, 
        {
            "location": "/query_engine/roadmap/#m3-query-and-optimizations-for-m3-coordinator-q2-2018", 
            "text": "Dedicated query engine service, M3 Query.  Support authentication and rate limiting.  Cost accounting per query and memory management to prevent M3 Query and M3 Coordinator from going OOM.  Push computation to storage nodes whenever possible.  Execution state manager to keep track of running queries.  Port the distributed computation to M3 Query service.", 
            "title": "M3 Query and optimizations for M3 Coordinator (Q2 2018)"
        }, 
        {
            "location": "/query_engine/roadmap/#dashboards-can-directly-interact-with-m3-query-or-m3-coordinator-to-get-data-from-m3db-q3-q4-2018", 
            "text": "Write a PromQL parser.  Write the current M3QL interfaces to conform to the common DAG structure.  Suggest auto aggregation rules.  Provide advanced query tracking to figure out bottlenecks.", 
            "title": "Dashboards can directly interact with M3 Query or M3 Coordinator to get data from M3DB (Q3-Q4 2018)"
        }, 
        {
            "location": "/query_engine/architecture/", 
            "text": "Architecture\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nM3 Query and M3 Coordinator are written entirely in Go, M3 Query is as a query engine for \nM3DB\n and M3 Coordinator is a remote read/write endpoint for Prometheus and M3DB. To learn more about Prometheus's remote endpoints and storage, \nsee here\n.", 
            "title": "Overview"
        }, 
        {
            "location": "/query_engine/architecture/#architecture", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "Architecture"
        }, 
        {
            "location": "/query_engine/architecture/#overview", 
            "text": "M3 Query and M3 Coordinator are written entirely in Go, M3 Query is as a query engine for  M3DB  and M3 Coordinator is a remote read/write endpoint for Prometheus and M3DB. To learn more about Prometheus's remote endpoints and storage,  see here .", 
            "title": "Overview"
        }, 
        {
            "location": "/query_engine/architecture/blocks/", 
            "text": "Blocks\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nThe fundamental data structures that M3 Query uses are \nBlocks\n. \nBlocks\n are what get created from the series iterators that M3DB returns. A \nBlock\n is associated with a start and end time. It contains data from multiple time series stored in columnar format.\n\n\nMost transformations within M3 Query will be applied across different series for each time interval. Therefore, having data stored in columnar format helps with the memory locality of the data. Moreover, most transformations within M3 Query can work in parallel on different blocks which can significantly increase the computation speed.\n\n\nDiagram\n\n\nBelow is a visual representation of a set of \nBlocks\n. On top is the M3QL query that gets executed, and on the bottom, are the results of the query containing 3 different Blocks.\n\n\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                                                       \u2502\n                              \u2502     fetch name:sign_up city_id:{new_york,san_diego,toronto} ios:*     \u2502\n                              \u2502                                                                       \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u25bc                        \u25bc                         \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502  Block One \u2502            \u2502  Block Two \u2502           \u2502 Block Three \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502   t  \u2502 t+1  \u2502 t+2  \u2502    \u2502  t+3 \u2502 t+4  \u2502 t+5  \u2502   \u2502  t+6 \u2502 t+7  \u2502 t+8  \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:new_york os:ios  \u2502 \u2502  5   \u2502  2   \u2502  10  \u2502    \u2502  10  \u2502  2   \u2502  10  \u2502   \u2502  5   \u2502  3   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502city_id:new_york os:android\u2502 \u2502  10  \u2502  8   \u2502  5   \u2502    \u2502  20  \u2502  4   \u2502  5   \u2502   \u2502  10  \u2502  8   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502 city_id:san_diego os:ios  \u2502 \u2502  10  \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  8   \u2502  6   \u2502  6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:toronto os:ios   \u2502 \u2502  2   \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  2   \u2502  5   \u2502  10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nM3DB =\n M3 Query Blocks\n\n\nIn order to convert M3DB blocks into M3 Query blocks, we need to consolidate across different namespaces. In short, M3DB namespaces are essentially different resolutions that metrics are stored at. For example, a metric might be stored at both 1min and 10min resolutions- meaning this metric is found in two namespaces.\n\n\nAt a high level, M3DB returns to M3 Query \nSeriesBlocks\n that contain a list of \nSeriesIterators\n for a given timeseries per namespace. M3 Query then aligns the blocks across common time bounds before applying consolidation.\n\n\nFor example, let's say we have a query that returns two timeseries from two different namespaces- 1min and 10min. When we create the M3 Query \nBlock\n, in order to accurately consolidate results from these two namespaces, we need to convert everything to have a 10min resolution. Otherwise it will not be possible to perform correctly apply functions.\n\n\n\n\nComing Soon: More documentation on how M3 Query applies consolidation.", 
            "title": "Blocks"
        }, 
        {
            "location": "/query_engine/architecture/blocks/#blocks", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "Blocks"
        }, 
        {
            "location": "/query_engine/architecture/blocks/#overview", 
            "text": "The fundamental data structures that M3 Query uses are  Blocks .  Blocks  are what get created from the series iterators that M3DB returns. A  Block  is associated with a start and end time. It contains data from multiple time series stored in columnar format.  Most transformations within M3 Query will be applied across different series for each time interval. Therefore, having data stored in columnar format helps with the memory locality of the data. Moreover, most transformations within M3 Query can work in parallel on different blocks which can significantly increase the computation speed.", 
            "title": "Overview"
        }, 
        {
            "location": "/query_engine/architecture/blocks/#diagram", 
            "text": "Below is a visual representation of a set of  Blocks . On top is the M3QL query that gets executed, and on the bottom, are the results of the query containing 3 different Blocks.                                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                                                       \u2502\n                              \u2502     fetch name:sign_up city_id:{new_york,san_diego,toronto} ios:*     \u2502\n                              \u2502                                                                       \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u25bc                        \u25bc                         \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502  Block One \u2502            \u2502  Block Two \u2502           \u2502 Block Three \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502   t  \u2502 t+1  \u2502 t+2  \u2502    \u2502  t+3 \u2502 t+4  \u2502 t+5  \u2502   \u2502  t+6 \u2502 t+7  \u2502 t+8  \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:new_york os:ios  \u2502 \u2502  5   \u2502  2   \u2502  10  \u2502    \u2502  10  \u2502  2   \u2502  10  \u2502   \u2502  5   \u2502  3   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502city_id:new_york os:android\u2502 \u2502  10  \u2502  8   \u2502  5   \u2502    \u2502  20  \u2502  4   \u2502  5   \u2502   \u2502  10  \u2502  8   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502 city_id:san_diego os:ios  \u2502 \u2502  10  \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  8   \u2502  6   \u2502  6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:toronto os:ios   \u2502 \u2502  2   \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  2   \u2502  5   \u2502  10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Diagram"
        }, 
        {
            "location": "/query_engine/architecture/blocks/#m3db-m3-query-blocks", 
            "text": "In order to convert M3DB blocks into M3 Query blocks, we need to consolidate across different namespaces. In short, M3DB namespaces are essentially different resolutions that metrics are stored at. For example, a metric might be stored at both 1min and 10min resolutions- meaning this metric is found in two namespaces.  At a high level, M3DB returns to M3 Query  SeriesBlocks  that contain a list of  SeriesIterators  for a given timeseries per namespace. M3 Query then aligns the blocks across common time bounds before applying consolidation.  For example, let's say we have a query that returns two timeseries from two different namespaces- 1min and 10min. When we create the M3 Query  Block , in order to accurately consolidate results from these two namespaces, we need to convert everything to have a 10min resolution. Otherwise it will not be possible to perform correctly apply functions.   Coming Soon: More documentation on how M3 Query applies consolidation.", 
            "title": "M3DB =&gt; M3 Query Blocks"
        }, 
        {
            "location": "/query_engine/architecture/functions/", 
            "text": "Function Processing\n\n\nSupported Functions\n\n\n\n\n\n\n\n\nM3QL\n\n\nPrometheus\n\n\nGraphite\n\n\n\n\n\n\n\n\n\n\nabs/absolute\n\n\nabs()\n\n\nabsolute(seriesList)\n\n\n\n\n\n\nalias [alias]\n\n\n\n\nalias(seriesList, newName)\n\n\n\n\n\n\naliasByTags [tag]\n\n\n\n\naliasByTags(seriesList, *tags)\n\n\n\n\n\n\naliasByBucket/aliasByHistogramBucket [tag]\n\n\n\n\n\n\n\n\n\n\nanomalies [flags]\n\n\n\n\n\n\n\n\n\n\nasPercent\n\n\n/\n\n\nasPercent(seriesList, total=None, *nodes)\n\n\n\n\n\n\navg/averageSeries [tag]\n\n\navg()\n\n\naverageSeries(*seriesLists)\n\n\n\n\n\n\nchanged\n\n\n\n\nchanged(seriesList)\n\n\n\n\n\n\nconstantLine [value]\n\n\n\n\nconstantLine(value)\n\n\n\n\n\n\ncount\n\n\ncount()\n\n\ncountSeries(*seriesLists)\n\n\n\n\n\n\nderivative\n\n\n\n\nderivative(seriesList)\n\n\n\n\n\n\ndiff\n\n\n-\n\n\ndiffSeries(*seriesLists)\n\n\n\n\n\n\ndivideSeries\n\n\n/\n\n\ndivideSeries(dividendSeriesList, divisorSeries)\n\n\n\n\n\n\neq/== [value]\n\n\n==\n\n\nremoveBelowValue(seriesList, n)/removeAboveValue(seriesList, n)\n\n\n\n\n\n\nne/!= [value]\n\n\n!=\n\n\nremoveBelowValue(seriesList, n)/removeAboveValue(seriesList, n)\n\n\n\n\n\n\nexcludeByTag [tag, pattern]\n\n\n\n\nexclude(seriesList, pattern)\n\n\n\n\n\n\nexecute/exec [fetch]\n\n\n\n\n\n\n\n\n\n\nfallbackSeries [replacement]\n\n\n\n\nfallbackSeries(seriesList, fallback)\n\n\n\n\n\n\nfetch\n\n\n\n\n\n\n\n\n\n\nge/=\n [value]\n\n\n=\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\ngt/\n [value]\n\n\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\nhead [limit]\n\n\ntopk()\n\n\nhighest(seriesList, n=1, func='average')\n\n\n\n\n\n\nhistogramCDF [idTag, rangeTag, value]\n\n\n\n\n\n\n\n\n\n\nhistogramPercentile [idTag, rangeTag, percentileValue]\n\n\n\n\n\n\n\n\n\n\nidentity [name]\n\n\n\n\nidentity(name)\n\n\n\n\n\n\nintegral\n\n\n\n\nintegral(seriesList)\n\n\n\n\n\n\nintersect [tags]\n\n\nand/or\n\n\n\n\n\n\n\n\nisNonNull\n\n\n\n\nisNonNull(seriesList)\n\n\n\n\n\n\njainCP\n\n\n\n\n\n\n\n\n\n\nkeepLastValue\n\n\n\n\nkeepLastValue(seriesList, limit=inf)\n\n\n\n\n\n\nle/\n= [value]\n\n\n=\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nlogarithm\n\n\nln()\n\n\nlogarithm(seriesList, base=10)\n\n\n\n\n\n\nlt/\n [value]\n\n\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nmax/maxSeries [tag]\n\n\nmax()\n\n\nmaxSeries(*seriesLists)\n\n\n\n\n\n\nmin/minSeries [tag]\n\n\nmin()\n\n\nminSeries(*seriesLists)\n\n\n\n\n\n\nmoving [interval, func]\n\n\n_over_time()\n\n\nmovingMax, movingMin, movingMedian, movingAverage, etc.\n\n\n\n\n\n\nmultiply/multiplySeries [tag]\n\n\n*\n\n\nmultiplySeries(*seriesLists)\n\n\n\n\n\n\nnonNegativeDerivative [maxValue]\n\n\n\n\nnonNegativeDerivative(seriesList, maxValue=None)\n\n\n\n\n\n\nnPercentile [percentile]\n\n\n\n\nnPercentile(seriesList, n)\n\n\n\n\n\n\noffset [amount]\n\n\n\n\noffset(seriesList, factor)\n\n\n\n\n\n\npercentileOfSeries [n, true/false, tag]\n\n\n\n\npercentileOfSeries(seriesList, n, interpolate=False)\n\n\n\n\n\n\nperSecond\n\n\nrate()\n\n\nperSecond(seriesList, maxValue=None)\n\n\n\n\n\n\npromHistogramPercentile [percentileValue]\n\n\n\n\n\n\n\n\n\n\nrange [tag]\n\n\n\n\nrangeOfSeries(*seriesLists)\n\n\n\n\n\n\nremoveAbovePercentile [percentile]\n\n\n\n\nremoveAbovePercentile(seriesList, n)\n\n\n\n\n\n\nremoveBelowPercentile [percentile]\n\n\n\n\nremoveBelowPercentile(seriesList, n)\n\n\n\n\n\n\nremoveAboveValue [value]\n\n\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nremoveBelowValue [value]\n\n\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\nremoveEmpty\n\n\n\n\nremoveEmptySeries(seriesList, xFilesFactor=None)\n\n\n\n\n\n\nscale [factor]\n\n\n\n\nscale(seriesList, factor)\n\n\n\n\n\n\nscaleToSeconds [seconds]\n\n\n\n\nscaleToSeconds(seriesList, seconds)\n\n\n\n\n\n\nsetDiff [tags]\n\n\n\n\n\n\n\n\n\n\nshowAnomalyThresholds [level, model]\n\n\n\n\n\n\n\n\n\n\nshowTags [true/false, tagName(s)]\n\n\n\n\n\n\n\n\n\n\nsort/sortSeries [avg, current, max, stddev, sum]\n\n\nsort()\n\n\nsortBy(seriesList, func='average', reverse=False)\n\n\n\n\n\n\nstdev [points, windowTolerance]\n\n\nstddev()\n\n\nstdev(seriesList, points, windowTolerance=0.1)\n\n\n\n\n\n\nsqrt/squareRoot\n\n\nsqrt()\n\n\nsquareRoot(seriesList)\n\n\n\n\n\n\nsummarize [interval, func, alignToFrom]\n\n\n\n\nsummarize(seriesList, intervalString, func='sum', alignToFrom=False)\n\n\n\n\n\n\nsum/sumSeries [tag]\n\n\nsum()\n\n\nsumSeries(*seriesLists)\n\n\n\n\n\n\nsustain [duration]\n\n\n\n\n\n\n\n\n\n\nsustainedAbove \n sustainedBelow\n\n\n\n\n\n\n\n\n\n\ntail [limit]\n\n\nbottomk()\n\n\nlowest(seriesList, n=1, func='average')\n\n\n\n\n\n\ntimeshift [duration]\n\n\n\n\ntimeShift(seriesList, timeShift, resetEnd=True, alignDST=False)\n\n\n\n\n\n\ntimestamp\n\n\ntimestamp()\n\n\n\n\n\n\n\n\ntransformNull [value]\n\n\n\n\ntransformNull(seriesList, default=0, referenceSeries=None)", 
            "title": "Function Processing"
        }, 
        {
            "location": "/query_engine/architecture/functions/#function-processing", 
            "text": "", 
            "title": "Function Processing"
        }, 
        {
            "location": "/query_engine/architecture/functions/#supported-functions", 
            "text": "M3QL  Prometheus  Graphite      abs/absolute  abs()  absolute(seriesList)    alias [alias]   alias(seriesList, newName)    aliasByTags [tag]   aliasByTags(seriesList, *tags)    aliasByBucket/aliasByHistogramBucket [tag]      anomalies [flags]      asPercent  /  asPercent(seriesList, total=None, *nodes)    avg/averageSeries [tag]  avg()  averageSeries(*seriesLists)    changed   changed(seriesList)    constantLine [value]   constantLine(value)    count  count()  countSeries(*seriesLists)    derivative   derivative(seriesList)    diff  -  diffSeries(*seriesLists)    divideSeries  /  divideSeries(dividendSeriesList, divisorSeries)    eq/== [value]  ==  removeBelowValue(seriesList, n)/removeAboveValue(seriesList, n)    ne/!= [value]  !=  removeBelowValue(seriesList, n)/removeAboveValue(seriesList, n)    excludeByTag [tag, pattern]   exclude(seriesList, pattern)    execute/exec [fetch]      fallbackSeries [replacement]   fallbackSeries(seriesList, fallback)    fetch      ge/=  [value]  =  removeBelowValue(seriesList, n)    gt/  [value]   removeBelowValue(seriesList, n)    head [limit]  topk()  highest(seriesList, n=1, func='average')    histogramCDF [idTag, rangeTag, value]      histogramPercentile [idTag, rangeTag, percentileValue]      identity [name]   identity(name)    integral   integral(seriesList)    intersect [tags]  and/or     isNonNull   isNonNull(seriesList)    jainCP      keepLastValue   keepLastValue(seriesList, limit=inf)    le/ = [value]  =  removeAboveValue(seriesList, n)    logarithm  ln()  logarithm(seriesList, base=10)    lt/  [value]   removeAboveValue(seriesList, n)    max/maxSeries [tag]  max()  maxSeries(*seriesLists)    min/minSeries [tag]  min()  minSeries(*seriesLists)    moving [interval, func]  _over_time()  movingMax, movingMin, movingMedian, movingAverage, etc.    multiply/multiplySeries [tag]  *  multiplySeries(*seriesLists)    nonNegativeDerivative [maxValue]   nonNegativeDerivative(seriesList, maxValue=None)    nPercentile [percentile]   nPercentile(seriesList, n)    offset [amount]   offset(seriesList, factor)    percentileOfSeries [n, true/false, tag]   percentileOfSeries(seriesList, n, interpolate=False)    perSecond  rate()  perSecond(seriesList, maxValue=None)    promHistogramPercentile [percentileValue]      range [tag]   rangeOfSeries(*seriesLists)    removeAbovePercentile [percentile]   removeAbovePercentile(seriesList, n)    removeBelowPercentile [percentile]   removeBelowPercentile(seriesList, n)    removeAboveValue [value]   removeAboveValue(seriesList, n)    removeBelowValue [value]   removeBelowValue(seriesList, n)    removeEmpty   removeEmptySeries(seriesList, xFilesFactor=None)    scale [factor]   scale(seriesList, factor)    scaleToSeconds [seconds]   scaleToSeconds(seriesList, seconds)    setDiff [tags]      showAnomalyThresholds [level, model]      showTags [true/false, tagName(s)]      sort/sortSeries [avg, current, max, stddev, sum]  sort()  sortBy(seriesList, func='average', reverse=False)    stdev [points, windowTolerance]  stddev()  stdev(seriesList, points, windowTolerance=0.1)    sqrt/squareRoot  sqrt()  squareRoot(seriesList)    summarize [interval, func, alignToFrom]   summarize(seriesList, intervalString, func='sum', alignToFrom=False)    sum/sumSeries [tag]  sum()  sumSeries(*seriesLists)    sustain [duration]      sustainedAbove   sustainedBelow      tail [limit]  bottomk()  lowest(seriesList, n=1, func='average')    timeshift [duration]   timeShift(seriesList, timeShift, resetEnd=True, alignDST=False)    timestamp  timestamp()     transformNull [value]   transformNull(seriesList, default=0, referenceSeries=None)", 
            "title": "Supported Functions"
        }, 
        {
            "location": "/how_to/single_node/", 
            "text": "M3DB Single Node Deployment\n\n\nDeploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it\nhas to offer. Our Docker image by default configures a single M3DB instance as one binary\ncontaining:\n\n\n\n\nAn M3DB storage instance (\nm3dbnode\n) for timeseries storage. This includes an embedded tag-based\n  metrics index, as well as as an embedded etcd server for storing the above mentioned cluster\n  topology and runtime configuration.\n\n\nA \"coordinator\" instance (\nm3coordinator\n) for writing and querying tagged metrics, as well as\n  managing cluster topology and runtime configuration.\n\n\n\n\nTo begin, first start up a Docker container with port \n7201\n (used to manage the cluster topology)\nand port \n9003\n (used to read and write metrics) exposed. We recommend you create a persistent data\ndirectory on your host for durability:\n\n\ndocker pull quay.io/m3/m3dbnode:latest\ndocker run -p 7201:7201 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3/m3dbnode:latest\n\n\n\n\n\n\n\n\n\n\nNext, create an initial namespace for your metrics in the database:\n\n\n\n\n\ncurl -X POST http://localhost:7201/api/v1/database/create -d '{\n  \ntype\n: \nlocal\n,\n  \nnamespaceName\n: \ndefault\n,\n  \nretentionTime\n: \n48h\n\n}'\n\n\n\n\nShortly after, you should see your node complete bootstrapping! Don't worry if you see warnings or\nerrors related to a local cache file, such as \n[W] could not load cache from file\n/var/lib/m3kv/m3db_embedded.json\n. Those are expected for a local instance and in general any\nwarn-level errors (prefixed with \n[W]\n) should not block bootstrapping.\n\n\n02:28:30.008072[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n02:28:30.270681[I] node tchannelthrift: listening on 0.0.0.0:9000\n02:28:30.271909[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n02:28:30.519468[I] node httpjson: listening on 0.0.0.0:9002\n02:28:30.520061[I] cluster httpjson: listening on 0.0.0.0:9003\n02:28:30.520652[I] bootstrap finished [{namespace metrics} {duration 55.4\u00b5s}]\n02:28:30.520909[I] bootstrapped\n\n\n\n\nThe node also self-hosts its OpenAPI docs, outlining available endpoints. You can access this by\ngoing to \nlocalhost:7201/api/v1/docs\n in your browser.\n\n\n\n\nNow you can experiment with writing tagged metrics:\n\n\ncurl -sSf -X POST http://localhost:9003/writetagged -d '{\n  \nnamespace\n: \ndefault\n,\n  \nid\n: \nfoo\n,\n  \ntags\n: [\n    {\n      \nname\n: \ncity\n,\n      \nvalue\n: \nnew_york\n\n    },\n    {\n      \nname\n: \nendpoint\n,\n      \nvalue\n: \n/request\n\n    }\n  ],\n  \ndatapoint\n: {\n    \ntimestamp\n: '\n$(date \n+%s\n)\n',\n    \nvalue\n: 42.123456789\n  }\n}\n'\n\n\n\n\nAnd reading the metrics you've written:\n\n\ncurl -sSf -X POST http://localhost:9003/query -d '{\n  \nnamespace\n: \ndefault\n,\n  \nquery\n: {\n    \nregexp\n: {\n      \nfield\n: \ncity\n,\n      \nregexp\n: \n.*\n\n    }\n  },\n  \nrangeStart\n: 0,\n  \nrangeEnd\n: '\n$(date \n+%s\n)\n'\n}' | jq .\n\n{\n  \nresults\n: [\n    {\n      \nid\n: \nfoo\n,\n      \ntags\n: [\n        {\n          \nname\n: \ncity\n,\n          \nvalue\n: \nnew_york\n\n        },\n        {\n          \nname\n: \nendpoint\n,\n          \nvalue\n: \n/request\n\n        }\n      ],\n      \ndatapoints\n: [\n        {\n          \ntimestamp\n: 1527039389,\n          \nvalue\n: 42.123456789\n        }\n      ]\n    }\n  ],\n  \nexhaustive\n: true\n}\n\n\n\n\nIntegrations\n\n\nPrometheus as a long term storage remote read/write endpoint\n.", 
            "title": "M3DB Single Node Deployment"
        }, 
        {
            "location": "/how_to/single_node/#m3db-single-node-deployment", 
            "text": "Deploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it\nhas to offer. Our Docker image by default configures a single M3DB instance as one binary\ncontaining:   An M3DB storage instance ( m3dbnode ) for timeseries storage. This includes an embedded tag-based\n  metrics index, as well as as an embedded etcd server for storing the above mentioned cluster\n  topology and runtime configuration.  A \"coordinator\" instance ( m3coordinator ) for writing and querying tagged metrics, as well as\n  managing cluster topology and runtime configuration.   To begin, first start up a Docker container with port  7201  (used to manage the cluster topology)\nand port  9003  (used to read and write metrics) exposed. We recommend you create a persistent data\ndirectory on your host for durability:  docker pull quay.io/m3/m3dbnode:latest\ndocker run -p 7201:7201 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3/m3dbnode:latest    Next, create an initial namespace for your metrics in the database:   curl -X POST http://localhost:7201/api/v1/database/create -d '{\n   type :  local ,\n   namespaceName :  default ,\n   retentionTime :  48h \n}'  Shortly after, you should see your node complete bootstrapping! Don't worry if you see warnings or\nerrors related to a local cache file, such as  [W] could not load cache from file\n/var/lib/m3kv/m3db_embedded.json . Those are expected for a local instance and in general any\nwarn-level errors (prefixed with  [W] ) should not block bootstrapping.  02:28:30.008072[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n02:28:30.270681[I] node tchannelthrift: listening on 0.0.0.0:9000\n02:28:30.271909[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n02:28:30.519468[I] node httpjson: listening on 0.0.0.0:9002\n02:28:30.520061[I] cluster httpjson: listening on 0.0.0.0:9003\n02:28:30.520652[I] bootstrap finished [{namespace metrics} {duration 55.4\u00b5s}]\n02:28:30.520909[I] bootstrapped  The node also self-hosts its OpenAPI docs, outlining available endpoints. You can access this by\ngoing to  localhost:7201/api/v1/docs  in your browser.   Now you can experiment with writing tagged metrics:  curl -sSf -X POST http://localhost:9003/writetagged -d '{\n   namespace :  default ,\n   id :  foo ,\n   tags : [\n    {\n       name :  city ,\n       value :  new_york \n    },\n    {\n       name :  endpoint ,\n       value :  /request \n    }\n  ],\n   datapoint : {\n     timestamp : ' $(date  +%s ) ',\n     value : 42.123456789\n  }\n}\n'  And reading the metrics you've written:  curl -sSf -X POST http://localhost:9003/query -d '{\n   namespace :  default ,\n   query : {\n     regexp : {\n       field :  city ,\n       regexp :  .* \n    }\n  },\n   rangeStart : 0,\n   rangeEnd : ' $(date  +%s ) '\n}' | jq .\n\n{\n   results : [\n    {\n       id :  foo ,\n       tags : [\n        {\n           name :  city ,\n           value :  new_york \n        },\n        {\n           name :  endpoint ,\n           value :  /request \n        }\n      ],\n       datapoints : [\n        {\n           timestamp : 1527039389,\n           value : 42.123456789\n        }\n      ]\n    }\n  ],\n   exhaustive : true\n}", 
            "title": "M3DB Single Node Deployment"
        }, 
        {
            "location": "/how_to/single_node/#integrations", 
            "text": "Prometheus as a long term storage remote read/write endpoint .", 
            "title": "Integrations"
        }, 
        {
            "location": "/how_to/cluster_hard_way/", 
            "text": "M3DB Cluster Deployment, Manually (The Hard Way)\n\n\nIntroduction\n\n\nThis document lists the manual steps involved in deploying a M3DB cluster. In practice, you'd be automating this using Terraform or using Kubernetes rather than doing this by hand; guides for doing so are available under the How-To section.\n\n\nPrimer Architecture\n\n\nA quick primer on M3DB architecture. Here\u2019s what a typical deployment looks like:\n\n\n\n\nA few different things to highlight about the diagram:\n\n\nRole Type\n\n\nThere are three \u2018role types\u2019 for a m3db deployment -\n\n\n\n\n\n\nCoordinator: \nm3coordinator\n serves to coordinate reads and writes across all hosts in the cluster. It\u2019s a lightweight process, and does not store any data. This role would typically be run alongside a Prometheus instance, or be baked into a collector agent.\n\n\n\n\n\n\nStorage Node: \nm3dbnode\n processes running on these hosts are the workhorses of the database, they store data; and serve reads and writes.\n\n\n\n\n\n\nSeed Node: First and foremost, these hosts are storage nodes themselves. In addition to that responsibility, they run an embedded ETCD server. This is to allow the various M3DB processes running across the cluster to reason about the topology/configuration of the cluster in a consistent manner.\n\n\n\n\n\n\nNote: In very large deployments, you\u2019d use a dedicated ETCD cluster, and only use M3DB Storage and Coordinator Nodes\n\n\nProvisioning\n\n\nEnough background, lets get you going with a real cluster! Provision your host (be it VMs from AWS/GCP/etc) or bare-metal servers in your DC with the latest and greatest flavour of Linux you favor. M3DB works on all popular distributions - Ubuntu/RHEL/CentOS, let us know if you run into issues on another platform and we\u2019ll be happy to assist.\n\n\nNetwork\n\n\nIf you\u2019re using AWS or GCP it is highly advised to use static IPs so that if you need to replace a host, you don\u2019t have to update your configuration files on all the hosts, you simply decomission the old seed node and provision a new seed node with the same host ID and static IP that the old seed node had.  For AWS you can use a \nElastic Network Interface\n on a VPC and for GCP you can simply use an \ninternal static IP address\n.\n\n\nIn this example you will be creating three static IP addresses for the three seed nodes.\n\n\nFurther, we assume you have hostnames configured correctly too. i.e. running \nhostname\n on a host in the cluster returns the host ID you'll be using when specifying instance host IDs when creating the M3DB cluster placement. E.g. running \nhostname\n on a node \nm3db001\n should return it's host ID \nm3db001\n.\n\n\nIn GCP the name of your instance when you create it will automatically be it's hostname. When you create an instance click \"Management, disks, networking, SSH keys\" and under \"Networking\" click the default interface and click the \"Primary internal IP\" drop down and select \"Reserve a static internal IP address\" and give it a name, i.e. \nm3db001\n, a description that describes it's a seed node IP address and use \"Assign automatically\".\n\n\nIn AWS it might be simpler to just use whatever the hostname you get for the provisioned VM as your host ID when specifying M3DB placement.  Either that or use the \nenvironment\n host ID resolver and pass your host ID when launching the database process with an environment variable.  You can set to the host ID and specify the environment variable name in config as \nenvVarName: M3DB_HOST_ID\n if you are using an environment variable named \nM3DB_HOST_ID\n.\n\n\nRelevant config snippet:\n\n\nhostID:\n  resolver: environment\n  envVarName: M3DB_HOST_ID\n\n\n\n\nThen start your process with:\n\n\nM3DB_HOST_ID=m3db001 m3dbnode -f config.yml\n\n\n\n\nKernel\n\n\nm3dbnode\n uses a lot of mmap-ed files for performance, as a result, you might need to bump \nvm.max_map_count\n. We suggest setting this value to 262,144 when provisioning your VM, so you don\u2019t have to come back and debug issues later.\n\n\nOS\n\n\nm3dbnode\n also can use a high number of files and we suggest setting a high max open number of files due to per partition fileset volumes.\n\n\nOn linux you can set a high limit for number of max open files in \n/etc/security/limits.conf\n:\n\n\nyour_m3db_user        hard nofile 500000\nyour_m3db_user        soft nofile 500000\n\n\n\n\nBefore running the process make sure the limits are set, if running manually you can raise the limit for the current user with \nulimit -n 500000\n.\n\n\nConfig files\n\n\nWe wouldn\u2019t feel right to call this guide, \u201cThe Hard Way\u201d and not require you to change some configs by hand.\n\n\nNote: the steps that follow assume you have the following 3 seed nodes - make necessary adjustment if you have more or are using a dedicated ETCD cluster. Example seed nodes:\n\n\n\n\nm3db001 (Region=us-east1, Zone=us-east1-a, Static IP=10.142.0.1)\n\n\nm3db002 (Region=us-east1, Zone=us-east1-b, Static IP=10.142.0.2)\n\n\nm3db003 (Region=us-east1, Zone=us-east1-c, Static IP=10.142.0.3)\n\n\n\n\nWe\u2019re going to start with the M3DB config template and modify it to work for your cluster. Start by downloading the \nconfig\n. Update the config \u2018service\u2019 and 'seedNodes' sections to read as follows:\n\n\nconfig:\n  service:\n    env: default_env\n    zone: embedded\n    service: m3db\n    cacheDir: /var/lib/m3kv\n    etcdClusters:\n      - zone: embedded\n        endpoints:\n          - 10.142.0.1:2379\n          - 10.142.0.2:2379\n          - 10.142.0.3:2379\n  seedNodes:\n    initialCluster:\n      - hostID: m3db001\n        endpoint: http://10.142.0.1:2380\n      - hostID: m3db002\n        endpoint: http://10.142.0.2:2380\n      - hostID: m3db003\n        endpoint: http://10.142.0.3:2380\n\n\n\n\nStart the seed nodes\n\n\nTransfer the config you just crafted to each host in the cluster. And then starting with the seed nodes, start up the m3dbnode process:\n\n\nm3dbnode -f \nconfig-name.yml\n\n\n\n\n\nNote, remember to daemon-ize this using your favourite utility: systemd/init.d/supervisor/etc\n\n\nInitialize Topology\n\n\nM3DB calls its cluster topology \u2018placement\u2019. Run the command below on any of the seed nodes to initialize your first placement.\n\n\nNote: Isolation group specifies how the cluster places shards to avoid more than one replica of a shard appearing in the same replica group. As such you must be using at least as many isolation groups as your replication factor. In this example we use the availibity zones \nus-east1-a\n, \nus-east1-b\n, \nus-east1-c\n as our isolation groups which matches our replication factor of 3.\n\n\ncurl -X POST localhost:7201/api/v1/placement/init -d '{\n    \nnum_shards\n: 1024,\n    \nreplication_factor\n: 3,\n    \ninstances\n: [\n        {\n            \nid\n: \nm3db001\n,\n            \nisolation_group\n: \nus-east1-a\n,\n            \nzone\n: \nembedded\n,\n            \nweight\n: 100,\n            \nendpoint\n: \n10.142.0.1:9000\n,\n            \nhostname\n: \nm3db001\n,\n            \nport\n: 9000\n        },\n        {\n            \nid\n: \nm3db002\n,\n            \nisolation_group\n: \nus-east1-b\n,\n            \nzone\n: \nembedded\n,\n            \nweight\n: 100,\n            \nendpoint\n: \n10.142.0.2:9000\n,\n            \nhostname\n: \nm3db002-us-east\n,\n            \nport\n: 9000\n        },\n        {\n            \nid\n: \nm3db003\n,\n            \nisolation_group\n: \nus-east1-c\n,\n            \nzone\n: \nembedded\n,\n            \nweight\n: 100,\n            \nendpoint\n: \n10.142.0.3:9000\n,\n            \nhostname\n: \nm3db003\n,\n            \nport\n: 9000\n        }\n    ]\n}'\n\n\n\n\nCreate namespace(s)\n\n\nA namespace in M3DB is similar to a table in Cassandra (C*). You can specify retention and a few distinct properties on a namespace.\n\n\nRun the following on any seed node to create a \u2018metrics\u2019 namespace with 30 day retention, 12 hour block sizes, ability to write out of order datapoints into past or future by 1 hour:\n\n\ncurl -X POST localhost:7201/api/v1/namespace -d '{\n  \nname\n: \nmetrics\n,\n  \noptions\n: {\n    \nbootstrapEnabled\n: true,\n    \nflushEnabled\n: true,\n    \nwritesToCommitLog\n: true,\n    \ncleanupEnabled\n: true,\n    \nsnapshotEnabled\n: false,\n    \nrepairEnabled\n: false,\n    \nretentionOptions\n: {\n      \nretentionPeriodDuration\n: \n720h\n,\n      \nblockSizeDuration\n: \n12h\n,\n      \nbufferFutureDuration\n: \n1h\n,\n      \nbufferPastDuration\n: \n1h\n,\n      \nblockDataExpiry\n: true,\n      \nblockDataExpiryAfterNotAccessPeriodDuration\n: \n5m\n\n    },\n    \nindexOptions\n: {\n      \nenabled\n: true,\n      \nblockSizeDuration\n: \n12h\n\n    }\n  }\n}'\n\n\n\n\nShortly after, you should see your node complete bootstrapping:\n\n\n20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts\n\n\n\n\nRead more about namespaces and the various knobs in the docs.\n\n\nTest it out\n\n\nNow you can experiment with writing tagged metrics:\n\n\ncurl -sSf -X POST localhost:9003/writetagged -d '{\n  \nnamespace\n: \nmetrics\n,\n  \nid\n: \nfoo\n,\n  \ntags\n: [\n    {\n      \nname\n: \ncity\n,\n      \nvalue\n: \nnew_york\n\n    },\n    {\n      \nname\n: \nendpoint\n,\n      \nvalue\n: \n/request\n\n    }\n  ],\n  \ndatapoint\n: {\n    \ntimestamp\n: '\n$(date \n+%s\n)\n',\n    \nvalue\n: 42.123456789\n  }\n}'\n\n\n\n\nAnd reading the metrics you've written:\n\n\ncurl -sSf -X POST http://localhost:9003/query -d '{\n  \nnamespace\n: \nmetrics\n,\n  \nquery\n: {\n    \nregexp\n: {\n      \nfield\n: \ncity\n,\n      \nregexp\n: \n.*\n\n    }\n  },\n  \nrangeStart\n: 0,\n  \nrangeEnd\n: '\n$(date \n+%s\n)\n'\n}' | jq .\n\n\n\n\nIntegrations\n\n\nPrometheus as a long term storage remote read/write endpoint\n.", 
            "title": "M3DB Cluster Deployment, Manually"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#m3db-cluster-deployment-manually-the-hard-way", 
            "text": "", 
            "title": "M3DB Cluster Deployment, Manually (The Hard Way)"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#introduction", 
            "text": "This document lists the manual steps involved in deploying a M3DB cluster. In practice, you'd be automating this using Terraform or using Kubernetes rather than doing this by hand; guides for doing so are available under the How-To section.", 
            "title": "Introduction"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#primer-architecture", 
            "text": "A quick primer on M3DB architecture. Here\u2019s what a typical deployment looks like:   A few different things to highlight about the diagram:", 
            "title": "Primer Architecture"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#role-type", 
            "text": "There are three \u2018role types\u2019 for a m3db deployment -    Coordinator:  m3coordinator  serves to coordinate reads and writes across all hosts in the cluster. It\u2019s a lightweight process, and does not store any data. This role would typically be run alongside a Prometheus instance, or be baked into a collector agent.    Storage Node:  m3dbnode  processes running on these hosts are the workhorses of the database, they store data; and serve reads and writes.    Seed Node: First and foremost, these hosts are storage nodes themselves. In addition to that responsibility, they run an embedded ETCD server. This is to allow the various M3DB processes running across the cluster to reason about the topology/configuration of the cluster in a consistent manner.    Note: In very large deployments, you\u2019d use a dedicated ETCD cluster, and only use M3DB Storage and Coordinator Nodes", 
            "title": "Role Type"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#provisioning", 
            "text": "Enough background, lets get you going with a real cluster! Provision your host (be it VMs from AWS/GCP/etc) or bare-metal servers in your DC with the latest and greatest flavour of Linux you favor. M3DB works on all popular distributions - Ubuntu/RHEL/CentOS, let us know if you run into issues on another platform and we\u2019ll be happy to assist.", 
            "title": "Provisioning"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#network", 
            "text": "If you\u2019re using AWS or GCP it is highly advised to use static IPs so that if you need to replace a host, you don\u2019t have to update your configuration files on all the hosts, you simply decomission the old seed node and provision a new seed node with the same host ID and static IP that the old seed node had.  For AWS you can use a  Elastic Network Interface  on a VPC and for GCP you can simply use an  internal static IP address .  In this example you will be creating three static IP addresses for the three seed nodes.  Further, we assume you have hostnames configured correctly too. i.e. running  hostname  on a host in the cluster returns the host ID you'll be using when specifying instance host IDs when creating the M3DB cluster placement. E.g. running  hostname  on a node  m3db001  should return it's host ID  m3db001 .  In GCP the name of your instance when you create it will automatically be it's hostname. When you create an instance click \"Management, disks, networking, SSH keys\" and under \"Networking\" click the default interface and click the \"Primary internal IP\" drop down and select \"Reserve a static internal IP address\" and give it a name, i.e.  m3db001 , a description that describes it's a seed node IP address and use \"Assign automatically\".  In AWS it might be simpler to just use whatever the hostname you get for the provisioned VM as your host ID when specifying M3DB placement.  Either that or use the  environment  host ID resolver and pass your host ID when launching the database process with an environment variable.  You can set to the host ID and specify the environment variable name in config as  envVarName: M3DB_HOST_ID  if you are using an environment variable named  M3DB_HOST_ID .  Relevant config snippet:  hostID:\n  resolver: environment\n  envVarName: M3DB_HOST_ID  Then start your process with:  M3DB_HOST_ID=m3db001 m3dbnode -f config.yml", 
            "title": "Network"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#kernel", 
            "text": "m3dbnode  uses a lot of mmap-ed files for performance, as a result, you might need to bump  vm.max_map_count . We suggest setting this value to 262,144 when provisioning your VM, so you don\u2019t have to come back and debug issues later.", 
            "title": "Kernel"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#os", 
            "text": "m3dbnode  also can use a high number of files and we suggest setting a high max open number of files due to per partition fileset volumes.  On linux you can set a high limit for number of max open files in  /etc/security/limits.conf :  your_m3db_user        hard nofile 500000\nyour_m3db_user        soft nofile 500000  Before running the process make sure the limits are set, if running manually you can raise the limit for the current user with  ulimit -n 500000 .", 
            "title": "OS"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#config-files", 
            "text": "We wouldn\u2019t feel right to call this guide, \u201cThe Hard Way\u201d and not require you to change some configs by hand.  Note: the steps that follow assume you have the following 3 seed nodes - make necessary adjustment if you have more or are using a dedicated ETCD cluster. Example seed nodes:   m3db001 (Region=us-east1, Zone=us-east1-a, Static IP=10.142.0.1)  m3db002 (Region=us-east1, Zone=us-east1-b, Static IP=10.142.0.2)  m3db003 (Region=us-east1, Zone=us-east1-c, Static IP=10.142.0.3)   We\u2019re going to start with the M3DB config template and modify it to work for your cluster. Start by downloading the  config . Update the config \u2018service\u2019 and 'seedNodes' sections to read as follows:  config:\n  service:\n    env: default_env\n    zone: embedded\n    service: m3db\n    cacheDir: /var/lib/m3kv\n    etcdClusters:\n      - zone: embedded\n        endpoints:\n          - 10.142.0.1:2379\n          - 10.142.0.2:2379\n          - 10.142.0.3:2379\n  seedNodes:\n    initialCluster:\n      - hostID: m3db001\n        endpoint: http://10.142.0.1:2380\n      - hostID: m3db002\n        endpoint: http://10.142.0.2:2380\n      - hostID: m3db003\n        endpoint: http://10.142.0.3:2380", 
            "title": "Config files"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#start-the-seed-nodes", 
            "text": "Transfer the config you just crafted to each host in the cluster. And then starting with the seed nodes, start up the m3dbnode process:  m3dbnode -f  config-name.yml   Note, remember to daemon-ize this using your favourite utility: systemd/init.d/supervisor/etc", 
            "title": "Start the seed nodes"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#initialize-topology", 
            "text": "M3DB calls its cluster topology \u2018placement\u2019. Run the command below on any of the seed nodes to initialize your first placement.  Note: Isolation group specifies how the cluster places shards to avoid more than one replica of a shard appearing in the same replica group. As such you must be using at least as many isolation groups as your replication factor. In this example we use the availibity zones  us-east1-a ,  us-east1-b ,  us-east1-c  as our isolation groups which matches our replication factor of 3.  curl -X POST localhost:7201/api/v1/placement/init -d '{\n     num_shards : 1024,\n     replication_factor : 3,\n     instances : [\n        {\n             id :  m3db001 ,\n             isolation_group :  us-east1-a ,\n             zone :  embedded ,\n             weight : 100,\n             endpoint :  10.142.0.1:9000 ,\n             hostname :  m3db001 ,\n             port : 9000\n        },\n        {\n             id :  m3db002 ,\n             isolation_group :  us-east1-b ,\n             zone :  embedded ,\n             weight : 100,\n             endpoint :  10.142.0.2:9000 ,\n             hostname :  m3db002-us-east ,\n             port : 9000\n        },\n        {\n             id :  m3db003 ,\n             isolation_group :  us-east1-c ,\n             zone :  embedded ,\n             weight : 100,\n             endpoint :  10.142.0.3:9000 ,\n             hostname :  m3db003 ,\n             port : 9000\n        }\n    ]\n}'", 
            "title": "Initialize Topology"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#create-namespaces", 
            "text": "A namespace in M3DB is similar to a table in Cassandra (C*). You can specify retention and a few distinct properties on a namespace.  Run the following on any seed node to create a \u2018metrics\u2019 namespace with 30 day retention, 12 hour block sizes, ability to write out of order datapoints into past or future by 1 hour:  curl -X POST localhost:7201/api/v1/namespace -d '{\n   name :  metrics ,\n   options : {\n     bootstrapEnabled : true,\n     flushEnabled : true,\n     writesToCommitLog : true,\n     cleanupEnabled : true,\n     snapshotEnabled : false,\n     repairEnabled : false,\n     retentionOptions : {\n       retentionPeriodDuration :  720h ,\n       blockSizeDuration :  12h ,\n       bufferFutureDuration :  1h ,\n       bufferPastDuration :  1h ,\n       blockDataExpiry : true,\n       blockDataExpiryAfterNotAccessPeriodDuration :  5m \n    },\n     indexOptions : {\n       enabled : true,\n       blockSizeDuration :  12h \n    }\n  }\n}'  Shortly after, you should see your node complete bootstrapping:  20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts  Read more about namespaces and the various knobs in the docs.", 
            "title": "Create namespace(s)"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#test-it-out", 
            "text": "Now you can experiment with writing tagged metrics:  curl -sSf -X POST localhost:9003/writetagged -d '{\n   namespace :  metrics ,\n   id :  foo ,\n   tags : [\n    {\n       name :  city ,\n       value :  new_york \n    },\n    {\n       name :  endpoint ,\n       value :  /request \n    }\n  ],\n   datapoint : {\n     timestamp : ' $(date  +%s ) ',\n     value : 42.123456789\n  }\n}'  And reading the metrics you've written:  curl -sSf -X POST http://localhost:9003/query -d '{\n   namespace :  metrics ,\n   query : {\n     regexp : {\n       field :  city ,\n       regexp :  .* \n    }\n  },\n   rangeStart : 0,\n   rangeEnd : ' $(date  +%s ) '\n}' | jq .", 
            "title": "Test it out"
        }, 
        {
            "location": "/how_to/cluster_hard_way/#integrations", 
            "text": "Prometheus as a long term storage remote read/write endpoint .", 
            "title": "Integrations"
        }, 
        {
            "location": "/how_to/kubernetes/", 
            "text": "M3DB on Kubernetes\n\n\nM3DB on Kubernetes is currently in the alpha phase of development. We currently provide static manifests to bootstrap a\ncluster. In the future we hope to create an \noperator\n and leverage \ncustom resource\ndefinitions\n (CRDs) to automatically\nhandle operations such as managing cluster topology, but for now our manifests should be adequate to get started.\n\n\nPrerequisites\n\n\nM3DB performs better when it has access to fast disks. Every incoming write is written to a commit log, which at high\nvolumes of writes can be sensitive to spikes in disk latency. Additionally the random seeks into files when loading cold\nfiles benefit from lower random read latency.\n\n\nBecause of this, the included manifests reference a\n\nStorageClass\n named \nfast\n. Manifests are\nprovided to provision such a StorageClass on AWS / Azure / GCP using the respective cloud provider's premium disk class.\n\n\nIf you do not already have a StorageClass named \nfast\n, create one using one of the provided manifests:\n\n\n# AWS EBS (class io1)\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-class-aws.yaml\n\n# Azure premium LRS\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-class-azure.yaml\n\n# GCE Persistent SSD\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-class-gcp.yaml\n\n\n\n\nIf you wish to use your cloud provider's default remote disk, or another disk class entirely, you'll have to modify the\nmanifests.\n\n\nDeploying\n\n\nApply the following manifest to create your cluster:\n\n\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/bundle.yaml\n\n\n\n\nApplying this bundle will create the following resources:\n\n\n\n\nAn \nm3db\n \nNamespace\n for\n   all M3DB-related resources.\n\n\nA 3-node etcd cluster in the form of a\n   \nStatefulSet\n backed by persistent\n   remote SSDs. This cluster stores the DB topology and other runtime configuration data.\n\n\nA 3-node M3DB cluster in the form of a StatefulSet.\n\n\nHeadless services\n for\n   the etcd and m3db StatefulSets to provide stable DNS hostnames per-pod.\n\n\n\n\nWait until all created pods are listed as ready:\n\n\n$ kubectl -n m3db get po\nNAME         READY     STATUS    RESTARTS   AGE\netcd-0       1/1       Running   0          22m\netcd-1       1/1       Running   0          22m\netcd-2       1/1       Running   0          22m\nm3dbnode-0   1/1       Running   0          22m\nm3dbnode-1   1/1       Running   0          22m\nm3dbnode-2   1/1       Running   0          22m\n\n\n\n\nYou can now proceed to initialize a namespace and placment for the cluster the same as you would for our other how-to\nguides:\n\n\n# Open a local connection to the coordinator service:\n$ kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -\n 7201\nForwarding from [::1]:7201 -\n 7201\n\n\n\n\n# Create an initial cluster topology\ncurl -sSf -X POST localhost:7201/api/v1/placement/init -d '{\n    \nnum_shards\n: 1024,\n    \nreplication_factor\n: 3,\n    \ninstances\n: [\n        {\n            \nid\n: \nm3dbnode-0\n,\n            \nisolation_group\n: \npod0\n,\n            \nzone\n: \nembedded\n,\n            \nweight\n: 100,\n            \nendpoint\n: \nm3dbnode-0.m3dbnode:9000\n,\n            \nhostname\n: \nm3dbnode-0.m3dbnode\n,\n            \nport\n: 9000\n        },\n        {\n            \nid\n: \nm3dbnode-1\n,\n            \nisolation_group\n: \npod1\n,\n            \nzone\n: \nembedded\n,\n            \nweight\n: 100,\n            \nendpoint\n: \nm3dbnode-1.m3dbnode:9000\n,\n            \nhostname\n: \nm3dbnode-1.m3dbnode\n,\n            \nport\n: 9000\n        },\n        {\n            \nid\n: \nm3dbnode-2\n,\n            \nisolation_group\n: \npod2\n,\n            \nzone\n: \nembedded\n,\n            \nweight\n: 100,\n            \nendpoint\n: \nm3dbnode-2.m3dbnode:9000\n,\n            \nhostname\n: \nm3dbnode-2.m3dbnode\n,\n            \nport\n: 9000\n        }\n    ]\n}'\n\n\n\n\n# Create a namespace to hold your metrics\ncurl -X POST localhost:7201/api/v1/namespace -d '{\n  \nname\n: \ndefault\n,\n  \noptions\n: {\n    \nbootstrapEnabled\n: true,\n    \nflushEnabled\n: true,\n    \nwritesToCommitLog\n: true,\n    \ncleanupEnabled\n: true,\n    \nsnapshotEnabled\n: false,\n    \nrepairEnabled\n: false,\n    \nretentionOptions\n: {\n      \nretentionPeriodDuration\n: \n720h\n,\n      \nblockSizeDuration\n: \n12h\n,\n      \nbufferFutureDuration\n: \n1h\n,\n      \nbufferPastDuration\n: \n1h\n,\n      \nblockDataExpiry\n: true,\n      \nblockDataExpiryAfterNotAccessPeriodDuration\n: \n5m\n\n    },\n    \nindexOptions\n: {\n      \nenabled\n: true,\n      \nblockSizeDuration\n: \n12h\n\n    }\n  }\n}'\n\n\n\n\nShortly after you should see your nodes finish bootstrapping:\n\n\n$ kubectl -n m3db logs -f m3dbnode-0\n21:36:54.831698[I] cluster database initializing topology\n21:36:54.831732[I] cluster database resolving topology\n21:37:22.821740[I] resolving namespaces with namespace watch\n21:37:22.821813[I] updating database namespaces [{adds [metrics]} {updates []} {removals []}]\n21:37:23.008109[I] node tchannelthrift: listening on 0.0.0.0:9000\n21:37:23.008384[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n21:37:23.217090[I] node httpjson: listening on 0.0.0.0:9002\n21:37:23.217240[I] cluster httpjson: listening on 0.0.0.0:9003\n21:37:23.217526[I] bootstrapping shards for range starting [{run bootstrap-data} {bootstrapper filesystem} ...\n...\n21:37:23.239534[I] bootstrap data fetched now initializing shards with series blocks [{namespace metrics} {numShards 256} {numSeries 0}]\n21:37:23.240778[I] bootstrap finished [{namespace metrics} {duration 23.325194ms}]\n21:37:23.240856[I] bootstrapped\n21:37:29.733025[I] successfully updated topology to 3 hosts\n\n\n\n\nYou can now write and read metrics using the API on the db nodes:\n\n\n$ kubectl -n m3db port-forward svc/m3dbnode 9003\nForwarding from 127.0.0.1:9003 -\n 9003\nForwarding from [::1]:9003 -\n 9003\n\n\n\n\ncurl -sSf -X POST localhost:9003/writetagged -d '{\n  \nnamespace\n: \ndefault\n,\n  \nid\n: \nfoo\n,\n  \ntags\n: [\n    {\n      \nname\n: \ncity\n,\n      \nvalue\n: \nnew_york\n\n    },\n    {\n      \nname\n: \nendpoint\n,\n      \nvalue\n: \n/request\n\n    }\n  ],\n  \ndatapoint\n: {\n    \ntimestamp\n: '\n$(date \n+%s\n)\n',\n    \nvalue\n: 42.123456789\n  }\n}'\n\n\n\n\n$ curl -sSf -X POST http://localhost:9003/query -d '{\n  \nnamespace\n: \ndefault\n,\n  \nquery\n: {\n    \nregexp\n: {\n      \nfield\n: \ncity\n,\n      \nregexp\n: \n.*\n\n    }\n  },\n  \nrangeStart\n: 0,\n  \nrangeEnd\n: '\n$(date \n+%s\n)\n'\n}' | jq .\n\n{\n  \nresults\n: [\n    {\n      \nid\n: \nfoo\n,\n      \ntags\n: [\n        {\n          \nname\n: \ncity\n,\n          \nvalue\n: \nnew_york\n\n        },\n        {\n          \nname\n: \nendpoint\n,\n          \nvalue\n: \n/request\n\n        }\n      ],\n      \ndatapoints\n: [\n        {\n          \ntimestamp\n: 1527630053,\n          \nvalue\n: 42.123456789\n        }\n      ]\n    }\n  ],\n  \nexhaustive\n: true\n}\n\n\n\n\nAdding nodes\n\n\nYou can easily scale your M3DB cluster by scaling the StatefulSet and informing the cluster topology of the change:\n\n\nkubectl -n m3db scale --replicas=4 statefulset/m3dbnode\n\n\n\n\nOnce the pod is ready you can modify the cluster topology:\n\n\nkubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -\n 7201\nForwarding from [::1]:7201 -\n 7201\n\n\n\n\ncurl -sSf -X POST localhost:7201/api/v1/placement/add -d '{\n    \ninstances\n: [\n        {\n            \nid\n: \nm3dbnode-3\n,\n            \nisolation_group\n: \npod3\n,\n            \nzone\n: \nembedded\n,\n            \nweight\n: 100,\n            \nendpoint\n: \nm3dbnode-3.m3dbnode:9000\n,\n            \nhostname\n: \nm3dbnode-3.m3dbnode\n,\n            \nport\n: 9000\n        }\n    ]\n}'\n\n\n\n\nIntegrations\n\n\nPrometheus\n\n\nAs mentioned in our integrations \nguide\n, M3DB can be used as a \nremote read/write\nendpoint\n for Prometheus.\n\n\nIf you run Prometheus on your Kubernetes cluster you can easily point it at M3DB in your Prometheus server config:\n\n\nremote_read:\n  - url: \nhttp://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/read\n\n    # To test reading even when local Prometheus has the data\n    read_recent: true\n\nremote_write:\n  - url: \nhttp://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/write\n\n    # To differentiate between local and remote storage we will add a storage label\n    write_relabel_configs:\n      - target_label: metrics_storage\n        replacement: m3db_remote\n\n\n\n\nScheduling\n\n\nIn some cases you might prefer M3DB to run on certain nodes in your cluster. For example: if your cluster is comprised\nof different instance types and some have more memory than others then you'd like M3DB to run on those nodes if\npossible. To accommodate this, the pods created by the StatefulSets use \npod\naffinities\n and\n\ntolerations\n to prefer to run on\ncertain nodes. Specifically:\n\n\n\n\nThe pods tolerate the taint \n\"dedicated-m3db\"\n to run on nodes that are specifically dedicated to m3db if you so\n   choose.\n\n\nVia \nnodeAffinity\n the pods prefer to run on nodes with the label \nm3db.io/dedicated-m3db=\"true\"\n.", 
            "title": "M3DB on Kubernetes"
        }, 
        {
            "location": "/how_to/kubernetes/#m3db-on-kubernetes", 
            "text": "M3DB on Kubernetes is currently in the alpha phase of development. We currently provide static manifests to bootstrap a\ncluster. In the future we hope to create an  operator  and leverage  custom resource\ndefinitions  (CRDs) to automatically\nhandle operations such as managing cluster topology, but for now our manifests should be adequate to get started.", 
            "title": "M3DB on Kubernetes"
        }, 
        {
            "location": "/how_to/kubernetes/#prerequisites", 
            "text": "M3DB performs better when it has access to fast disks. Every incoming write is written to a commit log, which at high\nvolumes of writes can be sensitive to spikes in disk latency. Additionally the random seeks into files when loading cold\nfiles benefit from lower random read latency.  Because of this, the included manifests reference a StorageClass  named  fast . Manifests are\nprovided to provision such a StorageClass on AWS / Azure / GCP using the respective cloud provider's premium disk class.  If you do not already have a StorageClass named  fast , create one using one of the provided manifests:  # AWS EBS (class io1)\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-class-aws.yaml\n\n# Azure premium LRS\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-class-azure.yaml\n\n# GCE Persistent SSD\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-class-gcp.yaml  If you wish to use your cloud provider's default remote disk, or another disk class entirely, you'll have to modify the\nmanifests.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/how_to/kubernetes/#deploying", 
            "text": "Apply the following manifest to create your cluster:  kubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/bundle.yaml  Applying this bundle will create the following resources:   An  m3db   Namespace  for\n   all M3DB-related resources.  A 3-node etcd cluster in the form of a\n    StatefulSet  backed by persistent\n   remote SSDs. This cluster stores the DB topology and other runtime configuration data.  A 3-node M3DB cluster in the form of a StatefulSet.  Headless services  for\n   the etcd and m3db StatefulSets to provide stable DNS hostnames per-pod.   Wait until all created pods are listed as ready:  $ kubectl -n m3db get po\nNAME         READY     STATUS    RESTARTS   AGE\netcd-0       1/1       Running   0          22m\netcd-1       1/1       Running   0          22m\netcd-2       1/1       Running   0          22m\nm3dbnode-0   1/1       Running   0          22m\nm3dbnode-1   1/1       Running   0          22m\nm3dbnode-2   1/1       Running   0          22m  You can now proceed to initialize a namespace and placment for the cluster the same as you would for our other how-to\nguides:  # Open a local connection to the coordinator service:\n$ kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -  7201\nForwarding from [::1]:7201 -  7201  # Create an initial cluster topology\ncurl -sSf -X POST localhost:7201/api/v1/placement/init -d '{\n     num_shards : 1024,\n     replication_factor : 3,\n     instances : [\n        {\n             id :  m3dbnode-0 ,\n             isolation_group :  pod0 ,\n             zone :  embedded ,\n             weight : 100,\n             endpoint :  m3dbnode-0.m3dbnode:9000 ,\n             hostname :  m3dbnode-0.m3dbnode ,\n             port : 9000\n        },\n        {\n             id :  m3dbnode-1 ,\n             isolation_group :  pod1 ,\n             zone :  embedded ,\n             weight : 100,\n             endpoint :  m3dbnode-1.m3dbnode:9000 ,\n             hostname :  m3dbnode-1.m3dbnode ,\n             port : 9000\n        },\n        {\n             id :  m3dbnode-2 ,\n             isolation_group :  pod2 ,\n             zone :  embedded ,\n             weight : 100,\n             endpoint :  m3dbnode-2.m3dbnode:9000 ,\n             hostname :  m3dbnode-2.m3dbnode ,\n             port : 9000\n        }\n    ]\n}'  # Create a namespace to hold your metrics\ncurl -X POST localhost:7201/api/v1/namespace -d '{\n   name :  default ,\n   options : {\n     bootstrapEnabled : true,\n     flushEnabled : true,\n     writesToCommitLog : true,\n     cleanupEnabled : true,\n     snapshotEnabled : false,\n     repairEnabled : false,\n     retentionOptions : {\n       retentionPeriodDuration :  720h ,\n       blockSizeDuration :  12h ,\n       bufferFutureDuration :  1h ,\n       bufferPastDuration :  1h ,\n       blockDataExpiry : true,\n       blockDataExpiryAfterNotAccessPeriodDuration :  5m \n    },\n     indexOptions : {\n       enabled : true,\n       blockSizeDuration :  12h \n    }\n  }\n}'  Shortly after you should see your nodes finish bootstrapping:  $ kubectl -n m3db logs -f m3dbnode-0\n21:36:54.831698[I] cluster database initializing topology\n21:36:54.831732[I] cluster database resolving topology\n21:37:22.821740[I] resolving namespaces with namespace watch\n21:37:22.821813[I] updating database namespaces [{adds [metrics]} {updates []} {removals []}]\n21:37:23.008109[I] node tchannelthrift: listening on 0.0.0.0:9000\n21:37:23.008384[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n21:37:23.217090[I] node httpjson: listening on 0.0.0.0:9002\n21:37:23.217240[I] cluster httpjson: listening on 0.0.0.0:9003\n21:37:23.217526[I] bootstrapping shards for range starting [{run bootstrap-data} {bootstrapper filesystem} ...\n...\n21:37:23.239534[I] bootstrap data fetched now initializing shards with series blocks [{namespace metrics} {numShards 256} {numSeries 0}]\n21:37:23.240778[I] bootstrap finished [{namespace metrics} {duration 23.325194ms}]\n21:37:23.240856[I] bootstrapped\n21:37:29.733025[I] successfully updated topology to 3 hosts  You can now write and read metrics using the API on the db nodes:  $ kubectl -n m3db port-forward svc/m3dbnode 9003\nForwarding from 127.0.0.1:9003 -  9003\nForwarding from [::1]:9003 -  9003  curl -sSf -X POST localhost:9003/writetagged -d '{\n   namespace :  default ,\n   id :  foo ,\n   tags : [\n    {\n       name :  city ,\n       value :  new_york \n    },\n    {\n       name :  endpoint ,\n       value :  /request \n    }\n  ],\n   datapoint : {\n     timestamp : ' $(date  +%s ) ',\n     value : 42.123456789\n  }\n}'  $ curl -sSf -X POST http://localhost:9003/query -d '{\n   namespace :  default ,\n   query : {\n     regexp : {\n       field :  city ,\n       regexp :  .* \n    }\n  },\n   rangeStart : 0,\n   rangeEnd : ' $(date  +%s ) '\n}' | jq .\n\n{\n   results : [\n    {\n       id :  foo ,\n       tags : [\n        {\n           name :  city ,\n           value :  new_york \n        },\n        {\n           name :  endpoint ,\n           value :  /request \n        }\n      ],\n       datapoints : [\n        {\n           timestamp : 1527630053,\n           value : 42.123456789\n        }\n      ]\n    }\n  ],\n   exhaustive : true\n}", 
            "title": "Deploying"
        }, 
        {
            "location": "/how_to/kubernetes/#adding-nodes", 
            "text": "You can easily scale your M3DB cluster by scaling the StatefulSet and informing the cluster topology of the change:  kubectl -n m3db scale --replicas=4 statefulset/m3dbnode  Once the pod is ready you can modify the cluster topology:  kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -  7201\nForwarding from [::1]:7201 -  7201  curl -sSf -X POST localhost:7201/api/v1/placement/add -d '{\n     instances : [\n        {\n             id :  m3dbnode-3 ,\n             isolation_group :  pod3 ,\n             zone :  embedded ,\n             weight : 100,\n             endpoint :  m3dbnode-3.m3dbnode:9000 ,\n             hostname :  m3dbnode-3.m3dbnode ,\n             port : 9000\n        }\n    ]\n}'", 
            "title": "Adding nodes"
        }, 
        {
            "location": "/how_to/kubernetes/#integrations", 
            "text": "", 
            "title": "Integrations"
        }, 
        {
            "location": "/how_to/kubernetes/#prometheus", 
            "text": "As mentioned in our integrations  guide , M3DB can be used as a  remote read/write\nendpoint  for Prometheus.  If you run Prometheus on your Kubernetes cluster you can easily point it at M3DB in your Prometheus server config:  remote_read:\n  - url:  http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/read \n    # To test reading even when local Prometheus has the data\n    read_recent: true\n\nremote_write:\n  - url:  http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/write \n    # To differentiate between local and remote storage we will add a storage label\n    write_relabel_configs:\n      - target_label: metrics_storage\n        replacement: m3db_remote", 
            "title": "Prometheus"
        }, 
        {
            "location": "/how_to/kubernetes/#scheduling", 
            "text": "In some cases you might prefer M3DB to run on certain nodes in your cluster. For example: if your cluster is comprised\nof different instance types and some have more memory than others then you'd like M3DB to run on those nodes if\npossible. To accommodate this, the pods created by the StatefulSets use  pod\naffinities  and tolerations  to prefer to run on\ncertain nodes. Specifically:   The pods tolerate the taint  \"dedicated-m3db\"  to run on nodes that are specifically dedicated to m3db if you so\n   choose.  Via  nodeAffinity  the pods prefer to run on nodes with the label  m3db.io/dedicated-m3db=\"true\" .", 
            "title": "Scheduling"
        }, 
        {
            "location": "/integrations/prometheus/", 
            "text": "Prometheus\n\n\nThis document is a getting started guide to integrating M3DB with Prometheus.\n\n\nM3 Coordinator configuration\n\n\nTo write to a remote M3DB cluster the simplest configuration is to run \nm3coordinator\n as a sidecar alongside Prometheus.\n\n\nStart by downloading the \nconfig template\n. Update the \nnamespaces\n and the \nclient\n section for a new cluster to match your cluster's configuration.\n\n\nYou'll need to specify the static IPs or hostnames of your M3DB seed nodes, and the name and retention values of the namespace you set up.  You can leave the namespace storage metrics type as \nunaggregated\n since it's required by default to have a cluster that receives all Prometheus metrics unaggregated.  In the future you might also want to aggregate and downsample metrics for longer retention, and you can come back and update the config once you've setup those clusters.\n\n\nIt should look something like:\n\n\nlistenAddress: 0.0.0.0:7201\n\nmetrics:\n  scope:\n    prefix: \ncoordinator\n\n  prometheus:\n    handlerPath: /metrics\n    listenAddress: 0.0.0.0:7203 # until https://github.com/m3db/m3/issues/682 is resolved\n  sanitization: prometheus\n  samplingRate: 1.0\n  extended: none\n\nclusters:\n   - namespaces:\n# We created a namespace called \ndefault\n and had set it to retention \n48h\n.\n       - namespace: default\n         retention: 48h\n         storageMetricsType: unaggregated\n     client:\n       config:\n         service:\n           env: default_env\n           zone: embedded\n           service: m3db\n           cacheDir: /var/lib/m3kv\n           etcdClusters:\n             - zone: embedded\n               endpoints:\n# We have five M3DB nodes but only three are seed nodes, they are listed here.\n                 - M3DB_NODE_01_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_02_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_03_STATIC_IP_ADDRESS:2379\n       writeConsistencyLevel: majority\n       readConsistencyLevel: unstrict_majority\n       writeTimeout: 10s\n       fetchTimeout: 15s\n       connectTimeout: 20s\n       writeRetry:\n         initialBackoff: 500ms\n         backoffFactor: 3\n         maxRetries: 2\n         jitter: true\n       fetchRetry:\n         initialBackoff: 500ms\n         backoffFactor: 2\n         maxRetries: 3\n         jitter: true\n       backgroundHealthCheckFailLimit: 4\n       backgroundHealthCheckFailThrottleFactor: 0.5\n\n\n\n\n\nNow start the process up:\n\n\nm3coordinator -f \nconfig-name.yml\n\n\n\n\n\nOr, use the docker container:\n\n\ndocker pull quay.io/m3/m3coordinator:latest\ndocker run -p 7201:7201 --name m3coordinator -v \nconfig-name.yml\n:/etc/m3coordinator/m3coordinator.yml quay.io/m3/m3coordinator:latest\n\n\n\n\nPrometheus configuration\n\n\nAdd to your Prometheus configuration the \nm3coordinator\n sidecar remote read/write endpoints, something like:\n\n\nremote_read:\n  - url: \nhttp://localhost:7201/api/v1/prom/remote/read\n\n    # To test reading even when local Prometheus has the data\n    read_recent: true\nremote_write:\n  - url: \nhttp://localhost:7201/api/v1/prom/remote/write", 
            "title": "Prometheus"
        }, 
        {
            "location": "/integrations/prometheus/#prometheus", 
            "text": "This document is a getting started guide to integrating M3DB with Prometheus.", 
            "title": "Prometheus"
        }, 
        {
            "location": "/integrations/prometheus/#m3-coordinator-configuration", 
            "text": "To write to a remote M3DB cluster the simplest configuration is to run  m3coordinator  as a sidecar alongside Prometheus.  Start by downloading the  config template . Update the  namespaces  and the  client  section for a new cluster to match your cluster's configuration.  You'll need to specify the static IPs or hostnames of your M3DB seed nodes, and the name and retention values of the namespace you set up.  You can leave the namespace storage metrics type as  unaggregated  since it's required by default to have a cluster that receives all Prometheus metrics unaggregated.  In the future you might also want to aggregate and downsample metrics for longer retention, and you can come back and update the config once you've setup those clusters.  It should look something like:  listenAddress: 0.0.0.0:7201\n\nmetrics:\n  scope:\n    prefix:  coordinator \n  prometheus:\n    handlerPath: /metrics\n    listenAddress: 0.0.0.0:7203 # until https://github.com/m3db/m3/issues/682 is resolved\n  sanitization: prometheus\n  samplingRate: 1.0\n  extended: none\n\nclusters:\n   - namespaces:\n# We created a namespace called  default  and had set it to retention  48h .\n       - namespace: default\n         retention: 48h\n         storageMetricsType: unaggregated\n     client:\n       config:\n         service:\n           env: default_env\n           zone: embedded\n           service: m3db\n           cacheDir: /var/lib/m3kv\n           etcdClusters:\n             - zone: embedded\n               endpoints:\n# We have five M3DB nodes but only three are seed nodes, they are listed here.\n                 - M3DB_NODE_01_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_02_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_03_STATIC_IP_ADDRESS:2379\n       writeConsistencyLevel: majority\n       readConsistencyLevel: unstrict_majority\n       writeTimeout: 10s\n       fetchTimeout: 15s\n       connectTimeout: 20s\n       writeRetry:\n         initialBackoff: 500ms\n         backoffFactor: 3\n         maxRetries: 2\n         jitter: true\n       fetchRetry:\n         initialBackoff: 500ms\n         backoffFactor: 2\n         maxRetries: 3\n         jitter: true\n       backgroundHealthCheckFailLimit: 4\n       backgroundHealthCheckFailThrottleFactor: 0.5  Now start the process up:  m3coordinator -f  config-name.yml   Or, use the docker container:  docker pull quay.io/m3/m3coordinator:latest\ndocker run -p 7201:7201 --name m3coordinator -v  config-name.yml :/etc/m3coordinator/m3coordinator.yml quay.io/m3/m3coordinator:latest", 
            "title": "M3 Coordinator configuration"
        }, 
        {
            "location": "/integrations/prometheus/#prometheus-configuration", 
            "text": "Add to your Prometheus configuration the  m3coordinator  sidecar remote read/write endpoints, something like:  remote_read:\n  - url:  http://localhost:7201/api/v1/prom/remote/read \n    # To test reading even when local Prometheus has the data\n    read_recent: true\nremote_write:\n  - url:  http://localhost:7201/api/v1/prom/remote/write", 
            "title": "Prometheus configuration"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/faqs/", 
            "text": "FAQs", 
            "title": "FAQs"
        }, 
        {
            "location": "/faqs/#faqs", 
            "text": "", 
            "title": "FAQs"
        }
    ]
}