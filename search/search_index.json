{
    "docs": [
        {
            "location": "/", 
            "text": "M3DB\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nAbout\n\n\nM3DB is a metrics platform that is built around the distributed timeseries database M3DB. The top level project repository can be found \nhere\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#m3db", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "M3DB"
        }, 
        {
            "location": "/#about", 
            "text": "M3DB is a metrics platform that is built around the distributed timeseries database M3DB. The top level project repository can be found  here .", 
            "title": "About"
        }, 
        {
            "location": "/introduction/components/components/", 
            "text": "Components", 
            "title": "Components"
        }, 
        {
            "location": "/introduction/components/components/#components", 
            "text": "", 
            "title": "Components"
        }, 
        {
            "location": "/introduction/motivation/motivation/", 
            "text": "Motivation", 
            "title": "Motivation"
        }, 
        {
            "location": "/introduction/motivation/motivation/#motivation", 
            "text": "", 
            "title": "Motivation"
        }, 
        {
            "location": "/m3db/", 
            "text": "M3DB, a distributed time series database\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nAbout\n\n\nM3DB, inspired by \nGorilla\n and \nInfluxDB\n, is a distributed time series database released as open source by \nUber Technologies\n. It can be used for storing realtime metrics at long retention.\n\n\nHere are some attributes of the project:\n\n\n\n\nDistributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently\n\n\nCluster management built on top of \netcd\n\n\nBuilt-in synchronous replication with configurable durability and read consistency (one, majority, all, etc)\n\n\nM3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy\n\n\nArbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write\n\n\nConfigurable out of order writes, currently limited to the size of the configured time window's block size\n\n\n\n\nCurrent Limitations\n\n\nDue to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.\n\n\nThe project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours).  As such out of order writes are limited to the size of a single compressed time series window.  Consequently backfilling large amounts of data is not currently possible.\n\n\nThe project has also has optimized for the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.", 
            "title": "Introduction"
        }, 
        {
            "location": "/m3db/#m3db-a-distributed-time-series-database", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "M3DB, a distributed time series database"
        }, 
        {
            "location": "/m3db/#about", 
            "text": "M3DB, inspired by  Gorilla  and  InfluxDB , is a distributed time series database released as open source by  Uber Technologies . It can be used for storing realtime metrics at long retention.  Here are some attributes of the project:   Distributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently  Cluster management built on top of  etcd  Built-in synchronous replication with configurable durability and read consistency (one, majority, all, etc)  M3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy  Arbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write  Configurable out of order writes, currently limited to the size of the configured time window's block size", 
            "title": "About"
        }, 
        {
            "location": "/m3db/#current-limitations", 
            "text": "Due to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.  The project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours).  As such out of order writes are limited to the size of a single compressed time series window.  Consequently backfilling large amounts of data is not currently possible.  The project has also has optimized for the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.", 
            "title": "Current Limitations"
        }, 
        {
            "location": "/m3db/architecture/", 
            "text": "Architecture\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nM3DB is written entirely in Go and has a single dependency of etcd which is used for cluster membership and topology definition. In the near future we plan to also support the concept of seed nodes that will run an embedded etcd server, as etcd can be run in an embedded fashion reasonably simply and statically compiled into the M3DB binary itself.\n\n\nHigh Level Goals\n\n\nSome of the high level goals for the project are defined as:\n\n\n\n\n\n\nMonitoring support:\n M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.\n\n\n\n\n\n\nHighly configurable:\n Provide a high level of configuration to support a wide set of use cases and runtime environments.\n\n\n\n\n\n\nVariable durability:\n Providing variable durability guarentees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarentees that data was replicated to a quorum of nodes and that the data was durable if desired.", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/#architecture", 
            "text": "Please note:  This documentation is a work in progress and more detail is required.", 
            "title": "Architecture"
        }, 
        {
            "location": "/m3db/architecture/#overview", 
            "text": "M3DB is written entirely in Go and has a single dependency of etcd which is used for cluster membership and topology definition. In the near future we plan to also support the concept of seed nodes that will run an embedded etcd server, as etcd can be run in an embedded fashion reasonably simply and statically compiled into the M3DB binary itself.", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/#high-level-goals", 
            "text": "Some of the high level goals for the project are defined as:    Monitoring support:  M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.    Highly configurable:  Provide a high level of configuration to support a wide set of use cases and runtime environments.    Variable durability:  Providing variable durability guarentees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarentees that data was replicated to a quorum of nodes and that the data was durable if desired.", 
            "title": "High Level Goals"
        }, 
        {
            "location": "/m3db/architecture/engine/", 
            "text": "Storage Engine Overview\n\n\nM3DB is a time series database that was primarily designed to be horizontally scalable and handle a large volume of monitoring time series data.\n\n\nTime Series Compression (M3TSZ)\n\n\nOne of M3DB's biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. This high compression ratio is implemented via the M3TSZ algorithm, a variant of the streaming time series compression algorithm described in \nFacebook's Gorilla paper\n with a few small differences.\n\n\nThe compression ratio will vary depending on the workload and configuration, but we found that with M3TSZ we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber's production workloads. This was a 40% improvement over standard TSZ which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.\n\n\nArchitecture\n\n\nM3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.\n\n\nIn-Memory Object Layout\n\n\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           Database            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n   \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          Namespace 1          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n   \u2502     \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502      \u2502\n   \u2502     \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  Shard 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Series 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [2PM - 4PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [4PM - 6PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   Blocks   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502     Block [6PM - 8PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502 Active Buffers (encoders)  \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n   \u2502                                                                 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nThe in-memory portion of M3DB is implemented via a hierarchy of objects:\n\n\n\n\n\n\nA \ndatabase\n of which there is only one per M3DB process.\n\n\n\n\n\n\nA \ndatabase\n \"owns\" numerous namespaces, and each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize (which we will discuss in more detail later). \nNamespaces\n are similar to tables in other databases.\n\n\n\n\n\n\nShards\n which are owned by \nnamespaces\n. \nShards\n are effectively the same as \"virtual shards\" in Cassandra in that they provide arbitrary distribution of time series data via a simple hash of the series ID.\n\n\n\n\n\n\nSeries\n which are owned by \nshards\n. A \nseries\n is generally what comes to mind when you think of \"time series\" data. Ex. The CPU level for a single host in a datacenter over a period of time could be represented as a series with id \"\n.system.cpu.utilization\" and a vector of tuples in the form of (TIMESTAMP, CPU_LEVEL). In other words, if you were rendering a graph a series would represent a single line on that graph. Note that the previous example is only a logical illustration and does not represent the way that M3DB actually stores data.\n\n\n\n\n\n\nBlocks\n belong to a series and are central to M3DB's design. A \nblock\n is simply a smaller wrapper object around a sealed (no longer writable) stream of compressed time series data. The compression comes with a few caveats though, namely that you cannot read individual datapoints in a compressed block. In other words, in order to read a single datapoint you must decompress the entire block up to the datapoint that you're trying to read.\n\n\n\n\n\n\nIf M3DB kept everything in memory (and in fact, early versions of it did), than you could conceptually think of it as being a composed from a hierarchy of maps:\n\n\ndatabase_obect      =\n map\n\nnamespace_object    =\n map\n\nshard_object        =\n map\n\nseries_object       =\n map\n\nseries_object       =\n map\n (This map should only have one or two entries)\n\n\nPersistent storage\n\n\nWhile in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy then it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.\n\n\nIn addition, with large volumes of data it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \"write-once, read-never\" pattern where less than a few percent of all the data that's stored is ever read. With that type of workload, its wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.\n\n\nLike most other databases, M3DB takes a two-pronged approach to persistant storage that involves combining a commitlog (for disaster recovery) with periodic snapshotting (for efficient retrieval):\n\n\n1) All writes are persisted to a \ncommitlog\n (the commitlog can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commitlog is completely uncompressed and exists only to recover \"unflushed\" data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request.\n2) Periodically (based on the configured blocksize) all \"active\" blocks are \"sealed\" (marked as immutable) and flushed to disk as \n\"fileset\" files\n. These files are highly compressed and can be indexed into via their complementary index files. Check out the \nflushing section\n to learn more about the background flushing process.\n\n\nThe blocksize parameter is the most important variable that needs to be turned for your particular workload. A small blocksize will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and your data will take up more space on disk.\n\n\nIf the database is stopped for any reason in-between \"flushes\" (writing fileset files out to disk), then when the node is started back up those writes will need to be recovered by reading the commitlog or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than 1.)\n\n\nWhile the \nfileset files\n are designed to support efficient data retrieval via the series primary key (the ID), there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB support various \ncaching policies\n which can significantly improve the performance of reads by caching data in memory.\n\n\nWrite Path\n\n\nWe now have enough context of M3DB's architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the \nwriteBatchRaw\n endpoint on M3DB's embedded thrift server. The write itself will contain the following information:\n\n\n\n\nThe namespace\n\n\nThe series ID (byte blob)\n\n\nThe timestamp\n\n\nThe value itself\n\n\n\n\nM3DB will consult the database object to check if the namespace exists, and if it does,then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then it will lookup the series corresponding encoder and encode the datapoint into the compressed stream. If the encoder doesn't exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some special logic for handling out-of-order writes which is discussed in the \nmerging all encoders section\n.\n\n\nAt the same time, the write will be appended to the commitlog queue (and depending on the commitlog configuration immediately fsync'd to disk or batched together with other writes and flushed out all at once).\n\n\nThe write will exist only in this \"active buffer\" and the commitlog until the block ends and is flushed to disk, at which point the write will exist in a fileset file for efficient storage and retrieval later and the commitlog entry can be garbage collected.\n\n\nNote:\n Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured \nconsistency level\n.\n\n\nRead Path\n\n\nA read begins when an M3DB client calls the \nFetchBatchResult\n or \nFetchBlocksRawResult\n endpoint on M3DB's embedded thrift server. The read request will contain the following information:\n\n\n\n\nThe namespace\n\n\nThe series ID (byte blob)\n\n\nThe period of time being requested (start and end)\n\n\n\n\nM3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:\n\n\n\n\nDoes the series exist? and if it does\n\n\nDoes the data exist in an \"active buffer\" (actively being compressed by an encoder), cached in-memory, on disk, or some combination of all three?\n\n\n\n\nDetermining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn't, then M3DB consults an in-memory bloom filter(s) for that shard / block start combination(s) to determine if the series exists on disk.\n\n\nIf the series exists, then for every block that the request spans, M3DB needs to consolidate data from the active buffers, in-memory cache, and fileset files (disk).\n\n\nLets imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a blocksize of 2 hours (i.e we need to find 3 different blocks.)\n\n\nIf the current time is 8PM, then the location of the requested blocks might be as follows:\n\n\n[2PM - 4PM (FileSet file)] - Sealed and flushed block that isn't cached\n[4PM - 6PM (In-memory cache)] - Sealed and flush block that is cached\n[6PM - 8PM (active buffer)] - Hasn't been sealed or flushed yet\n\n\n\n\nThen M3DB will need to consolidate:\n\n\n1) The not-yet-sealed block from the active buffers / encoders (located inside an internal lookup in the Series object) \n[6PM - 8PM]\n\n2) The in-memory cached block (also located inside an internal lookup in the Series object) \n[4PM - 6PM]\n\n3) The block from disk (the block retrieve from disk will then be cached according to the current \ncaching policy\n \n[2PM - 4PM]\n\n\nRetrieving blocks from the active buffers and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:\n\n\n\n\nConsult the in-memory bloom filter to determine if its possible the series exists on disk.\n\n\nIf the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is \nbefore\n the series ID that we're searching for. Review the \nindex_lookup.go\n file for implementation details.\n\n\nJump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we're looking for \nor\n we get far enough in the index file that it becomes clear that the ID we're looking for doesn't exist (this is possible because the index file is sorted by ID)\n\n\nJump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.\n\n\n\n\nOnce M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or the client returns a success to the caller for the read is dependent on the configured \nconsistency level\n.\n\n\nNote:\n Since M3DB nodes return compressed blocks (the M3DB client decompresses them) its not possible to return \"partial results\" for a given block. If any portion of a read requests spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.\n\n\nBackground processes\n\n\nM3DB has a variety of processes that run in the background during normal operation.\n\n\nTicking\n\n\nThe ticking process runs continously in the background and is responsible for a variety of tasks:\n\n\n\n\nMerging all encoders for a given series / block start combination\n\n\nRemoving expired / flushed series and blocks from memory\n\n\nCleanup of expired data (fileset/commit log) from the filesystem\n\n\n\n\nMerging all encoders\n\n\nM3TSZ is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For monitoring workloads this works very well because every subsequent datapoint is almost always larger than the previous one. However, real world systems are messy and occassionally out of order writes will be received. When this happens, M3DB will allocate a new encoder for the out of order datapoints. The multiple encoders need to be merged before flushing the data to disk, but to prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.\n\n\nRemoving expired / flushed series and blocks from memory\n\n\nDepending on the configured \ncaching policy\n, the \nin-memory object layout\n can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer need to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.\n\n\nFlushing\n\n\nAs discussed in the \narchitecture\n section, writes are actively buffered / compressed in-memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of \nfileset files\n to facilitate efficient storage and retrieval.\n\n\nThis is where the configurable \"blocksize\" comes into play. The blocksize is simply a duration of time that dictates how long active writes will be compressed (in a streaming manner) in memory before being \"sealed\" (marked as immutable) and flushed to disk. Lets use a blocksize of two hours as an example.\n\n\nIf the blocksize is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the \nfileset files\n will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.\n\n\nCaveats / Limitations\n\n\n\n\nM3DB currently supports exact ID based lookups. It does not support tag/secondary indexing. This feature is under development and future versions of M3DB will have support for a built-in reverse index.\n\n\nM3DB does not support updates / deletes. All data written to M3DB is immutable.\n\n\nM3DB does not support writing arbitrarily into the past and future. This is generally fine for monitoring workloads, but can be problematic for traditional \nOLTP\n and \nOLAP\n workloads. Future versions of M3DB will have better support for writes with arbitrary timestamps.\n\n\nM3DB does not support writing datapoints with values other than double-precision floats. Future versions of M3DB will have support for storing arbitrary values.\n\n\nM3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value (Uber has production databases running with retention periods as high as 5 years), its still required and generally speaking M3DB is optimized for workloads with a well-defined \nTTL\n.\n\n\nM3DB does not support either background data repair or Cassandra-style \nread repairs\n. Future versions of M3DB will support automatic repairs of data as an ongoing background process.", 
            "title": "Storage Engine"
        }, 
        {
            "location": "/m3db/architecture/engine/#storage-engine-overview", 
            "text": "M3DB is a time series database that was primarily designed to be horizontally scalable and handle a large volume of monitoring time series data.", 
            "title": "Storage Engine Overview"
        }, 
        {
            "location": "/m3db/architecture/engine/#time-series-compression-m3tsz", 
            "text": "One of M3DB's biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. This high compression ratio is implemented via the M3TSZ algorithm, a variant of the streaming time series compression algorithm described in  Facebook's Gorilla paper  with a few small differences.  The compression ratio will vary depending on the workload and configuration, but we found that with M3TSZ we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber's production workloads. This was a 40% improvement over standard TSZ which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.", 
            "title": "Time Series Compression (M3TSZ)"
        }, 
        {
            "location": "/m3db/architecture/engine/#architecture", 
            "text": "M3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.", 
            "title": "Architecture"
        }, 
        {
            "location": "/m3db/architecture/engine/#in-memory-object-layout", 
            "text": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           Database            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n   \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          Namespace 1          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n   \u2502     \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502      \u2502\n   \u2502     \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  Shard 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Series 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [2PM - 4PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [4PM - 6PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   Blocks   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502     Block [6PM - 8PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502 Active Buffers (encoders)  \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n   \u2502                                                                 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  The in-memory portion of M3DB is implemented via a hierarchy of objects:    A  database  of which there is only one per M3DB process.    A  database  \"owns\" numerous namespaces, and each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize (which we will discuss in more detail later).  Namespaces  are similar to tables in other databases.    Shards  which are owned by  namespaces .  Shards  are effectively the same as \"virtual shards\" in Cassandra in that they provide arbitrary distribution of time series data via a simple hash of the series ID.    Series  which are owned by  shards . A  series  is generally what comes to mind when you think of \"time series\" data. Ex. The CPU level for a single host in a datacenter over a period of time could be represented as a series with id \" .system.cpu.utilization\" and a vector of tuples in the form of (TIMESTAMP, CPU_LEVEL). In other words, if you were rendering a graph a series would represent a single line on that graph. Note that the previous example is only a logical illustration and does not represent the way that M3DB actually stores data.    Blocks  belong to a series and are central to M3DB's design. A  block  is simply a smaller wrapper object around a sealed (no longer writable) stream of compressed time series data. The compression comes with a few caveats though, namely that you cannot read individual datapoints in a compressed block. In other words, in order to read a single datapoint you must decompress the entire block up to the datapoint that you're trying to read.    If M3DB kept everything in memory (and in fact, early versions of it did), than you could conceptually think of it as being a composed from a hierarchy of maps:  database_obect      =  map \nnamespace_object    =  map \nshard_object        =  map \nseries_object       =  map \nseries_object       =  map  (This map should only have one or two entries)", 
            "title": "In-Memory Object Layout"
        }, 
        {
            "location": "/m3db/architecture/engine/#persistent-storage", 
            "text": "While in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy then it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.  In addition, with large volumes of data it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \"write-once, read-never\" pattern where less than a few percent of all the data that's stored is ever read. With that type of workload, its wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.  Like most other databases, M3DB takes a two-pronged approach to persistant storage that involves combining a commitlog (for disaster recovery) with periodic snapshotting (for efficient retrieval):  1) All writes are persisted to a  commitlog  (the commitlog can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commitlog is completely uncompressed and exists only to recover \"unflushed\" data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request.\n2) Periodically (based on the configured blocksize) all \"active\" blocks are \"sealed\" (marked as immutable) and flushed to disk as  \"fileset\" files . These files are highly compressed and can be indexed into via their complementary index files. Check out the  flushing section  to learn more about the background flushing process.  The blocksize parameter is the most important variable that needs to be turned for your particular workload. A small blocksize will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and your data will take up more space on disk.  If the database is stopped for any reason in-between \"flushes\" (writing fileset files out to disk), then when the node is started back up those writes will need to be recovered by reading the commitlog or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than 1.)  While the  fileset files  are designed to support efficient data retrieval via the series primary key (the ID), there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB support various  caching policies  which can significantly improve the performance of reads by caching data in memory.", 
            "title": "Persistent storage"
        }, 
        {
            "location": "/m3db/architecture/engine/#write-path", 
            "text": "We now have enough context of M3DB's architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the  writeBatchRaw  endpoint on M3DB's embedded thrift server. The write itself will contain the following information:   The namespace  The series ID (byte blob)  The timestamp  The value itself   M3DB will consult the database object to check if the namespace exists, and if it does,then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then it will lookup the series corresponding encoder and encode the datapoint into the compressed stream. If the encoder doesn't exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some special logic for handling out-of-order writes which is discussed in the  merging all encoders section .  At the same time, the write will be appended to the commitlog queue (and depending on the commitlog configuration immediately fsync'd to disk or batched together with other writes and flushed out all at once).  The write will exist only in this \"active buffer\" and the commitlog until the block ends and is flushed to disk, at which point the write will exist in a fileset file for efficient storage and retrieval later and the commitlog entry can be garbage collected.  Note:  Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured  consistency level .", 
            "title": "Write Path"
        }, 
        {
            "location": "/m3db/architecture/engine/#read-path", 
            "text": "A read begins when an M3DB client calls the  FetchBatchResult  or  FetchBlocksRawResult  endpoint on M3DB's embedded thrift server. The read request will contain the following information:   The namespace  The series ID (byte blob)  The period of time being requested (start and end)   M3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:   Does the series exist? and if it does  Does the data exist in an \"active buffer\" (actively being compressed by an encoder), cached in-memory, on disk, or some combination of all three?   Determining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn't, then M3DB consults an in-memory bloom filter(s) for that shard / block start combination(s) to determine if the series exists on disk.  If the series exists, then for every block that the request spans, M3DB needs to consolidate data from the active buffers, in-memory cache, and fileset files (disk).  Lets imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a blocksize of 2 hours (i.e we need to find 3 different blocks.)  If the current time is 8PM, then the location of the requested blocks might be as follows:  [2PM - 4PM (FileSet file)] - Sealed and flushed block that isn't cached\n[4PM - 6PM (In-memory cache)] - Sealed and flush block that is cached\n[6PM - 8PM (active buffer)] - Hasn't been sealed or flushed yet  Then M3DB will need to consolidate:  1) The not-yet-sealed block from the active buffers / encoders (located inside an internal lookup in the Series object)  [6PM - 8PM] \n2) The in-memory cached block (also located inside an internal lookup in the Series object)  [4PM - 6PM] \n3) The block from disk (the block retrieve from disk will then be cached according to the current  caching policy   [2PM - 4PM]  Retrieving blocks from the active buffers and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:   Consult the in-memory bloom filter to determine if its possible the series exists on disk.  If the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is  before  the series ID that we're searching for. Review the  index_lookup.go  file for implementation details.  Jump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we're looking for  or  we get far enough in the index file that it becomes clear that the ID we're looking for doesn't exist (this is possible because the index file is sorted by ID)  Jump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.   Once M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or the client returns a success to the caller for the read is dependent on the configured  consistency level .  Note:  Since M3DB nodes return compressed blocks (the M3DB client decompresses them) its not possible to return \"partial results\" for a given block. If any portion of a read requests spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.", 
            "title": "Read Path"
        }, 
        {
            "location": "/m3db/architecture/engine/#background-processes", 
            "text": "M3DB has a variety of processes that run in the background during normal operation.", 
            "title": "Background processes"
        }, 
        {
            "location": "/m3db/architecture/engine/#ticking", 
            "text": "The ticking process runs continously in the background and is responsible for a variety of tasks:   Merging all encoders for a given series / block start combination  Removing expired / flushed series and blocks from memory  Cleanup of expired data (fileset/commit log) from the filesystem", 
            "title": "Ticking"
        }, 
        {
            "location": "/m3db/architecture/engine/#merging-all-encoders", 
            "text": "M3TSZ is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For monitoring workloads this works very well because every subsequent datapoint is almost always larger than the previous one. However, real world systems are messy and occassionally out of order writes will be received. When this happens, M3DB will allocate a new encoder for the out of order datapoints. The multiple encoders need to be merged before flushing the data to disk, but to prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.", 
            "title": "Merging all encoders"
        }, 
        {
            "location": "/m3db/architecture/engine/#removing-expired-flushed-series-and-blocks-from-memory", 
            "text": "Depending on the configured  caching policy , the  in-memory object layout  can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer need to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.", 
            "title": "Removing expired / flushed series and blocks from memory"
        }, 
        {
            "location": "/m3db/architecture/engine/#flushing", 
            "text": "As discussed in the  architecture  section, writes are actively buffered / compressed in-memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of  fileset files  to facilitate efficient storage and retrieval.  This is where the configurable \"blocksize\" comes into play. The blocksize is simply a duration of time that dictates how long active writes will be compressed (in a streaming manner) in memory before being \"sealed\" (marked as immutable) and flushed to disk. Lets use a blocksize of two hours as an example.  If the blocksize is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the  fileset files  will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.", 
            "title": "Flushing"
        }, 
        {
            "location": "/m3db/architecture/engine/#caveats-limitations", 
            "text": "M3DB currently supports exact ID based lookups. It does not support tag/secondary indexing. This feature is under development and future versions of M3DB will have support for a built-in reverse index.  M3DB does not support updates / deletes. All data written to M3DB is immutable.  M3DB does not support writing arbitrarily into the past and future. This is generally fine for monitoring workloads, but can be problematic for traditional  OLTP  and  OLAP  workloads. Future versions of M3DB will have better support for writes with arbitrary timestamps.  M3DB does not support writing datapoints with values other than double-precision floats. Future versions of M3DB will have support for storing arbitrary values.  M3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value (Uber has production databases running with retention periods as high as 5 years), its still required and generally speaking M3DB is optimized for workloads with a well-defined  TTL .  M3DB does not support either background data repair or Cassandra-style  read repairs . Future versions of M3DB will support automatic repairs of data as an ongoing background process.", 
            "title": "Caveats / Limitations"
        }, 
        {
            "location": "/m3db/architecture/sharding/", 
            "text": "Sharding\n\n\nTimeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default \nmurmur3\n is used as the hashing function and 4096 virtual shards are configured.\n\n\nBenefits\n\n\nShards provide a variety of benefits throughout the M3DB stack:\n\n\n\n\nThey make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level.\n\n\nThey provide more fine grained lock granularity at the memory level.\n\n\nThey inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.\n\n\n\n\nReplication\n\n\nLogical shards are placed on per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data are distinct to the racks that locate all other replica\u2019s data.\n\n\nReplication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.\n\n\nReplica\n\n\nEach replica has its own assignment of a single logical shard per virtual shard.\n\n\nConceptually it can be defined as:\n\n\nReplica {\n  id uint32\n  shards []Shard\n}\n\n\n\n\nShard state\n\n\nEach shard can be conceptually defined as:\n\n\nShard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}\n\n\n\n\nShard assignment\n\n\nThe assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.\n\n\nFor a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as \nLEAVING\n and another host assigned as \nINITIALIZING\n for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only \nAVAILABLE\n shards count towards consistency, the work to group the \nLEAVING\n and \nINITIALIZING\n shards together when calculating a write success/error is not complete, see \nissue 417\n.\n\n\nIt is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the \nINITIALIZING\n state and to transition the state to \nAVAILABLE\n once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the \nLEAVING\n shard still assigned to the node that previously owned it and transitions the shard state on the new node from \nINITIALIZING\n state to \nAVAILABLE\n.\n\n\nNodes will not start serving reads for the new shard until it is \nAVAILABLE\n, meaning not until they have bootstrapped data for those shards.\n\n\nCluster operations\n\n\nNode add\n\n\nWhen a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become \nINITIALIZING\n, the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as \nLEAVING\n.\n\n\nNode down\n\n\nA node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.\n\n\nNode remove\n\n\nWhen a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are \nINITIALIZING\n and need to be bootstrapped and will begin bootstrapping the data using all replicas available.", 
            "title": "Sharding and Replication"
        }, 
        {
            "location": "/m3db/architecture/sharding/#sharding", 
            "text": "Timeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default  murmur3  is used as the hashing function and 4096 virtual shards are configured.", 
            "title": "Sharding"
        }, 
        {
            "location": "/m3db/architecture/sharding/#benefits", 
            "text": "Shards provide a variety of benefits throughout the M3DB stack:   They make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level.  They provide more fine grained lock granularity at the memory level.  They inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.", 
            "title": "Benefits"
        }, 
        {
            "location": "/m3db/architecture/sharding/#replication", 
            "text": "Logical shards are placed on per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data are distinct to the racks that locate all other replica\u2019s data.  Replication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.", 
            "title": "Replication"
        }, 
        {
            "location": "/m3db/architecture/sharding/#replica", 
            "text": "Each replica has its own assignment of a single logical shard per virtual shard.  Conceptually it can be defined as:  Replica {\n  id uint32\n  shards []Shard\n}", 
            "title": "Replica"
        }, 
        {
            "location": "/m3db/architecture/sharding/#shard-state", 
            "text": "Each shard can be conceptually defined as:  Shard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}", 
            "title": "Shard state"
        }, 
        {
            "location": "/m3db/architecture/sharding/#shard-assignment", 
            "text": "The assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.  For a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as  LEAVING  and another host assigned as  INITIALIZING  for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only  AVAILABLE  shards count towards consistency, the work to group the  LEAVING  and  INITIALIZING  shards together when calculating a write success/error is not complete, see  issue 417 .  It is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the  INITIALIZING  state and to transition the state to  AVAILABLE  once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the  LEAVING  shard still assigned to the node that previously owned it and transitions the shard state on the new node from  INITIALIZING  state to  AVAILABLE .  Nodes will not start serving reads for the new shard until it is  AVAILABLE , meaning not until they have bootstrapped data for those shards.", 
            "title": "Shard assignment"
        }, 
        {
            "location": "/m3db/architecture/sharding/#cluster-operations", 
            "text": "", 
            "title": "Cluster operations"
        }, 
        {
            "location": "/m3db/architecture/sharding/#node-add", 
            "text": "When a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become  INITIALIZING , the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as  LEAVING .", 
            "title": "Node add"
        }, 
        {
            "location": "/m3db/architecture/sharding/#node-down", 
            "text": "A node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.", 
            "title": "Node down"
        }, 
        {
            "location": "/m3db/architecture/sharding/#node-remove", 
            "text": "When a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are  INITIALIZING  and need to be bootstrapped and will begin bootstrapping the data using all replicas available.", 
            "title": "Node remove"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/", 
            "text": "Consistency Levels\n\n\nM3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.\n\n\nWrite consistency levels\n\n\n\n\n\n\nOne:\n Corresponds to a single node succeeding for an operation to succeed.\n\n\n\n\n\n\nMajority:\n Corresponds to the majority of nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nAll:\n Corresponds to all nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nRead consistency levels\n\n\n\n\n\n\nOne\n: Corresponds to reading from a single node to designate success.\n\n\n\n\n\n\nUnstrictMajority\n: Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.\n\n\n\n\n\n\nMajority\n: Corresponds to reading from the majority of nodes to designate success.\n\n\n\n\n\n\nAll:\n Corresponds to reading from all of the nodes to designate success.\n\n\n\n\n\n\nConnect consistency levels\n\n\nConnect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.\n\n\n\n\n\n\nAny:\n Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.\n\n\n\n\n\n\nNone:\n Corresponds to connecting to no nodes for all shards and as such will always succeed.\n\n\n\n\n\n\nOne:\n Corresponds to connecting to a single node for all shards.\n\n\n\n\n\n\nMajority:\n Corresponds to connecting to the majority of nodes for all shards.\n\n\n\n\n\n\nAll:\n Corresponds to connecting to all of the nodes for all shards.", 
            "title": "Consistency Levels"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/#consistency-levels", 
            "text": "M3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.", 
            "title": "Consistency Levels"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/#write-consistency-levels", 
            "text": "One:  Corresponds to a single node succeeding for an operation to succeed.    Majority:  Corresponds to the majority of nodes succeeding for an operation to succeed.    All:  Corresponds to all nodes succeeding for an operation to succeed.", 
            "title": "Write consistency levels"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/#read-consistency-levels", 
            "text": "One : Corresponds to reading from a single node to designate success.    UnstrictMajority : Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.    Majority : Corresponds to reading from the majority of nodes to designate success.    All:  Corresponds to reading from all of the nodes to designate success.", 
            "title": "Read consistency levels"
        }, 
        {
            "location": "/m3db/architecture/consistencylevels/#connect-consistency-levels", 
            "text": "Connect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.    Any:  Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.    None:  Corresponds to connecting to no nodes for all shards and as such will always succeed.    One:  Corresponds to connecting to a single node for all shards.    Majority:  Corresponds to connecting to the majority of nodes for all shards.    All:  Corresponds to connecting to all of the nodes for all shards.", 
            "title": "Connect consistency levels"
        }, 
        {
            "location": "/m3db/architecture/storage/", 
            "text": "Storage\n\n\nOverview\n\n\nThe primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.\n\n\nThey are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)\n\n\nFileSets\n\n\nA fileset has the following files:\n\n\n\n\nInfo file:\n Stores the block time window start and size and other important metadata about the fileset volume.\n\n\nSummaries file:\n Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.\n\n\nIndex file:\n Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval.\n\n\nData file:\n Stores the series compressed data streams.\n\n\nBloom filter file:\n Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.\n\n\nDigests file:\n Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.\n\n\nCheckpoint file:\n Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.\n\n\n\n\n                                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File      \u2502\n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500\n\u2502- Idx                \u2502\n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                 \u2502\n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size               \u2502\n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum           \u2502\n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset  \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2502- Encoded Tags       |  |\n\u2502- Snapshot Time      \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502- Type (Flush/Snap)  \u2502                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n                                                                              \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502\n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502\n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502\n\u2502- Index digest       \u2502                           \u2514\u2500\n\u2502  - Marker (16 bytes)\u2502\n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502\n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checkpoint File   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Digests digest     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.\n\n\nFileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.", 
            "title": "Storage"
        }, 
        {
            "location": "/m3db/architecture/storage/#storage", 
            "text": "", 
            "title": "Storage"
        }, 
        {
            "location": "/m3db/architecture/storage/#overview", 
            "text": "The primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.  They are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/storage/#filesets", 
            "text": "A fileset has the following files:   Info file:  Stores the block time window start and size and other important metadata about the fileset volume.  Summaries file:  Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.  Index file:  Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval.  Data file:  Stores the series compressed data streams.  Bloom filter file:  Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.  Digests file:  Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.  Checkpoint file:  Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File      \u2502\n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500 \u2502- Idx                \u2502\n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                 \u2502\n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size               \u2502\n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum           \u2502\n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset  \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2502- Encoded Tags       |  |\n\u2502- Snapshot Time      \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502- Type (Flush/Snap)  \u2502                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n                                                                              \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502\n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502\n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502\n\u2502- Index digest       \u2502                           \u2514\u2500 \u2502  - Marker (16 bytes)\u2502\n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502\n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checkpoint File   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Digests digest     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.  FileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.", 
            "title": "FileSets"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/", 
            "text": "Commit Logs\n\n\nOverview\n\n\nM3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)\n\n\nIntegrity Levels\n\n\nThere are two integrity levels available for commit logs:\n\n\n\n\nSynchronous:\n write operations must wait until it has finished writing an entry in the commit log to complete.\n\n\nBehind:\n write operations must finish enqueueing an entry to the commit log write queue to complete.\n\n\n\n\nDepending on the data loss requirements users can choose either integrity level.\n\n\nProperties\n\n\nCommit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.\n\n\nStructure\n\n\nCommit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.\n\n\nThe structures can be conceptually described as:\n\n\nCommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}\n\n\n\n\nGarbage Collected\n\n\nCommit logs are garbage collected after all blocks within the retention period in which data inside the commit logs could be applicable have already been flushed to disk as immutable compressed filesets.\n\n\nCompaction\n\n\nThere is currently no compaction process for commitlogs. They are deleted once they fall out of their configurable retention period \nor\n all the \nfileset files\n for that period are flushed.", 
            "title": "Commit Logs"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#commit-logs", 
            "text": "", 
            "title": "Commit Logs"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#overview", 
            "text": "M3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#integrity-levels", 
            "text": "There are two integrity levels available for commit logs:   Synchronous:  write operations must wait until it has finished writing an entry in the commit log to complete.  Behind:  write operations must finish enqueueing an entry to the commit log write queue to complete.   Depending on the data loss requirements users can choose either integrity level.", 
            "title": "Integrity Levels"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#properties", 
            "text": "Commit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.", 
            "title": "Properties"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#structure", 
            "text": "Commit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.  The structures can be conceptually described as:  CommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}", 
            "title": "Structure"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#garbage-collected", 
            "text": "Commit logs are garbage collected after all blocks within the retention period in which data inside the commit logs could be applicable have already been flushed to disk as immutable compressed filesets.", 
            "title": "Garbage Collected"
        }, 
        {
            "location": "/m3db/architecture/commitlogs/#compaction", 
            "text": "There is currently no compaction process for commitlogs. They are deleted once they fall out of their configurable retention period  or  all the  fileset files  for that period are flushed.", 
            "title": "Compaction"
        }, 
        {
            "location": "/m3db/architecture/peer_streaming/", 
            "text": "Peer Streaming\n\n\nClient\n\n\nPeer streaming is managed by the M3DB client.  It fetches all blocks from peers for a specified time range for bootstrapping purposes.  It performs the following steps:\n\n\n\n\nFetch all metadata for blocks from all peers who own the specified shard\n\n\nCompares metadata from different peers and determines the best peer(s) from which to stream the actual data\n\n\nStreams the block data from peers\n\n\n\n\nSteps 1, 2 and 3 all happen concurrently.  As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata.  If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from.  If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.\n\n\nIn terms of error handling, the client will respect the consistency level specified for bootstrap.  This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata.  For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \"none\" is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers.  Fetching block data as well will indefinitely retry until the consistency level is achieved.\n\n\nThe client supports dynamically changing the bootstrap consistency level, which is helfpul in disaster scenarios where the consistency level cannot be achieved.  To break the indefinite streaming attempt an operator can change the consistency level to \"none\" and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.\n\n\nThe diagram below depicts the control flow and concurrency (goroutines and channels) in detail:\n\n\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                                               \u2502\n             \u2502         FetchBootstrapBlocksFromPeers         \u2502\n             \u2502                                               \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Main routine          \u2502\n\u2502                               \u2502\n\u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2) Spin up background routine \u2502                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      Create with metadataCh\n                \u2502                                \u2502\n                \u2502                                \u25bc\n                \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                               \u2502\n                \u2502                \u2502      Background routine       \u2502\n                \u2502                \u2502                               \u2502\n                \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                \u2502\n                \u2502                          For each peer\n                \u2502                                \u2502\n                \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u25bc                \u25bc                 \u25bc\n                \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502       StreamBlocksMetadataFromPeer        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502  Stream paginated blocks metadata from a  \u2502\n                \u2502          \u2502        peer while pageToken != nil        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502 For each blocks metadata --\n put metadata \u2502\n                \u2502          \u2502              into metadataCh              \u2502\n                \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           StreamBlocksFromPeers           \u2502\n\u2502                                           \u2502\n\u2502 1) Create a background goroutine (details \u2502\n\u2502               to the right)               \u2502\n\u2502                                           \u2502\n\u2502 2) Create a queue per-peer which each have\u2502\n\u2502   their own internal goroutine and will   \u2502\n\u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              specific peer.               \u2502          \u2502\n\u2502                                           \u2502          \u2502\n\u2502 3) Loop through the enqueCh and pick an   \u2502 Creates with metadataCh\n\u2502appropriate peer(s) for each series (based \u2502     and enqueueCh\n\u2502on whether all the peers have the same data\u2502          \u2502\n\u2502 or not) and then put that into the queue  \u2502          \u2502\n\u2502for that peer so the data will be streamed \u2502          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n                \u2502                                      \u25bc\n                \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n                \u2502    \u2502                streamMetadataFn variable)                \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n                \u2502    \u2502per series/block combination from different peers until we\u2502\n                \u2502    \u2502   have them from all peers for a series/block metadata   \u2502\n                \u2502    \u2502   combination and then \nsubmit\n them to the enqueueCh    \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502At the end, flush any remaining series/block combinations \u2502\n                \u2502    \u2502(that we received from less than N peers) into the enqueCh\u2502\n                \u2502    \u2502                         as well.                         \u2502\n                \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n          For each peer\n                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502            \u2502             \u2502\n   \u2502            \u2502             \u2502\n   \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Peer Streaming"
        }, 
        {
            "location": "/m3db/architecture/peer_streaming/#peer-streaming", 
            "text": "", 
            "title": "Peer Streaming"
        }, 
        {
            "location": "/m3db/architecture/peer_streaming/#client", 
            "text": "Peer streaming is managed by the M3DB client.  It fetches all blocks from peers for a specified time range for bootstrapping purposes.  It performs the following steps:   Fetch all metadata for blocks from all peers who own the specified shard  Compares metadata from different peers and determines the best peer(s) from which to stream the actual data  Streams the block data from peers   Steps 1, 2 and 3 all happen concurrently.  As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata.  If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from.  If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.  In terms of error handling, the client will respect the consistency level specified for bootstrap.  This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata.  For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \"none\" is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers.  Fetching block data as well will indefinitely retry until the consistency level is achieved.  The client supports dynamically changing the bootstrap consistency level, which is helfpul in disaster scenarios where the consistency level cannot be achieved.  To break the indefinite streaming attempt an operator can change the consistency level to \"none\" and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.  The diagram below depicts the control flow and concurrency (goroutines and channels) in detail:               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                                               \u2502\n             \u2502         FetchBootstrapBlocksFromPeers         \u2502\n             \u2502                                               \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Main routine          \u2502\n\u2502                               \u2502\n\u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2) Spin up background routine \u2502                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      Create with metadataCh\n                \u2502                                \u2502\n                \u2502                                \u25bc\n                \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                               \u2502\n                \u2502                \u2502      Background routine       \u2502\n                \u2502                \u2502                               \u2502\n                \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                \u2502\n                \u2502                          For each peer\n                \u2502                                \u2502\n                \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u25bc                \u25bc                 \u25bc\n                \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502       StreamBlocksMetadataFromPeer        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502  Stream paginated blocks metadata from a  \u2502\n                \u2502          \u2502        peer while pageToken != nil        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502 For each blocks metadata --  put metadata \u2502\n                \u2502          \u2502              into metadataCh              \u2502\n                \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           StreamBlocksFromPeers           \u2502\n\u2502                                           \u2502\n\u2502 1) Create a background goroutine (details \u2502\n\u2502               to the right)               \u2502\n\u2502                                           \u2502\n\u2502 2) Create a queue per-peer which each have\u2502\n\u2502   their own internal goroutine and will   \u2502\n\u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              specific peer.               \u2502          \u2502\n\u2502                                           \u2502          \u2502\n\u2502 3) Loop through the enqueCh and pick an   \u2502 Creates with metadataCh\n\u2502appropriate peer(s) for each series (based \u2502     and enqueueCh\n\u2502on whether all the peers have the same data\u2502          \u2502\n\u2502 or not) and then put that into the queue  \u2502          \u2502\n\u2502for that peer so the data will be streamed \u2502          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n                \u2502                                      \u25bc\n                \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n                \u2502    \u2502                streamMetadataFn variable)                \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n                \u2502    \u2502per series/block combination from different peers until we\u2502\n                \u2502    \u2502   have them from all peers for a series/block metadata   \u2502\n                \u2502    \u2502   combination and then  submit  them to the enqueueCh    \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502At the end, flush any remaining series/block combinations \u2502\n                \u2502    \u2502(that we received from less than N peers) into the enqueCh\u2502\n                \u2502    \u2502                         as well.                         \u2502\n                \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n          For each peer\n                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502            \u2502             \u2502\n   \u2502            \u2502             \u2502\n   \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Client"
        }, 
        {
            "location": "/m3db/architecture/caching/", 
            "text": "Caching policies\n\n\nOverview\n\n\nBlocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don't need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \"cache\" itself is not a separate datastructure in memory, cached blocks are simply stored in their respective \nin-memory objects\n with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.\n\n\nFor general purpose workloads, the \nlru\n caching policy is reccommended.\n\n\nNone Cache Policy\n\n\nThe \nnone\n cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.\n\n\nAll Cache Policy\n\n\nThe \nall\n cache policy is the opposite of the \nnone\n cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.\n\n\nRecently Read Cache Policy\n\n\nThe \nrecently_read\n cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the \nrecently_read\n cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.\n\n\nData eviction from memory is triggered by the \"ticking\" process described in the \nbackground processes section\n\n\nLeast Recently Used (LRU) Cache Policy\n\n\nThe \nlru\n cache policy uses an \nlru\n list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in \nwired_list.go\n for implementation details.", 
            "title": "Caching"
        }, 
        {
            "location": "/m3db/architecture/caching/#caching-policies", 
            "text": "", 
            "title": "Caching policies"
        }, 
        {
            "location": "/m3db/architecture/caching/#overview", 
            "text": "Blocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don't need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \"cache\" itself is not a separate datastructure in memory, cached blocks are simply stored in their respective  in-memory objects  with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.  For general purpose workloads, the  lru  caching policy is reccommended.", 
            "title": "Overview"
        }, 
        {
            "location": "/m3db/architecture/caching/#none-cache-policy", 
            "text": "The  none  cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.", 
            "title": "None Cache Policy"
        }, 
        {
            "location": "/m3db/architecture/caching/#all-cache-policy", 
            "text": "The  all  cache policy is the opposite of the  none  cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.", 
            "title": "All Cache Policy"
        }, 
        {
            "location": "/m3db/architecture/caching/#recently-read-cache-policy", 
            "text": "The  recently_read  cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the  recently_read  cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.  Data eviction from memory is triggered by the \"ticking\" process described in the  background processes section", 
            "title": "Recently Read Cache Policy"
        }, 
        {
            "location": "/m3db/architecture/caching/#least-recently-used-lru-cache-policy", 
            "text": "The  lru  cache policy uses an  lru  list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in  wired_list.go  for implementation details.", 
            "title": "Least Recently Used (LRU) Cache Policy"
        }, 
        {
            "location": "/how_to/single_node/", 
            "text": "Deploying a single node\n\n\nDeploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it\nhas to offer. Our Docker image by default configures a single M3DB instance as one binary\ncontaining:\n\n\n\n\nAn M3DB storage instance (\nm3dbnode\n) for timeseries storage. This includes an embedded tag-based\n  metrics index, as well as as an embedded etcd server for storing the above mentioned cluster\n  topology and runtime configuration.\n\n\nA \"coordinator\" instance (\nm3coordinator\n) for writing and querying tagged metrics, as well as\n  managing cluster topology and runtime configuration.\n\n\n\n\nTo begin, first start up a Docker container with port \n7201\n (used to manage the cluster topology)\nand port \n9003\n (used to read and write metrics) exposed. We recommend you create a persistent data\ndirectory on your host for durability:\n\n\ndocker run -p 7201:7201 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3db/m3db:latest\n\n\n\n\n\n\n\n\n\n\nNext, create an initial namespace for your metrics:\n\n\n\n\n\ncurl -X POST localhost:7201/namespace/add -d '{\n  \nname\n: \ndefault\n,\n  \noptions\n: {\n    \nbootstrapEnabled\n: true,\n    \nflushEnabled\n: true,\n    \nwritesToCommitLog\n: true,\n    \ncleanupEnabled\n: true,\n    \nsnapshotEnabled\n: false,\n    \nrepairEnabled\n: false,\n    \nretentionOptions\n: {\n      \nretentionPeriodNanos\n: 172800000000000,\n      \nblockSizeNanos\n: 7200000000000,\n      \nbufferFutureNanos\n: 600000000000,\n      \nbufferPastNanos\n: 600000000000,\n      \nblockDataExpiry\n: true,\n      \nblockDataExpiryAfterNotAccessPeriodNanos\n: 300000000000\n    },\n    \nindexOptions\n: {\n      \nenabled\n: true,\n      \nblockSizeNanos\n: 7200000000000\n    }\n  }\n}'\n\n\n\n\nWith a namespace to hold your metrics created, you can initialize your first placement:\n\n\ncurl -X POST localhost:7201/placement/init -d '{\n    \nnum_shards\n: 64,\n    \nreplication_factor\n: 1,\n    \ninstances\n: [\n        {\n            \nid\n: \nm3db_local\n,\n            \nisolation_group\n: \nrack-a\n,\n            \nzone\n: \nembedded\n,\n            \nweight\n: 1024,\n            \nendpoint\n: \n127.0.0.1:9000\n,\n            \nhostname\n: \n127.0.0.1\n,\n            \nport\n: 9000\n        }\n    ]\n}'\n\n\n\n\nShortly after, you should see your node complete bootstrapping! Don't worry if you see warnings or\nerrors related to a local cache file, such as \n[W] could not load cache from file\n/var/lib/m3kv/m3db_embedded.json\n. Those are expected for a local instance and in general any\nwarn-level errors (prefixed with \n[W]\n) should not block bootstrapping.\n\n\n20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace default} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 1 hosts\n\n\n\n\nNow you can experiment with writing tagged metrics:\n\n\ncurl -sSf -X POST http://localhost:9003/writetagged -d '{\n  \nnamespace\n: \ndefault\n,\n  \nid\n: \nfoo\n,\n  \ntags\n: [\n    {\n      \nname\n: \ncity\n,\n      \nvalue\n: \nnew_york\n\n    },\n    {\n      \nname\n: \nendpoint\n,\n      \nvalue\n: \n/request\n\n    }\n  ],\n  \ndatapoint\n: {\n    \ntimestamp\n: '\n$(date \n+%s\n)\n',\n    \nvalue\n: 42.123456789\n  }\n}\n'\n\n\n\n\nAnd reading the metrics you've written:\n\n\ncurl -sSf -X POST http://localhost:9003/query -d '{\n  \nnamespace\n: \ndefault\n,\n  \nquery\n: {\n    \nregexp\n: {\n      \nfield\n: \ncity\n,\n      \nregexp\n: \n.*\n\n    }\n  },\n  \nrangeStart\n: 0,\n  \nrangeEnd\n: '\n$(date \n+%s\n)\n'\n}' | jq .\n\n{\n  \nresults\n: [\n    {\n      \nid\n: \nfoo\n,\n      \ntags\n: [\n        {\n          \nname\n: \ncity\n,\n          \nvalue\n: \nnew_york\n\n        },\n        {\n          \nname\n: \nendpoint\n,\n          \nvalue\n: \n/request\n\n        }\n      ],\n      \ndatapoints\n: [\n        {\n          \ntimestamp\n: 1527039389,\n          \nvalue\n: 42.123456789\n        }\n      ]\n    }\n  ],\n  \nexhaustive\n: true\n}", 
            "title": "Single Node"
        }, 
        {
            "location": "/how_to/single_node/#deploying-a-single-node", 
            "text": "Deploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it\nhas to offer. Our Docker image by default configures a single M3DB instance as one binary\ncontaining:   An M3DB storage instance ( m3dbnode ) for timeseries storage. This includes an embedded tag-based\n  metrics index, as well as as an embedded etcd server for storing the above mentioned cluster\n  topology and runtime configuration.  A \"coordinator\" instance ( m3coordinator ) for writing and querying tagged metrics, as well as\n  managing cluster topology and runtime configuration.   To begin, first start up a Docker container with port  7201  (used to manage the cluster topology)\nand port  9003  (used to read and write metrics) exposed. We recommend you create a persistent data\ndirectory on your host for durability:  docker run -p 7201:7201 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3db/m3db:latest    Next, create an initial namespace for your metrics:   curl -X POST localhost:7201/namespace/add -d '{\n   name :  default ,\n   options : {\n     bootstrapEnabled : true,\n     flushEnabled : true,\n     writesToCommitLog : true,\n     cleanupEnabled : true,\n     snapshotEnabled : false,\n     repairEnabled : false,\n     retentionOptions : {\n       retentionPeriodNanos : 172800000000000,\n       blockSizeNanos : 7200000000000,\n       bufferFutureNanos : 600000000000,\n       bufferPastNanos : 600000000000,\n       blockDataExpiry : true,\n       blockDataExpiryAfterNotAccessPeriodNanos : 300000000000\n    },\n     indexOptions : {\n       enabled : true,\n       blockSizeNanos : 7200000000000\n    }\n  }\n}'  With a namespace to hold your metrics created, you can initialize your first placement:  curl -X POST localhost:7201/placement/init -d '{\n     num_shards : 64,\n     replication_factor : 1,\n     instances : [\n        {\n             id :  m3db_local ,\n             isolation_group :  rack-a ,\n             zone :  embedded ,\n             weight : 1024,\n             endpoint :  127.0.0.1:9000 ,\n             hostname :  127.0.0.1 ,\n             port : 9000\n        }\n    ]\n}'  Shortly after, you should see your node complete bootstrapping! Don't worry if you see warnings or\nerrors related to a local cache file, such as  [W] could not load cache from file\n/var/lib/m3kv/m3db_embedded.json . Those are expected for a local instance and in general any\nwarn-level errors (prefixed with  [W] ) should not block bootstrapping.  20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace default} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 1 hosts  Now you can experiment with writing tagged metrics:  curl -sSf -X POST http://localhost:9003/writetagged -d '{\n   namespace :  default ,\n   id :  foo ,\n   tags : [\n    {\n       name :  city ,\n       value :  new_york \n    },\n    {\n       name :  endpoint ,\n       value :  /request \n    }\n  ],\n   datapoint : {\n     timestamp : ' $(date  +%s ) ',\n     value : 42.123456789\n  }\n}\n'  And reading the metrics you've written:  curl -sSf -X POST http://localhost:9003/query -d '{\n   namespace :  default ,\n   query : {\n     regexp : {\n       field :  city ,\n       regexp :  .* \n    }\n  },\n   rangeStart : 0,\n   rangeEnd : ' $(date  +%s ) '\n}' | jq .\n\n{\n   results : [\n    {\n       id :  foo ,\n       tags : [\n        {\n           name :  city ,\n           value :  new_york \n        },\n        {\n           name :  endpoint ,\n           value :  /request \n        }\n      ],\n       datapoints : [\n        {\n           timestamp : 1527039389,\n           value : 42.123456789\n        }\n      ]\n    }\n  ],\n   exhaustive : true\n}", 
            "title": "Deploying a single node"
        }, 
        {
            "location": "/integrations/prometheus/", 
            "text": "Integrations", 
            "title": "Prometheus"
        }, 
        {
            "location": "/integrations/prometheus/#integrations", 
            "text": "", 
            "title": "Integrations"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/faqs/", 
            "text": "FAQs", 
            "title": "FAQs"
        }, 
        {
            "location": "/faqs/#faqs", 
            "text": "", 
            "title": "FAQs"
        }
    ]
}