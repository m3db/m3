{
    "docs": [
        {
            "location": "/",
            "text": "M3\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nAbout\n\n\nM3 is a metrics platform that is built on M3DB, a distributed timeseries database. The project monorepo can be found \nhere\n.",
            "title": "Introduction"
        },
        {
            "location": "/#m3",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "M3"
        },
        {
            "location": "/#about",
            "text": "M3 is a metrics platform that is built on M3DB, a distributed timeseries database. The project monorepo can be found  here .",
            "title": "About"
        },
        {
            "location": "/introduction/components/components/",
            "text": "Components\n\n\nM3 Coordinator\n\n\nM3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and M3DB. It is a bridge that users can deploy to access the benefits of M3DB such as long term storage and multi-DC setup with other monitoring systems, such as Prometheus. See \nthis presentation\n for more on long term storage in Prometheus.\n\n\nM3DB\n\n\nM3DB is a distributed time series database that provides scalable storage and a reverse index of time series. It is optimized as a cost effective and reliable realtime and long term retention metrics store and index.  For more details, see the \nM3DB documentation\n.\n\n\nM3 Query\n\n\nM3 Query is a service that houses a distributed query engine for querying both realtime and historical metrics, supporting several different query languages. It is designed to support both low latency realtime queries and queries that can take longer to execute, aggregating over much larger datasets, for analytical use cases.  For more details, see the \nquery engine documentation\n.\n\n\nM3 Aggregator\n\n\nM3 Aggregator is a service that runs as a dedicated metrics aggregator and provides stream based downsampling, based on dynamic rules stored in etcd. It uses leader election and aggregation window tracking, leveraging etcd to manage this state, to reliably emit at-least-once aggregations for downsampled metrics to long term storage. This provides cost effective and reliable downsampling & roll up of metrics. These features also reside in the M3 Coordinator, however the dedicated aggregator is sharded and replicated, whereas the M3 Coordinator is not and requires care to deploy and run in a highly available way. There is work remaining to make the aggregator more accessible to users without requiring them to write their own compatible producer and consumer.",
            "title": "Components"
        },
        {
            "location": "/introduction/components/components/#components",
            "text": "",
            "title": "Components"
        },
        {
            "location": "/introduction/components/components/#m3-coordinator",
            "text": "M3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and M3DB. It is a bridge that users can deploy to access the benefits of M3DB such as long term storage and multi-DC setup with other monitoring systems, such as Prometheus. See  this presentation  for more on long term storage in Prometheus.",
            "title": "M3 Coordinator"
        },
        {
            "location": "/introduction/components/components/#m3db",
            "text": "M3DB is a distributed time series database that provides scalable storage and a reverse index of time series. It is optimized as a cost effective and reliable realtime and long term retention metrics store and index.  For more details, see the  M3DB documentation .",
            "title": "M3DB"
        },
        {
            "location": "/introduction/components/components/#m3-query",
            "text": "M3 Query is a service that houses a distributed query engine for querying both realtime and historical metrics, supporting several different query languages. It is designed to support both low latency realtime queries and queries that can take longer to execute, aggregating over much larger datasets, for analytical use cases.  For more details, see the  query engine documentation .",
            "title": "M3 Query"
        },
        {
            "location": "/introduction/components/components/#m3-aggregator",
            "text": "M3 Aggregator is a service that runs as a dedicated metrics aggregator and provides stream based downsampling, based on dynamic rules stored in etcd. It uses leader election and aggregation window tracking, leveraging etcd to manage this state, to reliably emit at-least-once aggregations for downsampled metrics to long term storage. This provides cost effective and reliable downsampling & roll up of metrics. These features also reside in the M3 Coordinator, however the dedicated aggregator is sharded and replicated, whereas the M3 Coordinator is not and requires care to deploy and run in a highly available way. There is work remaining to make the aggregator more accessible to users without requiring them to write their own compatible producer and consumer.",
            "title": "M3 Aggregator"
        },
        {
            "location": "/introduction/motivation/motivation/",
            "text": "Motivation\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nWe decided to open source the M3 platform as a scalable remote storage backend for Prometheus so that others may attempt to reuse our work and avoid building yet another scalable metrics platform. As documentation for Prometheus states, it is limited by single nodes in its scalability and durability. The M3 platform aims to provide a turnkey, scalable, and configurable multi-tenant store for Prometheus and other standard metrics schemas.",
            "title": "Motivation"
        },
        {
            "location": "/introduction/motivation/motivation/#motivation",
            "text": "Please note:  This documentation is a work in progress and more detail is required.  We decided to open source the M3 platform as a scalable remote storage backend for Prometheus so that others may attempt to reuse our work and avoid building yet another scalable metrics platform. As documentation for Prometheus states, it is limited by single nodes in its scalability and durability. The M3 platform aims to provide a turnkey, scalable, and configurable multi-tenant store for Prometheus and other standard metrics schemas.",
            "title": "Motivation"
        },
        {
            "location": "/introduction/media/media/",
            "text": "Media\n\n\nBlogs\n\n\n\n\nM3: Uber\u2019s Open Source, Large-scale Metrics Platform for Prometheus\n By Rob Skillington - Aug 7, 2018.\n\n\n\n\nRecorded Talks\n\n\n\n\n\n\nPanel Discussion: Prometheus Long-Term Storage Approaches\n including highlights of the M3 stack by Nikunj Aggarwal - Aug 9, 2018.\n\n\n\n\n\n\nPutting billions of time series to work at Uber with autonomous monitoring\n By Prateek Rungta - Jun 6, 2018. \nSlides\n\n\n\n\n\n\nAdventures in building a high-volume Time-Series Database\n By Richard Artoul & Prateek Rungta - Nov 4, 2018.\n\n\n\n\n\n\nUpcoming Talks\n\n\n\n\n\n\nKeynote: Smooth Operator\u266a: Large Scale Automated Storage with Kubernetes\n By Celina Ward & Matt Schallert - Dec 13, 2018.\n\n\n\n\n\n\nLearnings, patterns and Uber\u2019s metrics platform M3, open sourced as a Prometheus long term storage backend\n By Rob Skillington - Nov 5, 2018.",
            "title": "Media"
        },
        {
            "location": "/introduction/media/media/#media",
            "text": "",
            "title": "Media"
        },
        {
            "location": "/introduction/media/media/#blogs",
            "text": "M3: Uber\u2019s Open Source, Large-scale Metrics Platform for Prometheus  By Rob Skillington - Aug 7, 2018.",
            "title": "Blogs"
        },
        {
            "location": "/introduction/media/media/#recorded-talks",
            "text": "Panel Discussion: Prometheus Long-Term Storage Approaches  including highlights of the M3 stack by Nikunj Aggarwal - Aug 9, 2018.    Putting billions of time series to work at Uber with autonomous monitoring  By Prateek Rungta - Jun 6, 2018.  Slides    Adventures in building a high-volume Time-Series Database  By Richard Artoul & Prateek Rungta - Nov 4, 2018.",
            "title": "Recorded Talks"
        },
        {
            "location": "/introduction/media/media/#upcoming-talks",
            "text": "Keynote: Smooth Operator\u266a: Large Scale Automated Storage with Kubernetes  By Celina Ward & Matt Schallert - Dec 13, 2018.    Learnings, patterns and Uber\u2019s metrics platform M3, open sourced as a Prometheus long term storage backend  By Rob Skillington - Nov 5, 2018.",
            "title": "Upcoming Talks"
        },
        {
            "location": "/m3db/",
            "text": "M3DB, a distributed time series database\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nAbout\n\n\nM3DB, inspired by \nGorilla\n and \nCassandra\n, is a distributed time series database released as open source by \nUber Technologies\n. It can be used for storing realtime metrics at long retention.\n\n\nHere are some attributes of the project:\n\n\n\n\nDistributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently\n\n\nCluster management built on top of \netcd\n\n\nBuilt-in synchronous replication with configurable durability and read consistency (one, majority, all, etc)\n\n\nM3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy\n\n\nArbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write\n\n\nConfigurable out of order writes, currently limited to the size of the configured time window's block size\n\n\n\n\nCurrent Limitations\n\n\nDue to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.\n\n\nThe project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours).  As such out of order writes are limited to the size of a single compressed time series window.  Consequently backfilling large amounts of data is not currently possible.\n\n\nThe project has also has optimized for the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.",
            "title": "Introduction"
        },
        {
            "location": "/m3db/#m3db-a-distributed-time-series-database",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "M3DB, a distributed time series database"
        },
        {
            "location": "/m3db/#about",
            "text": "M3DB, inspired by  Gorilla  and  Cassandra , is a distributed time series database released as open source by  Uber Technologies . It can be used for storing realtime metrics at long retention.  Here are some attributes of the project:   Distributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently  Cluster management built on top of  etcd  Built-in synchronous replication with configurable durability and read consistency (one, majority, all, etc)  M3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy  Arbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write  Configurable out of order writes, currently limited to the size of the configured time window's block size",
            "title": "About"
        },
        {
            "location": "/m3db/#current-limitations",
            "text": "Due to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.  The project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours).  As such out of order writes are limited to the size of a single compressed time series window.  Consequently backfilling large amounts of data is not currently possible.  The project has also has optimized for the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.",
            "title": "Current Limitations"
        },
        {
            "location": "/m3db/architecture/",
            "text": "Architecture\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nM3DB is written entirely in Go and does not have any required dependencies. For larger deployments, one may use an etcd cluster to manage M3DB cluster membership and topology definition.\n\n\nHigh Level Goals\n\n\nSome of the high level goals for the project are defined as:\n\n\n\n\n\n\nMonitoring support:\n M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.\n\n\n\n\n\n\nHighly configurable:\n Provide a high level of configuration to support a wide set of use cases and runtime environments.\n\n\n\n\n\n\nVariable durability:\n Providing variable durability guarantees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarantees that data was replicated to a quorum of nodes and that the data was durable if desired.",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/#architecture",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "Architecture"
        },
        {
            "location": "/m3db/architecture/#overview",
            "text": "M3DB is written entirely in Go and does not have any required dependencies. For larger deployments, one may use an etcd cluster to manage M3DB cluster membership and topology definition.",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/#high-level-goals",
            "text": "Some of the high level goals for the project are defined as:    Monitoring support:  M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.    Highly configurable:  Provide a high level of configuration to support a wide set of use cases and runtime environments.    Variable durability:  Providing variable durability guarantees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarantees that data was replicated to a quorum of nodes and that the data was durable if desired.",
            "title": "High Level Goals"
        },
        {
            "location": "/m3db/architecture/engine/",
            "text": "Storage Engine Overview\n\n\nM3DB is a time series database that was primarily designed to be horizontally scalable and handle a large volume of monitoring time series data.\n\n\nTime Series Compression (M3TSZ)\n\n\nOne of M3DB's biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. This high compression ratio is implemented via the M3TSZ algorithm, a variant of the streaming time series compression algorithm described in \nFacebook's Gorilla paper\n with a few small differences.\n\n\nThe compression ratio will vary depending on the workload and configuration, but we found that with M3TSZ we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber's production workloads. This was a 40% improvement over standard TSZ which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.\n\n\nArchitecture\n\n\nM3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.\n\n\nIn-Memory Object Layout\n\n\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           Database            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n   \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          Namespace 1          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n   \u2502     \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502      \u2502\n   \u2502     \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  Shard 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Series 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [2PM - 4PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [4PM - 6PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   Blocks   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502     Block [6PM - 8PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502 Active Buffers (encoders)  \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n   \u2502                                                                 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nThe in-memory portion of M3DB is implemented via a hierarchy of objects:\n\n\n\n\n\n\nA \ndatabase\n of which there is only one per M3DB process.\n\n\n\n\n\n\nA \ndatabase\n \"owns\" numerous namespaces, and each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize (which we will discuss in more detail later). \nNamespaces\n are similar to tables in other databases.\n\n\n\n\n\n\nShards\n which are owned by \nnamespaces\n. \nShards\n are effectively the same as \"virtual shards\" in Cassandra in that they provide arbitrary distribution of time series data via a simple hash of the series ID.\n\n\n\n\n\n\nSeries\n which are owned by \nshards\n. A \nseries\n is generally what comes to mind when you think of \"time series\" data. Ex. The CPU level for a single host in a datacenter over a period of time could be represented as a series with id \"\n.system.cpu.utilization\" and a vector of tuples in the form of (TIMESTAMP, CPU_LEVEL). In other words, if you were rendering a graph a series would represent a single line on that graph. Note that the previous example is only a logical illustration and does not represent the way that M3DB actually stores data.\n\n\n\n\n\n\nBlocks\n belong to a series and are central to M3DB's design. A \nblock\n is simply a smaller wrapper object around a sealed (no longer writable) stream of compressed time series data. The compression comes with a few caveats though, namely that you cannot read individual datapoints in a compressed block. In other words, in order to read a single datapoint you must decompress the entire block up to the datapoint that you're trying to read.\n\n\n\n\n\n\nIf M3DB kept everything in memory (and in fact, early versions of it did), than you could conceptually think of it as being a composed from a hierarchy of maps:\n\n\ndatabase_obect      => map\n\nnamespace_object    => map\n\nshard_object        => map\n\nseries_object       => map\n\nseries_object       => map\n (This map should only have one or two entries)\n\n\nPersistent storage\n\n\nWhile in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy then it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.\n\n\nIn addition, with large volumes of data it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \"write-once, read-never\" pattern where less than a few percent of all the data that's stored is ever read. With that type of workload, its wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.\n\n\nLike most other databases, M3DB takes a two-pronged approach to persistant storage that involves combining a commitlog (for disaster recovery) with periodic snapshotting (for efficient retrieval):\n\n\n\n\nAll writes are persisted to a \ncommitlog\n (the commitlog can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commitlog is completely uncompressed and exists only to recover \"unflushed\" data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request.\n\n\nPeriodically (based on the configured blocksize) all \"active\" blocks are \"sealed\" (marked as immutable) and flushed to disk as \n\"fileset\" files\n. These files are highly compressed and can be indexed into via their complementary index files. Check out the \nflushing section\n to learn more about the background flushing process.\n\n\n\n\nThe blocksize parameter is the most important variable that needs to be tuned for your particular workload. A small blocksize will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and your data will take up more space on disk.\n\n\nIf the database is stopped for any reason in-between \"flushes\" (writing fileset files out to disk), then when the node is started back up those writes will need to be recovered by reading the commitlog or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than 1).\n\n\nWhile the \nfileset files\n are designed to support efficient data retrieval via the series primary key (the ID), there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB support various \ncaching policies\n which can significantly improve the performance of reads by caching data in memory.\n\n\nWrite Path\n\n\nWe now have enough context of M3DB's architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the \nwriteBatchRaw\n endpoint on M3DB's embedded thrift server. The write itself will contain the following information:\n\n\n\n\nThe namespace\n\n\nThe series ID (byte blob)\n\n\nThe timestamp\n\n\nThe value itself\n\n\n\n\nM3DB will consult the database object to check if the namespace exists, and if it does,then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then it will lookup the series' corresponding encoder and encode the datapoint into the compressed stream. If the encoder doesn't exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some special logic for handling out-of-order writes which is discussed in the \nmerging all encoders section\n.\n\n\nAt the same time, the write will be appended to the commitlog queue (and depending on the commitlog configuration immediately fsync'd to disk or batched together with other writes and flushed out all at once).\n\n\nThe write will exist only in this \"active buffer\" and the commitlog until the block ends and is flushed to disk, at which point the write will exist in a fileset file for efficient storage and retrieval later and the commitlog entry can be garbage collected.\n\n\nNote:\n Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured \nconsistency level\n.\n\n\nRead Path\n\n\nA read begins when an M3DB client calls the \nFetchBatchResult\n or \nFetchBlocksRawResult\n endpoint on M3DB's embedded thrift server. The read request will contain the following information:\n\n\n\n\nThe namespace\n\n\nThe series ID (byte blob)\n\n\nThe period of time being requested (start and end)\n\n\n\n\nM3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:\n\n\n\n\nDoes the series exist? and if it does\n\n\nDoes the data exist in an \"active buffer\" (actively being compressed by an encoder), cached in-memory, on disk, or some combination of all three?\n\n\n\n\nDetermining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn't, then M3DB consults an in-memory bloom filter(s) for that shard / block start combination(s) to determine if the series exists on disk.\n\n\nIf the series exists, then for every block that the request spans, M3DB needs to consolidate data from the active buffers, in-memory cache, and fileset files (disk).\n\n\nLets imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a blocksize of 2 hours (i.e we need to find 3 different blocks.)\n\n\nIf the current time is 8PM, then the location of the requested blocks might be as follows:\n\n\n[2PM - 4PM (FileSet file)]    - Sealed and flushed block that isn't cached\n[4PM - 6PM (In-memory cache)] - Sealed and flush block that is cached\n[6PM - 8PM (active buffer)]   - Hasn't been sealed or flushed yet\n\n\n\n\nThen M3DB will need to consolidate:\n\n\n\n\nThe not-yet-sealed block from the active buffers / encoders (located inside an internal lookup in the Series object) \n[6PM - 8PM]\n\n\nThe in-memory cached block (also located inside an internal lookup in the Series object) \n[4PM - 6PM]\n\n\nThe block from disk (the block retrieve from disk will then be cached according to the current \ncaching policy\n \n[2PM - 4PM]\n\n\n\n\nRetrieving blocks from the active buffers and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:\n\n\n\n\nConsult the in-memory bloom filter to determine if its possible the series exists on disk.\n\n\nIf the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is \nbefore\n the series ID that we're searching for. Review the \nindex_lookup.go\n file for implementation details.\n\n\nJump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we're looking for \nor\n we get far enough in the index file that it becomes clear that the ID we're looking for doesn't exist (this is possible because the index file is sorted by ID)\n\n\nJump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.\n\n\n\n\nOnce M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or not the client returns a success to the caller for the read is dependent on the configured \nconsistency level\n.\n\n\nNote:\n Since M3DB nodes return compressed blocks (the M3DB client decompresses them), it's not possible to return \"partial results\" for a given block. If any portion of a read request spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.\n\n\nBackground processes\n\n\nM3DB has a variety of processes that run in the background during normal operation.\n\n\nTicking\n\n\nThe ticking process runs continously in the background and is responsible for a variety of tasks:\n\n\n\n\nMerging all encoders for a given series / block start combination\n\n\nRemoving expired / flushed series and blocks from memory\n\n\nCleanup of expired data (fileset/commit log) from the filesystem\n\n\n\n\nMerging all encoders\n\n\nM3TSZ is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For monitoring workloads this works very well because every subsequent datapoint is almost always chronologically after the previous one. However, real world systems are messy and occasionally out of order writes will be received. When this happens, M3DB will allocate a new encoder for the out of order datapoints. The multiple encoders need to be merged before flushing the data to disk, but to prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.\n\n\nRemoving expired / flushed series and blocks from memory\n\n\nDepending on the configured \ncaching policy\n, the \nin-memory object layout\n can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer need to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.\n\n\nFlushing\n\n\nAs discussed in the \narchitecture\n section, writes are actively buffered / compressed in-memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of \nfileset files\n to facilitate efficient storage and retrieval.\n\n\nThis is where the configurable \"blocksize\" comes into play. The blocksize is simply a duration of time that dictates how long active writes will be compressed (in a streaming manner) in memory before being \"sealed\" (marked as immutable) and flushed to disk. Lets use a blocksize of two hours as an example.\n\n\nIf the blocksize is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the \nfileset files\n will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.\n\n\nCaveats / Limitations\n\n\n\n\nM3DB currently supports exact ID based lookups. It does not support tag/secondary indexing. This feature is under development and future versions of M3DB will have support for a built-in reverse index.\n\n\nM3DB does not support updates / deletes. All data written to M3DB is immutable.\n\n\nM3DB does not support writing arbitrarily into the past and future. This is generally fine for monitoring workloads, but can be problematic for traditional \nOLTP\n and \nOLAP\n workloads. Future versions of M3DB will have better support for writes with arbitrary timestamps.\n\n\nM3DB does not support writing datapoints with values other than double-precision floats. Future versions of M3DB will have support for storing arbitrary values.\n\n\nM3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value (Uber has production databases running with retention periods as high as 5 years), its still required and generally speaking M3DB is optimized for workloads with a well-defined \nTTL\n.\n\n\nM3DB does not support either background data repair or Cassandra-style \nread repairs\n. Future versions of M3DB will support automatic repairs of data as an ongoing background process.",
            "title": "Storage Engine"
        },
        {
            "location": "/m3db/architecture/engine/#storage-engine-overview",
            "text": "M3DB is a time series database that was primarily designed to be horizontally scalable and handle a large volume of monitoring time series data.",
            "title": "Storage Engine Overview"
        },
        {
            "location": "/m3db/architecture/engine/#time-series-compression-m3tsz",
            "text": "One of M3DB's biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. This high compression ratio is implemented via the M3TSZ algorithm, a variant of the streaming time series compression algorithm described in  Facebook's Gorilla paper  with a few small differences.  The compression ratio will vary depending on the workload and configuration, but we found that with M3TSZ we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber's production workloads. This was a 40% improvement over standard TSZ which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.",
            "title": "Time Series Compression (M3TSZ)"
        },
        {
            "location": "/m3db/architecture/engine/#architecture",
            "text": "M3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.",
            "title": "Architecture"
        },
        {
            "location": "/m3db/architecture/engine/#in-memory-object-layout",
            "text": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           Database            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502                                                                 \u2502\n   \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n   \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          Namespace 1          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n   \u2502     \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502      \u2502\n   \u2502     \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  Shard 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Series 1  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [2PM - 4PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502      Block [4PM - 6PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   Blocks   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502     Block [6PM - 8PM]      \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502                            \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2502 Active Buffers (encoders)  \u2502 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2502                                 \u2502   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2502                                         \u2502     \u2502      \u2502\n   \u2502     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2502                                                    \u2502      \u2502\n   \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n   \u2502                                                                 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  The in-memory portion of M3DB is implemented via a hierarchy of objects:    A  database  of which there is only one per M3DB process.    A  database  \"owns\" numerous namespaces, and each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize (which we will discuss in more detail later).  Namespaces  are similar to tables in other databases.    Shards  which are owned by  namespaces .  Shards  are effectively the same as \"virtual shards\" in Cassandra in that they provide arbitrary distribution of time series data via a simple hash of the series ID.    Series  which are owned by  shards . A  series  is generally what comes to mind when you think of \"time series\" data. Ex. The CPU level for a single host in a datacenter over a period of time could be represented as a series with id \" .system.cpu.utilization\" and a vector of tuples in the form of (TIMESTAMP, CPU_LEVEL). In other words, if you were rendering a graph a series would represent a single line on that graph. Note that the previous example is only a logical illustration and does not represent the way that M3DB actually stores data.    Blocks  belong to a series and are central to M3DB's design. A  block  is simply a smaller wrapper object around a sealed (no longer writable) stream of compressed time series data. The compression comes with a few caveats though, namely that you cannot read individual datapoints in a compressed block. In other words, in order to read a single datapoint you must decompress the entire block up to the datapoint that you're trying to read.    If M3DB kept everything in memory (and in fact, early versions of it did), than you could conceptually think of it as being a composed from a hierarchy of maps:  database_obect      => map \nnamespace_object    => map \nshard_object        => map \nseries_object       => map \nseries_object       => map  (This map should only have one or two entries)",
            "title": "In-Memory Object Layout"
        },
        {
            "location": "/m3db/architecture/engine/#persistent-storage",
            "text": "While in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy then it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.  In addition, with large volumes of data it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \"write-once, read-never\" pattern where less than a few percent of all the data that's stored is ever read. With that type of workload, its wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.  Like most other databases, M3DB takes a two-pronged approach to persistant storage that involves combining a commitlog (for disaster recovery) with periodic snapshotting (for efficient retrieval):   All writes are persisted to a  commitlog  (the commitlog can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commitlog is completely uncompressed and exists only to recover \"unflushed\" data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request.  Periodically (based on the configured blocksize) all \"active\" blocks are \"sealed\" (marked as immutable) and flushed to disk as  \"fileset\" files . These files are highly compressed and can be indexed into via their complementary index files. Check out the  flushing section  to learn more about the background flushing process.   The blocksize parameter is the most important variable that needs to be tuned for your particular workload. A small blocksize will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and your data will take up more space on disk.  If the database is stopped for any reason in-between \"flushes\" (writing fileset files out to disk), then when the node is started back up those writes will need to be recovered by reading the commitlog or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than 1).  While the  fileset files  are designed to support efficient data retrieval via the series primary key (the ID), there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB support various  caching policies  which can significantly improve the performance of reads by caching data in memory.",
            "title": "Persistent storage"
        },
        {
            "location": "/m3db/architecture/engine/#write-path",
            "text": "We now have enough context of M3DB's architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the  writeBatchRaw  endpoint on M3DB's embedded thrift server. The write itself will contain the following information:   The namespace  The series ID (byte blob)  The timestamp  The value itself   M3DB will consult the database object to check if the namespace exists, and if it does,then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then it will lookup the series' corresponding encoder and encode the datapoint into the compressed stream. If the encoder doesn't exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some special logic for handling out-of-order writes which is discussed in the  merging all encoders section .  At the same time, the write will be appended to the commitlog queue (and depending on the commitlog configuration immediately fsync'd to disk or batched together with other writes and flushed out all at once).  The write will exist only in this \"active buffer\" and the commitlog until the block ends and is flushed to disk, at which point the write will exist in a fileset file for efficient storage and retrieval later and the commitlog entry can be garbage collected.  Note:  Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured  consistency level .",
            "title": "Write Path"
        },
        {
            "location": "/m3db/architecture/engine/#read-path",
            "text": "A read begins when an M3DB client calls the  FetchBatchResult  or  FetchBlocksRawResult  endpoint on M3DB's embedded thrift server. The read request will contain the following information:   The namespace  The series ID (byte blob)  The period of time being requested (start and end)   M3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:   Does the series exist? and if it does  Does the data exist in an \"active buffer\" (actively being compressed by an encoder), cached in-memory, on disk, or some combination of all three?   Determining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn't, then M3DB consults an in-memory bloom filter(s) for that shard / block start combination(s) to determine if the series exists on disk.  If the series exists, then for every block that the request spans, M3DB needs to consolidate data from the active buffers, in-memory cache, and fileset files (disk).  Lets imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a blocksize of 2 hours (i.e we need to find 3 different blocks.)  If the current time is 8PM, then the location of the requested blocks might be as follows:  [2PM - 4PM (FileSet file)]    - Sealed and flushed block that isn't cached\n[4PM - 6PM (In-memory cache)] - Sealed and flush block that is cached\n[6PM - 8PM (active buffer)]   - Hasn't been sealed or flushed yet  Then M3DB will need to consolidate:   The not-yet-sealed block from the active buffers / encoders (located inside an internal lookup in the Series object)  [6PM - 8PM]  The in-memory cached block (also located inside an internal lookup in the Series object)  [4PM - 6PM]  The block from disk (the block retrieve from disk will then be cached according to the current  caching policy   [2PM - 4PM]   Retrieving blocks from the active buffers and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:   Consult the in-memory bloom filter to determine if its possible the series exists on disk.  If the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is  before  the series ID that we're searching for. Review the  index_lookup.go  file for implementation details.  Jump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we're looking for  or  we get far enough in the index file that it becomes clear that the ID we're looking for doesn't exist (this is possible because the index file is sorted by ID)  Jump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.   Once M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or not the client returns a success to the caller for the read is dependent on the configured  consistency level .  Note:  Since M3DB nodes return compressed blocks (the M3DB client decompresses them), it's not possible to return \"partial results\" for a given block. If any portion of a read request spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.",
            "title": "Read Path"
        },
        {
            "location": "/m3db/architecture/engine/#background-processes",
            "text": "M3DB has a variety of processes that run in the background during normal operation.",
            "title": "Background processes"
        },
        {
            "location": "/m3db/architecture/engine/#ticking",
            "text": "The ticking process runs continously in the background and is responsible for a variety of tasks:   Merging all encoders for a given series / block start combination  Removing expired / flushed series and blocks from memory  Cleanup of expired data (fileset/commit log) from the filesystem",
            "title": "Ticking"
        },
        {
            "location": "/m3db/architecture/engine/#merging-all-encoders",
            "text": "M3TSZ is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For monitoring workloads this works very well because every subsequent datapoint is almost always chronologically after the previous one. However, real world systems are messy and occasionally out of order writes will be received. When this happens, M3DB will allocate a new encoder for the out of order datapoints. The multiple encoders need to be merged before flushing the data to disk, but to prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.",
            "title": "Merging all encoders"
        },
        {
            "location": "/m3db/architecture/engine/#removing-expired-flushed-series-and-blocks-from-memory",
            "text": "Depending on the configured  caching policy , the  in-memory object layout  can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer need to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.",
            "title": "Removing expired / flushed series and blocks from memory"
        },
        {
            "location": "/m3db/architecture/engine/#flushing",
            "text": "As discussed in the  architecture  section, writes are actively buffered / compressed in-memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of  fileset files  to facilitate efficient storage and retrieval.  This is where the configurable \"blocksize\" comes into play. The blocksize is simply a duration of time that dictates how long active writes will be compressed (in a streaming manner) in memory before being \"sealed\" (marked as immutable) and flushed to disk. Lets use a blocksize of two hours as an example.  If the blocksize is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the  fileset files  will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.",
            "title": "Flushing"
        },
        {
            "location": "/m3db/architecture/engine/#caveats-limitations",
            "text": "M3DB currently supports exact ID based lookups. It does not support tag/secondary indexing. This feature is under development and future versions of M3DB will have support for a built-in reverse index.  M3DB does not support updates / deletes. All data written to M3DB is immutable.  M3DB does not support writing arbitrarily into the past and future. This is generally fine for monitoring workloads, but can be problematic for traditional  OLTP  and  OLAP  workloads. Future versions of M3DB will have better support for writes with arbitrary timestamps.  M3DB does not support writing datapoints with values other than double-precision floats. Future versions of M3DB will have support for storing arbitrary values.  M3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value (Uber has production databases running with retention periods as high as 5 years), its still required and generally speaking M3DB is optimized for workloads with a well-defined  TTL .  M3DB does not support either background data repair or Cassandra-style  read repairs . Future versions of M3DB will support automatic repairs of data as an ongoing background process.",
            "title": "Caveats / Limitations"
        },
        {
            "location": "/m3db/architecture/sharding/",
            "text": "Sharding\n\n\nTimeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default \nmurmur3\n is used as the hashing function and 4096 virtual shards are configured.\n\n\nBenefits\n\n\nShards provide a variety of benefits throughout the M3DB stack:\n\n\n\n\nThey make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level.\n\n\nThey provide more fine grained lock granularity at the memory level.\n\n\nThey inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.\n\n\n\n\nReplication\n\n\nLogical shards are placed per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data is distinct to the racks that locate all other replicas\u2019 data.\n\n\nReplication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.\n\n\nReplica\n\n\nEach replica has its own assignment of a single logical shard per virtual shard.\n\n\nConceptually it can be defined as:\n\n\nReplica {\n  id uint32\n  shards []Shard\n}\n\n\n\n\nShard state\n\n\nEach shard can be conceptually defined as:\n\n\nShard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}\n\n\n\n\nShard assignment\n\n\nThe assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.\n\n\nFor a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as \nLEAVING\n and another host assigned as \nINITIALIZING\n for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only \nAVAILABLE\n shards count towards consistency, the work to group the \nLEAVING\n and \nINITIALIZING\n shards together when calculating a write success/error is not complete, see \nissue 417\n.\n\n\nIt is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the \nINITIALIZING\n state and to transition the state to \nAVAILABLE\n once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the \nLEAVING\n shard still assigned to the node that previously owned it and transitions the shard state on the new node from \nINITIALIZING\n state to \nAVAILABLE\n.\n\n\nNodes will not start serving reads for the new shard until it is \nAVAILABLE\n, meaning not until they have bootstrapped data for those shards.\n\n\nCluster operations\n\n\nNode add\n\n\nWhen a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become \nINITIALIZING\n, the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as \nLEAVING\n.\n\n\nNode down\n\n\nA node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.\n\n\nNode remove\n\n\nWhen a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are \nINITIALIZING\n and need to be bootstrapped and will begin bootstrapping the data using all replicas available.",
            "title": "Sharding and Replication"
        },
        {
            "location": "/m3db/architecture/sharding/#sharding",
            "text": "Timeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default  murmur3  is used as the hashing function and 4096 virtual shards are configured.",
            "title": "Sharding"
        },
        {
            "location": "/m3db/architecture/sharding/#benefits",
            "text": "Shards provide a variety of benefits throughout the M3DB stack:   They make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level.  They provide more fine grained lock granularity at the memory level.  They inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.",
            "title": "Benefits"
        },
        {
            "location": "/m3db/architecture/sharding/#replication",
            "text": "Logical shards are placed per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data is distinct to the racks that locate all other replicas\u2019 data.  Replication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.",
            "title": "Replication"
        },
        {
            "location": "/m3db/architecture/sharding/#replica",
            "text": "Each replica has its own assignment of a single logical shard per virtual shard.  Conceptually it can be defined as:  Replica {\n  id uint32\n  shards []Shard\n}",
            "title": "Replica"
        },
        {
            "location": "/m3db/architecture/sharding/#shard-state",
            "text": "Each shard can be conceptually defined as:  Shard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}",
            "title": "Shard state"
        },
        {
            "location": "/m3db/architecture/sharding/#shard-assignment",
            "text": "The assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.  For a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as  LEAVING  and another host assigned as  INITIALIZING  for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only  AVAILABLE  shards count towards consistency, the work to group the  LEAVING  and  INITIALIZING  shards together when calculating a write success/error is not complete, see  issue 417 .  It is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the  INITIALIZING  state and to transition the state to  AVAILABLE  once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the  LEAVING  shard still assigned to the node that previously owned it and transitions the shard state on the new node from  INITIALIZING  state to  AVAILABLE .  Nodes will not start serving reads for the new shard until it is  AVAILABLE , meaning not until they have bootstrapped data for those shards.",
            "title": "Shard assignment"
        },
        {
            "location": "/m3db/architecture/sharding/#cluster-operations",
            "text": "",
            "title": "Cluster operations"
        },
        {
            "location": "/m3db/architecture/sharding/#node-add",
            "text": "When a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become  INITIALIZING , the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as  LEAVING .",
            "title": "Node add"
        },
        {
            "location": "/m3db/architecture/sharding/#node-down",
            "text": "A node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.",
            "title": "Node down"
        },
        {
            "location": "/m3db/architecture/sharding/#node-remove",
            "text": "When a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are  INITIALIZING  and need to be bootstrapped and will begin bootstrapping the data using all replicas available.",
            "title": "Node remove"
        },
        {
            "location": "/m3db/architecture/consistencylevels/",
            "text": "Consistency Levels\n\n\nM3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.\n\n\nWrite consistency levels\n\n\n\n\n\n\nOne:\n Corresponds to a single node succeeding for an operation to succeed.\n\n\n\n\n\n\nMajority:\n Corresponds to the majority of nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nAll:\n Corresponds to all nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nRead consistency levels\n\n\n\n\n\n\nOne\n: Corresponds to reading from a single node to designate success.\n\n\n\n\n\n\nUnstrictMajority\n: Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.\n\n\n\n\n\n\nMajority\n: Corresponds to reading from the majority of nodes to designate success.\n\n\n\n\n\n\nAll:\n Corresponds to reading from all of the nodes to designate success.\n\n\n\n\n\n\nConnect consistency levels\n\n\nConnect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.\n\n\n\n\n\n\nAny:\n Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.\n\n\n\n\n\n\nNone:\n Corresponds to connecting to no nodes for all shards and as such will always succeed.\n\n\n\n\n\n\nOne:\n Corresponds to connecting to a single node for all shards.\n\n\n\n\n\n\nMajority:\n Corresponds to connecting to the majority of nodes for all shards.\n\n\n\n\n\n\nAll:\n Corresponds to connecting to all of the nodes for all shards.",
            "title": "Consistency Levels"
        },
        {
            "location": "/m3db/architecture/consistencylevels/#consistency-levels",
            "text": "M3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.",
            "title": "Consistency Levels"
        },
        {
            "location": "/m3db/architecture/consistencylevels/#write-consistency-levels",
            "text": "One:  Corresponds to a single node succeeding for an operation to succeed.    Majority:  Corresponds to the majority of nodes succeeding for an operation to succeed.    All:  Corresponds to all nodes succeeding for an operation to succeed.",
            "title": "Write consistency levels"
        },
        {
            "location": "/m3db/architecture/consistencylevels/#read-consistency-levels",
            "text": "One : Corresponds to reading from a single node to designate success.    UnstrictMajority : Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.    Majority : Corresponds to reading from the majority of nodes to designate success.    All:  Corresponds to reading from all of the nodes to designate success.",
            "title": "Read consistency levels"
        },
        {
            "location": "/m3db/architecture/consistencylevels/#connect-consistency-levels",
            "text": "Connect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.    Any:  Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.    None:  Corresponds to connecting to no nodes for all shards and as such will always succeed.    One:  Corresponds to connecting to a single node for all shards.    Majority:  Corresponds to connecting to the majority of nodes for all shards.    All:  Corresponds to connecting to all of the nodes for all shards.",
            "title": "Connect consistency levels"
        },
        {
            "location": "/m3db/architecture/storage/",
            "text": "Storage\n\n\nOverview\n\n\nThe primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.\n\n\nThey are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)\n\n\nFileSets\n\n\nA fileset has the following files:\n\n\n\n\nInfo file:\n Stores the block time window start and size and other important metadata about the fileset volume.\n\n\nSummaries file:\n Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.\n\n\nIndex file:\n Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval.\n\n\nData file:\n Stores the series compressed data streams.\n\n\nBloom filter file:\n Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.\n\n\nDigests file:\n Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.\n\n\nCheckpoint file:\n Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.\n\n\n\n\n                                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File      \u2502\n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500>\u2502- Idx                \u2502\n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                 \u2502\n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size               \u2502\n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum           \u2502\n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset  \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2502- Encoded Tags       |  |\n\u2502- Snapshot Time      \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502- Type (Flush/Snap)  \u2502                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n                                                                              \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502\n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502\n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502\n\u2502- Index digest       \u2502                           \u2514\u2500>\u2502  - Marker (16 bytes)\u2502\n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502\n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checkpoint File   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Digests digest     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.\n\n\nFileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.",
            "title": "Storage"
        },
        {
            "location": "/m3db/architecture/storage/#storage",
            "text": "",
            "title": "Storage"
        },
        {
            "location": "/m3db/architecture/storage/#overview",
            "text": "The primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.  They are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/storage/#filesets",
            "text": "A fileset has the following files:   Info file:  Stores the block time window start and size and other important metadata about the fileset volume.  Summaries file:  Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.  Index file:  Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval.  Data file:  Stores the series compressed data streams.  Bloom filter file:  Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.  Digests file:  Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.  Checkpoint file:  Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File      \u2502\n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500>\u2502- Idx                \u2502\n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                 \u2502\n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size               \u2502\n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum           \u2502\n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset  \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2502- Encoded Tags       |  |\n\u2502- Snapshot Time      \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502- Type (Flush/Snap)  \u2502                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n                                                                              \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502\n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502\n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502\n\u2502- Index digest       \u2502                           \u2514\u2500>\u2502  - Marker (16 bytes)\u2502\n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502\n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checkpoint File   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Digests digest     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.  FileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.",
            "title": "FileSets"
        },
        {
            "location": "/m3db/architecture/commitlogs/",
            "text": "Commit Logs And Snapshot Files\n\n\nOverview\n\n\nM3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)\n\n\nIntegrity Levels\n\n\nThere are two integrity levels available for commit logs:\n\n\n\n\nSynchronous:\n write operations must wait until it has finished writing an entry in the commit log to complete.\n\n\nBehind:\n write operations must finish enqueueing an entry to the commit log write queue to complete.\n\n\n\n\nDepending on the data loss requirements users can choose either integrity level.\n\n\nProperties\n\n\nCommit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.\n\n\nStructure\n\n\nCommit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.\n\n\nThe structures can be conceptually described as:\n\n\nCommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}\n\n\n\n\nCompaction / Snapshotting\n\n\nCommit log files are compacted via the snapshotting proccess which (if enabled at the namespace level) will snapshot all data in memory into compressed files which have the same structure as the \nfileset files\n but are stored in a different location. Once these snapshot files are created, then all the commit log files whose data are captured by the snapshot files can be deleted. This can result in significant disk savings for M3DB nodes running with large block sizes and high write volume where the size of the (uncompressed) commit logs can quickly get out of hand.\n\n\nIn addition, since the snapshot files are already compressed, bootstrapping from them is much faster than bootstrapping from raw commit log files because the individual data points don't need to be decoded and then M3TSZ encoded. The M3DB node just needs to read the raw bytes off disk and load them into memory.\n\n\nCleanup\n\n\nCommit log files are automatically deleted once all the data they contain has been flushed to disk as immutable compressed filesets \nor\n all the data they contain has been captured by a compressed snapshot file. Similarly, snapshot files are deleted once all the data they contain has been flushed to disk as filesets.",
            "title": "Commit Logs"
        },
        {
            "location": "/m3db/architecture/commitlogs/#commit-logs-and-snapshot-files",
            "text": "",
            "title": "Commit Logs And Snapshot Files"
        },
        {
            "location": "/m3db/architecture/commitlogs/#overview",
            "text": "M3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/commitlogs/#integrity-levels",
            "text": "There are two integrity levels available for commit logs:   Synchronous:  write operations must wait until it has finished writing an entry in the commit log to complete.  Behind:  write operations must finish enqueueing an entry to the commit log write queue to complete.   Depending on the data loss requirements users can choose either integrity level.",
            "title": "Integrity Levels"
        },
        {
            "location": "/m3db/architecture/commitlogs/#properties",
            "text": "Commit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.",
            "title": "Properties"
        },
        {
            "location": "/m3db/architecture/commitlogs/#structure",
            "text": "Commit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.  The structures can be conceptually described as:  CommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}",
            "title": "Structure"
        },
        {
            "location": "/m3db/architecture/commitlogs/#compaction-snapshotting",
            "text": "Commit log files are compacted via the snapshotting proccess which (if enabled at the namespace level) will snapshot all data in memory into compressed files which have the same structure as the  fileset files  but are stored in a different location. Once these snapshot files are created, then all the commit log files whose data are captured by the snapshot files can be deleted. This can result in significant disk savings for M3DB nodes running with large block sizes and high write volume where the size of the (uncompressed) commit logs can quickly get out of hand.  In addition, since the snapshot files are already compressed, bootstrapping from them is much faster than bootstrapping from raw commit log files because the individual data points don't need to be decoded and then M3TSZ encoded. The M3DB node just needs to read the raw bytes off disk and load them into memory.",
            "title": "Compaction / Snapshotting"
        },
        {
            "location": "/m3db/architecture/commitlogs/#cleanup",
            "text": "Commit log files are automatically deleted once all the data they contain has been flushed to disk as immutable compressed filesets  or  all the data they contain has been captured by a compressed snapshot file. Similarly, snapshot files are deleted once all the data they contain has been flushed to disk as filesets.",
            "title": "Cleanup"
        },
        {
            "location": "/m3db/architecture/peer_streaming/",
            "text": "Peer Streaming\n\n\nClient\n\n\nPeer streaming is managed by the M3DB client.  It fetches all blocks from peers for a specified time range for bootstrapping purposes.  It performs the following steps:\n\n\n\n\nFetch all metadata for blocks from all peers who own the specified shard\n\n\nCompares metadata from different peers and determines the best peer(s) from which to stream the actual data\n\n\nStreams the block data from peers\n\n\n\n\nSteps 1, 2 and 3 all happen concurrently.  As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata.  If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from.  If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.\n\n\nIn terms of error handling, the client will respect the consistency level specified for bootstrap.  This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata.  For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \"none\" is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers.  Fetching block data as well will indefinitely retry until the consistency level is achieved.\n\n\nThe client supports dynamically changing the bootstrap consistency level, which is helfpul in disaster scenarios where the consistency level cannot be achieved.  To break the indefinite streaming attempt an operator can change the consistency level to \"none\" and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.\n\n\nThe diagram below depicts the control flow and concurrency (goroutines and channels) in detail:\n\n\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                                               \u2502\n             \u2502         FetchBootstrapBlocksFromPeers         \u2502\n             \u2502                                               \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Main routine          \u2502\n\u2502                               \u2502\n\u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2) Spin up background routine \u2502                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      Create with metadataCh\n                \u2502                                \u2502\n                \u2502                                \u25bc\n                \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                               \u2502\n                \u2502                \u2502      Background routine       \u2502\n                \u2502                \u2502                               \u2502\n                \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                \u2502\n                \u2502                          For each peer\n                \u2502                                \u2502\n                \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u25bc                \u25bc                 \u25bc\n                \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502       StreamBlocksMetadataFromPeer        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502  Stream paginated blocks metadata from a  \u2502\n                \u2502          \u2502        peer while pageToken != nil        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502     For each blocks' metadata --> put     \u2502\n                \u2502          \u2502         metadata into metadataCh          \u2502\n                \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           StreamBlocksFromPeers           \u2502\n\u2502                                           \u2502\n\u2502 1) Create a background goroutine (details \u2502\n\u2502               to the right)               \u2502\n\u2502                                           \u2502\n\u2502 2) Create a queue per-peer which each have\u2502\n\u2502   their own internal goroutine and will   \u2502\n\u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              specific peer                \u2502          \u2502\n\u2502                                           \u2502          \u2502\n\u2502 3) Loop through the enqueCh and pick an   \u2502 Creates with metadataCh\n\u2502appropriate peer(s) for each series (based \u2502     and enqueueCh\n\u2502on whether all the peers have the same data\u2502          \u2502\n\u2502 or not) and then put that into the queue  \u2502          \u2502\n\u2502for that peer so the data will be streamed \u2502          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n                \u2502                                      \u25bc\n                \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n                \u2502    \u2502                streamMetadataFn variable)                \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n                \u2502    \u2502per series/block combination from different peers until we\u2502\n                \u2502    \u2502   have them from all peers for a series/block metadata   \u2502\n                \u2502    \u2502   combination and then \"submit\" them to the enqueueCh    \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502At the end, flush any remaining series/block combinations \u2502\n                \u2502    \u2502(that we received from less than N peers) into the enqueCh\u2502\n                \u2502    \u2502                         as well.                         \u2502\n                \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n          For each peer\n                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502            \u2502             \u2502\n   \u2502            \u2502             \u2502\n   \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Peer Streaming"
        },
        {
            "location": "/m3db/architecture/peer_streaming/#peer-streaming",
            "text": "",
            "title": "Peer Streaming"
        },
        {
            "location": "/m3db/architecture/peer_streaming/#client",
            "text": "Peer streaming is managed by the M3DB client.  It fetches all blocks from peers for a specified time range for bootstrapping purposes.  It performs the following steps:   Fetch all metadata for blocks from all peers who own the specified shard  Compares metadata from different peers and determines the best peer(s) from which to stream the actual data  Streams the block data from peers   Steps 1, 2 and 3 all happen concurrently.  As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata.  If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from.  If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.  In terms of error handling, the client will respect the consistency level specified for bootstrap.  This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata.  For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \"none\" is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers.  Fetching block data as well will indefinitely retry until the consistency level is achieved.  The client supports dynamically changing the bootstrap consistency level, which is helfpul in disaster scenarios where the consistency level cannot be achieved.  To break the indefinite streaming attempt an operator can change the consistency level to \"none\" and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.  The diagram below depicts the control flow and concurrency (goroutines and channels) in detail:               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                                               \u2502\n             \u2502         FetchBootstrapBlocksFromPeers         \u2502\n             \u2502                                               \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Main routine          \u2502\n\u2502                               \u2502\n\u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2) Spin up background routine \u2502                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      Create with metadataCh\n                \u2502                                \u2502\n                \u2502                                \u25bc\n                \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                               \u2502\n                \u2502                \u2502      Background routine       \u2502\n                \u2502                \u2502                               \u2502\n                \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                \u2502\n                \u2502                          For each peer\n                \u2502                                \u2502\n                \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u25bc                \u25bc                 \u25bc\n                \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502       StreamBlocksMetadataFromPeer        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502  Stream paginated blocks metadata from a  \u2502\n                \u2502          \u2502        peer while pageToken != nil        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502     For each blocks' metadata --> put     \u2502\n                \u2502          \u2502         metadata into metadataCh          \u2502\n                \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           StreamBlocksFromPeers           \u2502\n\u2502                                           \u2502\n\u2502 1) Create a background goroutine (details \u2502\n\u2502               to the right)               \u2502\n\u2502                                           \u2502\n\u2502 2) Create a queue per-peer which each have\u2502\n\u2502   their own internal goroutine and will   \u2502\n\u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              specific peer                \u2502          \u2502\n\u2502                                           \u2502          \u2502\n\u2502 3) Loop through the enqueCh and pick an   \u2502 Creates with metadataCh\n\u2502appropriate peer(s) for each series (based \u2502     and enqueueCh\n\u2502on whether all the peers have the same data\u2502          \u2502\n\u2502 or not) and then put that into the queue  \u2502          \u2502\n\u2502for that peer so the data will be streamed \u2502          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n                \u2502                                      \u25bc\n                \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n                \u2502    \u2502                streamMetadataFn variable)                \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n                \u2502    \u2502per series/block combination from different peers until we\u2502\n                \u2502    \u2502   have them from all peers for a series/block metadata   \u2502\n                \u2502    \u2502   combination and then \"submit\" them to the enqueueCh    \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502At the end, flush any remaining series/block combinations \u2502\n                \u2502    \u2502(that we received from less than N peers) into the enqueCh\u2502\n                \u2502    \u2502                         as well.                         \u2502\n                \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n          For each peer\n                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502            \u2502             \u2502\n   \u2502            \u2502             \u2502\n   \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Client"
        },
        {
            "location": "/m3db/architecture/caching/",
            "text": "Caching policies\n\n\nOverview\n\n\nBlocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don't need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \"cache\" itself is not a separate datastructure in memory, cached blocks are simply stored in their respective \nin-memory objects\n with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.\n\n\nFor general purpose workloads, the \nlru\n caching policy is reccommended.\n\n\nNone Cache Policy\n\n\nThe \nnone\n cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.\n\n\nAll Cache Policy\n\n\nThe \nall\n cache policy is the opposite of the \nnone\n cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.\n\n\nRecently Read Cache Policy\n\n\nThe \nrecently_read\n cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the \nrecently_read\n cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.\n\n\nData eviction from memory is triggered by the \"ticking\" process described in the \nbackground processes section\n\n\nLeast Recently Used (LRU) Cache Policy\n\n\nThe \nlru\n cache policy uses an \nlru\n list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in \nwired_list.go\n for implementation details.",
            "title": "Caching"
        },
        {
            "location": "/m3db/architecture/caching/#caching-policies",
            "text": "",
            "title": "Caching policies"
        },
        {
            "location": "/m3db/architecture/caching/#overview",
            "text": "Blocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don't need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \"cache\" itself is not a separate datastructure in memory, cached blocks are simply stored in their respective  in-memory objects  with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.  For general purpose workloads, the  lru  caching policy is reccommended.",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/caching/#none-cache-policy",
            "text": "The  none  cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.",
            "title": "None Cache Policy"
        },
        {
            "location": "/m3db/architecture/caching/#all-cache-policy",
            "text": "The  all  cache policy is the opposite of the  none  cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.",
            "title": "All Cache Policy"
        },
        {
            "location": "/m3db/architecture/caching/#recently-read-cache-policy",
            "text": "The  recently_read  cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the  recently_read  cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.  Data eviction from memory is triggered by the \"ticking\" process described in the  background processes section",
            "title": "Recently Read Cache Policy"
        },
        {
            "location": "/m3db/architecture/caching/#least-recently-used-lru-cache-policy",
            "text": "The  lru  cache policy uses an  lru  list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in  wired_list.go  for implementation details.",
            "title": "Least Recently Used (LRU) Cache Policy"
        },
        {
            "location": "/query_engine/",
            "text": "M3 Query, a distributed query engine for M3DB and Prometheus\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.",
            "title": "Introduction"
        },
        {
            "location": "/query_engine/#m3-query-a-distributed-query-engine-for-m3db-and-prometheus",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "M3 Query, a distributed query engine for M3DB and Prometheus"
        },
        {
            "location": "/query_engine/roadmap/",
            "text": "Roadmap\n\n\nLaunch M3 Coordinator as a bridge for the Read/Write path of M3DB into open source (Q1 2018)\n\n\nV1 (late Q4 2017 - early Q1 2018)\n\n\n\n\nCreate a gRPC/Protobuf service.\n\n\nHandlers for Prometheus remote read/write endpoints.\n\n\nPerform fanout to M3DB nodes.\n\n\nTooling to set up M3 Coordinator alongside Prometheus.\n\n\n\n\nV2 (late Q1 2018)\n\n\n\n\nSupport cross datacenter calls with remote aggregations.\n\n\nBenchmark the performance of the coordinator and M3DB using popular datasets.\n\n\nSupport for multiple M3DB clusters.\n\n\n\n\nM3 Query and optimizations for M3 Coordinator (Q2 2018)\n\n\n\n\nDedicated query engine service, M3 Query.\n\n\nSupport authentication and rate limiting.\n\n\nCost accounting per query and memory management to prevent M3 Query and M3 Coordinator from going OOM.\n\n\nPush computation to storage nodes whenever possible.\n\n\nExecution state manager to keep track of running queries.\n\n\nPort the distributed computation to M3 Query service.\n\n\n\n\nDashboards can directly interact with M3 Query or M3 Coordinator to get data from M3DB (Q3-Q4 2018)\n\n\n\n\nWrite a PromQL parser.\n\n\nWrite the current M3QL interfaces to conform to the common DAG structure.\n\n\nSuggest auto aggregation rules.\n\n\nProvide advanced query tracking to figure out bottlenecks.",
            "title": "Roadmap"
        },
        {
            "location": "/query_engine/roadmap/#roadmap",
            "text": "",
            "title": "Roadmap"
        },
        {
            "location": "/query_engine/roadmap/#launch-m3-coordinator-as-a-bridge-for-the-readwrite-path-of-m3db-into-open-source-q1-2018",
            "text": "",
            "title": "Launch M3 Coordinator as a bridge for the Read/Write path of M3DB into open source (Q1 2018)"
        },
        {
            "location": "/query_engine/roadmap/#v1-late-q4-2017-early-q1-2018",
            "text": "Create a gRPC/Protobuf service.  Handlers for Prometheus remote read/write endpoints.  Perform fanout to M3DB nodes.  Tooling to set up M3 Coordinator alongside Prometheus.",
            "title": "V1 (late Q4 2017 - early Q1 2018)"
        },
        {
            "location": "/query_engine/roadmap/#v2-late-q1-2018",
            "text": "Support cross datacenter calls with remote aggregations.  Benchmark the performance of the coordinator and M3DB using popular datasets.  Support for multiple M3DB clusters.",
            "title": "V2 (late Q1 2018)"
        },
        {
            "location": "/query_engine/roadmap/#m3-query-and-optimizations-for-m3-coordinator-q2-2018",
            "text": "Dedicated query engine service, M3 Query.  Support authentication and rate limiting.  Cost accounting per query and memory management to prevent M3 Query and M3 Coordinator from going OOM.  Push computation to storage nodes whenever possible.  Execution state manager to keep track of running queries.  Port the distributed computation to M3 Query service.",
            "title": "M3 Query and optimizations for M3 Coordinator (Q2 2018)"
        },
        {
            "location": "/query_engine/roadmap/#dashboards-can-directly-interact-with-m3-query-or-m3-coordinator-to-get-data-from-m3db-q3-q4-2018",
            "text": "Write a PromQL parser.  Write the current M3QL interfaces to conform to the common DAG structure.  Suggest auto aggregation rules.  Provide advanced query tracking to figure out bottlenecks.",
            "title": "Dashboards can directly interact with M3 Query or M3 Coordinator to get data from M3DB (Q3-Q4 2018)"
        },
        {
            "location": "/query_engine/architecture/",
            "text": "Architecture\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nM3 Query and M3 Coordinator are written entirely in Go, M3 Query is as a query engine for \nM3DB\n and M3 Coordinator is a remote read/write endpoint for Prometheus and M3DB. To learn more about Prometheus's remote endpoints and storage, \nsee here\n.",
            "title": "Overview"
        },
        {
            "location": "/query_engine/architecture/#architecture",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "Architecture"
        },
        {
            "location": "/query_engine/architecture/#overview",
            "text": "M3 Query and M3 Coordinator are written entirely in Go, M3 Query is as a query engine for  M3DB  and M3 Coordinator is a remote read/write endpoint for Prometheus and M3DB. To learn more about Prometheus's remote endpoints and storage,  see here .",
            "title": "Overview"
        },
        {
            "location": "/query_engine/architecture/blocks/",
            "text": "Blocks\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nThe fundamental data structures that M3 Query uses are \nBlocks\n. \nBlocks\n are what get created from the series iterators that M3DB returns. A \nBlock\n is associated with a start and end time. It contains data from multiple time series stored in columnar format.\n\n\nMost transformations within M3 Query will be applied across different series for each time interval. Therefore, having data stored in columnar format helps with the memory locality of the data. Moreover, most transformations within M3 Query can work in parallel on different blocks which can significantly increase the computation speed.\n\n\nDiagram\n\n\nBelow is a visual representation of a set of \nBlocks\n. On top is the M3QL query that gets executed, and on the bottom, are the results of the query containing 3 different Blocks.\n\n\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                                                       \u2502\n                              \u2502     fetch name:sign_up city_id:{new_york,san_diego,toronto} ios:*     \u2502\n                              \u2502                                                                       \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u25bc                        \u25bc                         \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502  Block One \u2502            \u2502  Block Two \u2502           \u2502 Block Three \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502   t  \u2502 t+1  \u2502 t+2  \u2502    \u2502  t+3 \u2502 t+4  \u2502 t+5  \u2502   \u2502  t+6 \u2502 t+7  \u2502 t+8  \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:new_york os:ios  \u2502 \u2502  5   \u2502  2   \u2502  10  \u2502    \u2502  10  \u2502  2   \u2502  10  \u2502   \u2502  5   \u2502  3   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502city_id:new_york os:android\u2502 \u2502  10  \u2502  8   \u2502  5   \u2502    \u2502  20  \u2502  4   \u2502  5   \u2502   \u2502  10  \u2502  8   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502 city_id:san_diego os:ios  \u2502 \u2502  10  \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  8   \u2502  6   \u2502  6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:toronto os:ios   \u2502 \u2502  2   \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  2   \u2502  5   \u2502  10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nM3DB => M3 Query Blocks\n\n\nIn order to convert M3DB blocks into M3 Query blocks, we need to consolidate across different namespaces. In short, M3DB namespaces are essentially different resolutions that metrics are stored at. For example, a metric might be stored at both 1min and 10min resolutions- meaning this metric is found in two namespaces.\n\n\nAt a high level, M3DB returns to M3 Query \nSeriesBlocks\n that contain a list of \nSeriesIterators\n for a given timeseries per namespace. M3 Query then aligns the blocks across common time bounds before applying consolidation.\n\n\nFor example, let's say we have a query that returns two timeseries from two different namespaces- 1min and 10min. When we create the M3 Query \nBlock\n, in order to accurately consolidate results from these two namespaces, we need to convert everything to have a 10min resolution. Otherwise it will not be possible to perform correctly apply functions.\n\n\n\n\nComing Soon: More documentation on how M3 Query applies consolidation.",
            "title": "Blocks"
        },
        {
            "location": "/query_engine/architecture/blocks/#blocks",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "Blocks"
        },
        {
            "location": "/query_engine/architecture/blocks/#overview",
            "text": "The fundamental data structures that M3 Query uses are  Blocks .  Blocks  are what get created from the series iterators that M3DB returns. A  Block  is associated with a start and end time. It contains data from multiple time series stored in columnar format.  Most transformations within M3 Query will be applied across different series for each time interval. Therefore, having data stored in columnar format helps with the memory locality of the data. Moreover, most transformations within M3 Query can work in parallel on different blocks which can significantly increase the computation speed.",
            "title": "Overview"
        },
        {
            "location": "/query_engine/architecture/blocks/#diagram",
            "text": "Below is a visual representation of a set of  Blocks . On top is the M3QL query that gets executed, and on the bottom, are the results of the query containing 3 different Blocks.                                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                                                       \u2502\n                              \u2502     fetch name:sign_up city_id:{new_york,san_diego,toronto} ios:*     \u2502\n                              \u2502                                                                       \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u25bc                        \u25bc                         \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502  Block One \u2502            \u2502  Block Two \u2502           \u2502 Block Three \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502   t  \u2502 t+1  \u2502 t+2  \u2502    \u2502  t+3 \u2502 t+4  \u2502 t+5  \u2502   \u2502  t+6 \u2502 t+7  \u2502 t+8  \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:new_york os:ios  \u2502 \u2502  5   \u2502  2   \u2502  10  \u2502    \u2502  10  \u2502  2   \u2502  10  \u2502   \u2502  5   \u2502  3   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502city_id:new_york os:android\u2502 \u2502  10  \u2502  8   \u2502  5   \u2502    \u2502  20  \u2502  4   \u2502  5   \u2502   \u2502  10  \u2502  8   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502 city_id:san_diego os:ios  \u2502 \u2502  10  \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  8   \u2502  6   \u2502  6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:toronto os:ios   \u2502 \u2502  2   \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  2   \u2502  5   \u2502  10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Diagram"
        },
        {
            "location": "/query_engine/architecture/blocks/#m3db-m3-query-blocks",
            "text": "In order to convert M3DB blocks into M3 Query blocks, we need to consolidate across different namespaces. In short, M3DB namespaces are essentially different resolutions that metrics are stored at. For example, a metric might be stored at both 1min and 10min resolutions- meaning this metric is found in two namespaces.  At a high level, M3DB returns to M3 Query  SeriesBlocks  that contain a list of  SeriesIterators  for a given timeseries per namespace. M3 Query then aligns the blocks across common time bounds before applying consolidation.  For example, let's say we have a query that returns two timeseries from two different namespaces- 1min and 10min. When we create the M3 Query  Block , in order to accurately consolidate results from these two namespaces, we need to convert everything to have a 10min resolution. Otherwise it will not be possible to perform correctly apply functions.   Coming Soon: More documentation on how M3 Query applies consolidation.",
            "title": "M3DB =&gt; M3 Query Blocks"
        },
        {
            "location": "/query_engine/architecture/functions/",
            "text": "Function Processing\n\n\nSupported Functions\n\n\n\n\n\n\n\n\nM3QL\n\n\nPrometheus\n\n\nGraphite\n\n\n\n\n\n\n\n\n\n\nabs/absolute\n\n\nabs()\n\n\nabsolute(seriesList)\n\n\n\n\n\n\nalias [alias]\n\n\n\n\nalias(seriesList, newName)\n\n\n\n\n\n\naliasByTags [tag]\n\n\n\n\naliasByTags(seriesList, *tags)\n\n\n\n\n\n\naliasByBucket/aliasByHistogramBucket [tag]\n\n\n\n\n\n\n\n\n\n\nanomalies [flags]\n\n\n\n\n\n\n\n\n\n\nasPercent\n\n\n/\n\n\nasPercent(seriesList, total=None, *nodes)\n\n\n\n\n\n\navg/averageSeries [tag]\n\n\navg()\n\n\naverageSeries(*seriesLists)\n\n\n\n\n\n\nchanged\n\n\n\n\nchanged(seriesList)\n\n\n\n\n\n\nconstantLine [value]\n\n\n\n\nconstantLine(value)\n\n\n\n\n\n\ncount\n\n\ncount()\n\n\ncountSeries(*seriesLists)\n\n\n\n\n\n\nderivative\n\n\n\n\nderivative(seriesList)\n\n\n\n\n\n\ndiff\n\n\n-\n\n\ndiffSeries(*seriesLists)\n\n\n\n\n\n\ndivideSeries\n\n\n/\n\n\ndivideSeries(dividendSeriesList, divisorSeries)\n\n\n\n\n\n\neq/== [value]\n\n\n==\n\n\nremoveBelowValue(seriesList, n)/removeAboveValue(seriesList, n)\n\n\n\n\n\n\nne/!= [value]\n\n\n!=\n\n\nremoveBelowValue(seriesList, n)/removeAboveValue(seriesList, n)\n\n\n\n\n\n\nexcludeByTag [tag, pattern]\n\n\n\n\nexclude(seriesList, pattern)\n\n\n\n\n\n\nexecute/exec [fetch]\n\n\n\n\n\n\n\n\n\n\nfallbackSeries [replacement]\n\n\n\n\nfallbackSeries(seriesList, fallback)\n\n\n\n\n\n\nfetch\n\n\n\n\n\n\n\n\n\n\nge/=> [value]\n\n\n>=\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\ngt/> [value]\n\n\n>\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\nhead [limit]\n\n\ntopk()\n\n\nhighest(seriesList, n=1, func='average')\n\n\n\n\n\n\nhistogramCDF [idTag, rangeTag, value]\n\n\n\n\n\n\n\n\n\n\nhistogramPercentile [idTag, rangeTag, percentileValue]\n\n\n\n\n\n\n\n\n\n\nidentity [name]\n\n\n\n\nidentity(name)\n\n\n\n\n\n\nintegral\n\n\n\n\nintegral(seriesList)\n\n\n\n\n\n\nintersect [tags]\n\n\nand/or\n\n\n\n\n\n\n\n\nisNonNull\n\n\n\n\nisNonNull(seriesList)\n\n\n\n\n\n\njainCP\n\n\n\n\n\n\n\n\n\n\nkeepLastValue\n\n\n\n\nkeepLastValue(seriesList, limit=inf)\n\n\n\n\n\n\nle/<= [value]\n\n\n<=\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nlogarithm\n\n\nln()\n\n\nlogarithm(seriesList, base=10)\n\n\n\n\n\n\nlt/< [value]\n\n\n<\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nmax/maxSeries [tag]\n\n\nmax()\n\n\nmaxSeries(*seriesLists)\n\n\n\n\n\n\nmin/minSeries [tag]\n\n\nmin()\n\n\nminSeries(*seriesLists)\n\n\n\n\n\n\nmoving [interval, func]\n\n\n_over_time()\n\n\nmovingMax, movingMin, movingMedian, movingAverage, etc.\n\n\n\n\n\n\nmultiply/multiplySeries [tag]\n\n\n*\n\n\nmultiplySeries(*seriesLists)\n\n\n\n\n\n\nnonNegativeDerivative [maxValue]\n\n\n\n\nnonNegativeDerivative(seriesList, maxValue=None)\n\n\n\n\n\n\nnPercentile [percentile]\n\n\n\n\nnPercentile(seriesList, n)\n\n\n\n\n\n\noffset [amount]\n\n\n\n\noffset(seriesList, factor)\n\n\n\n\n\n\npercentileOfSeries [n, true/false, tag]\n\n\n\n\npercentileOfSeries(seriesList, n, interpolate=False)\n\n\n\n\n\n\nperSecond\n\n\nrate()\n\n\nperSecond(seriesList, maxValue=None)\n\n\n\n\n\n\npromHistogramPercentile [percentileValue]\n\n\n\n\n\n\n\n\n\n\nrange [tag]\n\n\n\n\nrangeOfSeries(*seriesLists)\n\n\n\n\n\n\nremoveAbovePercentile [percentile]\n\n\n\n\nremoveAbovePercentile(seriesList, n)\n\n\n\n\n\n\nremoveBelowPercentile [percentile]\n\n\n\n\nremoveBelowPercentile(seriesList, n)\n\n\n\n\n\n\nremoveAboveValue [value]\n\n\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nremoveBelowValue [value]\n\n\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\nremoveEmpty\n\n\n\n\nremoveEmptySeries(seriesList, xFilesFactor=None)\n\n\n\n\n\n\nscale [factor]\n\n\n\n\nscale(seriesList, factor)\n\n\n\n\n\n\nscaleToSeconds [seconds]\n\n\n\n\nscaleToSeconds(seriesList, seconds)\n\n\n\n\n\n\nsetDiff [tags]\n\n\n\n\n\n\n\n\n\n\nshowAnomalyThresholds [level, model]\n\n\n\n\n\n\n\n\n\n\nshowTags [true/false, tagName(s)]\n\n\n\n\n\n\n\n\n\n\nsort/sortSeries [avg, current, max, stddev, sum]\n\n\nsort()\n\n\nsortBy(seriesList, func='average', reverse=False)\n\n\n\n\n\n\nstdev [points, windowTolerance]\n\n\nstddev()\n\n\nstdev(seriesList, points, windowTolerance=0.1)\n\n\n\n\n\n\nsqrt/squareRoot\n\n\nsqrt()\n\n\nsquareRoot(seriesList)\n\n\n\n\n\n\nsummarize [interval, func, alignToFrom]\n\n\n\n\nsummarize(seriesList, intervalString, func='sum', alignToFrom=False)\n\n\n\n\n\n\nsum/sumSeries [tag]\n\n\nsum()\n\n\nsumSeries(*seriesLists)\n\n\n\n\n\n\nsustain [duration]\n\n\n\n\n\n\n\n\n\n\nsustainedAbove & sustainedBelow\n\n\n\n\n\n\n\n\n\n\ntail [limit]\n\n\nbottomk()\n\n\nlowest(seriesList, n=1, func='average')\n\n\n\n\n\n\ntimeshift [duration]\n\n\n\n\ntimeShift(seriesList, timeShift, resetEnd=True, alignDST=False)\n\n\n\n\n\n\ntimestamp\n\n\ntimestamp()\n\n\n\n\n\n\n\n\ntransformNull [value]\n\n\n\n\ntransformNull(seriesList, default=0, referenceSeries=None)",
            "title": "Function Processing"
        },
        {
            "location": "/query_engine/architecture/functions/#function-processing",
            "text": "",
            "title": "Function Processing"
        },
        {
            "location": "/query_engine/architecture/functions/#supported-functions",
            "text": "M3QL  Prometheus  Graphite      abs/absolute  abs()  absolute(seriesList)    alias [alias]   alias(seriesList, newName)    aliasByTags [tag]   aliasByTags(seriesList, *tags)    aliasByBucket/aliasByHistogramBucket [tag]      anomalies [flags]      asPercent  /  asPercent(seriesList, total=None, *nodes)    avg/averageSeries [tag]  avg()  averageSeries(*seriesLists)    changed   changed(seriesList)    constantLine [value]   constantLine(value)    count  count()  countSeries(*seriesLists)    derivative   derivative(seriesList)    diff  -  diffSeries(*seriesLists)    divideSeries  /  divideSeries(dividendSeriesList, divisorSeries)    eq/== [value]  ==  removeBelowValue(seriesList, n)/removeAboveValue(seriesList, n)    ne/!= [value]  !=  removeBelowValue(seriesList, n)/removeAboveValue(seriesList, n)    excludeByTag [tag, pattern]   exclude(seriesList, pattern)    execute/exec [fetch]      fallbackSeries [replacement]   fallbackSeries(seriesList, fallback)    fetch      ge/=> [value]  >=  removeBelowValue(seriesList, n)    gt/> [value]  >  removeBelowValue(seriesList, n)    head [limit]  topk()  highest(seriesList, n=1, func='average')    histogramCDF [idTag, rangeTag, value]      histogramPercentile [idTag, rangeTag, percentileValue]      identity [name]   identity(name)    integral   integral(seriesList)    intersect [tags]  and/or     isNonNull   isNonNull(seriesList)    jainCP      keepLastValue   keepLastValue(seriesList, limit=inf)    le/<= [value]  <=  removeAboveValue(seriesList, n)    logarithm  ln()  logarithm(seriesList, base=10)    lt/< [value]  <  removeAboveValue(seriesList, n)    max/maxSeries [tag]  max()  maxSeries(*seriesLists)    min/minSeries [tag]  min()  minSeries(*seriesLists)    moving [interval, func]  _over_time()  movingMax, movingMin, movingMedian, movingAverage, etc.    multiply/multiplySeries [tag]  *  multiplySeries(*seriesLists)    nonNegativeDerivative [maxValue]   nonNegativeDerivative(seriesList, maxValue=None)    nPercentile [percentile]   nPercentile(seriesList, n)    offset [amount]   offset(seriesList, factor)    percentileOfSeries [n, true/false, tag]   percentileOfSeries(seriesList, n, interpolate=False)    perSecond  rate()  perSecond(seriesList, maxValue=None)    promHistogramPercentile [percentileValue]      range [tag]   rangeOfSeries(*seriesLists)    removeAbovePercentile [percentile]   removeAbovePercentile(seriesList, n)    removeBelowPercentile [percentile]   removeBelowPercentile(seriesList, n)    removeAboveValue [value]   removeAboveValue(seriesList, n)    removeBelowValue [value]   removeBelowValue(seriesList, n)    removeEmpty   removeEmptySeries(seriesList, xFilesFactor=None)    scale [factor]   scale(seriesList, factor)    scaleToSeconds [seconds]   scaleToSeconds(seriesList, seconds)    setDiff [tags]      showAnomalyThresholds [level, model]      showTags [true/false, tagName(s)]      sort/sortSeries [avg, current, max, stddev, sum]  sort()  sortBy(seriesList, func='average', reverse=False)    stdev [points, windowTolerance]  stddev()  stdev(seriesList, points, windowTolerance=0.1)    sqrt/squareRoot  sqrt()  squareRoot(seriesList)    summarize [interval, func, alignToFrom]   summarize(seriesList, intervalString, func='sum', alignToFrom=False)    sum/sumSeries [tag]  sum()  sumSeries(*seriesLists)    sustain [duration]      sustainedAbove & sustainedBelow      tail [limit]  bottomk()  lowest(seriesList, n=1, func='average')    timeshift [duration]   timeShift(seriesList, timeShift, resetEnd=True, alignDST=False)    timestamp  timestamp()     transformNull [value]   transformNull(seriesList, default=0, referenceSeries=None)",
            "title": "Supported Functions"
        },
        {
            "location": "/how_to/single_node/",
            "text": "M3DB Single Node Deployment\n\n\nDeploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it\nhas to offer. Our Docker image by default configures a single M3DB instance as one binary\ncontaining:\n\n\n\n\nAn M3DB storage instance (\nm3dbnode\n) for timeseries storage. This includes an embedded tag-based\n  metrics index, as well as as an embedded etcd server for storing the above mentioned cluster\n  topology and runtime configuration.\n\n\nA \"coordinator\" instance (\nm3coordinator\n) for writing and querying tagged metrics, as well as\n  managing cluster topology and runtime configuration.\n\n\n\n\nTo begin, first start up a Docker container with port \n7201\n (used to manage the cluster topology)\nand port \n9003\n (used to read and write metrics) exposed. We recommend you create a persistent data\ndirectory on your host for durability:\n\n\ndocker pull quay.io/m3/m3dbnode:latest\ndocker run -p 7201:7201 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3/m3dbnode:latest\n\n\n\n\n\n\n\n\n\n\nNext, create an initial namespace for your metrics in the database:\n\n\n\n\n\ncurl -X POST http://localhost:7201/api/v1/database/create -d '{\n  \"type\": \"local\",\n  \"namespaceName\": \"default\",\n  \"retentionTime\": \"48h\"\n}'\n\n\n\n\nShortly after, you should see your node complete bootstrapping! Don't worry if you see warnings or\nerrors related to a local cache file, such as \n[W] could not load cache from file\n/var/lib/m3kv/m3db_embedded.json\n. Those are expected for a local instance and in general any\nwarn-level errors (prefixed with \n[W]\n) should not block bootstrapping.\n\n\n02:28:30.008072[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n02:28:30.270681[I] node tchannelthrift: listening on 0.0.0.0:9000\n02:28:30.271909[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n02:28:30.519468[I] node httpjson: listening on 0.0.0.0:9002\n02:28:30.520061[I] cluster httpjson: listening on 0.0.0.0:9003\n02:28:30.520652[I] bootstrap finished [{namespace metrics} {duration 55.4\u00b5s}]\n02:28:30.520909[I] bootstrapped\n\n\n\n\nThe node also self-hosts its OpenAPI docs, outlining available endpoints. You can access this by\ngoing to \nlocalhost:7201/api/v1/openapi\n in your browser.\n\n\n\n\nNow you can experiment with writing tagged metrics:\n\n\ncurl -sSf -X POST http://localhost:9003/writetagged -d '{\n  \"namespace\": \"default\",\n  \"id\": \"foo\",\n  \"tags\": [\n    {\n      \"name\": \"city\",\n      \"value\": \"new_york\"\n    },\n    {\n      \"name\": \"endpoint\",\n      \"value\": \"/request\"\n    }\n  ],\n  \"datapoint\": {\n    \"timestamp\": '\"$(date \"+%s\")\"',\n    \"value\": 42.123456789\n  }\n}\n'\n\n\n\n\nAnd reading the metrics you've written:\n\n\ncurl -sSf -X POST http://localhost:9003/query -d '{\n  \"namespace\": \"default\",\n  \"query\": {\n    \"regexp\": {\n      \"field\": \"city\",\n      \"regexp\": \".*\"\n    }\n  },\n  \"rangeStart\": 0,\n  \"rangeEnd\": '\"$(date \"+%s\")\"'\n}' | jq .\n\n{\n  \"results\": [\n    {\n      \"id\": \"foo\",\n      \"tags\": [\n        {\n          \"name\": \"city\",\n          \"value\": \"new_york\"\n        },\n        {\n          \"name\": \"endpoint\",\n          \"value\": \"/request\"\n        }\n      ],\n      \"datapoints\": [\n        {\n          \"timestamp\": 1527039389,\n          \"value\": 42.123456789\n        }\n      ]\n    }\n  ],\n  \"exhaustive\": true\n}\n\n\n\n\nIntegrations\n\n\nPrometheus as a long term storage remote read/write endpoint\n.",
            "title": "M3DB Single Node Deployment"
        },
        {
            "location": "/how_to/single_node/#m3db-single-node-deployment",
            "text": "Deploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it\nhas to offer. Our Docker image by default configures a single M3DB instance as one binary\ncontaining:   An M3DB storage instance ( m3dbnode ) for timeseries storage. This includes an embedded tag-based\n  metrics index, as well as as an embedded etcd server for storing the above mentioned cluster\n  topology and runtime configuration.  A \"coordinator\" instance ( m3coordinator ) for writing and querying tagged metrics, as well as\n  managing cluster topology and runtime configuration.   To begin, first start up a Docker container with port  7201  (used to manage the cluster topology)\nand port  9003  (used to read and write metrics) exposed. We recommend you create a persistent data\ndirectory on your host for durability:  docker pull quay.io/m3/m3dbnode:latest\ndocker run -p 7201:7201 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3/m3dbnode:latest    Next, create an initial namespace for your metrics in the database:   curl -X POST http://localhost:7201/api/v1/database/create -d '{\n  \"type\": \"local\",\n  \"namespaceName\": \"default\",\n  \"retentionTime\": \"48h\"\n}'  Shortly after, you should see your node complete bootstrapping! Don't worry if you see warnings or\nerrors related to a local cache file, such as  [W] could not load cache from file\n/var/lib/m3kv/m3db_embedded.json . Those are expected for a local instance and in general any\nwarn-level errors (prefixed with  [W] ) should not block bootstrapping.  02:28:30.008072[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n02:28:30.270681[I] node tchannelthrift: listening on 0.0.0.0:9000\n02:28:30.271909[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n02:28:30.519468[I] node httpjson: listening on 0.0.0.0:9002\n02:28:30.520061[I] cluster httpjson: listening on 0.0.0.0:9003\n02:28:30.520652[I] bootstrap finished [{namespace metrics} {duration 55.4\u00b5s}]\n02:28:30.520909[I] bootstrapped  The node also self-hosts its OpenAPI docs, outlining available endpoints. You can access this by\ngoing to  localhost:7201/api/v1/openapi  in your browser.   Now you can experiment with writing tagged metrics:  curl -sSf -X POST http://localhost:9003/writetagged -d '{\n  \"namespace\": \"default\",\n  \"id\": \"foo\",\n  \"tags\": [\n    {\n      \"name\": \"city\",\n      \"value\": \"new_york\"\n    },\n    {\n      \"name\": \"endpoint\",\n      \"value\": \"/request\"\n    }\n  ],\n  \"datapoint\": {\n    \"timestamp\": '\"$(date \"+%s\")\"',\n    \"value\": 42.123456789\n  }\n}\n'  And reading the metrics you've written:  curl -sSf -X POST http://localhost:9003/query -d '{\n  \"namespace\": \"default\",\n  \"query\": {\n    \"regexp\": {\n      \"field\": \"city\",\n      \"regexp\": \".*\"\n    }\n  },\n  \"rangeStart\": 0,\n  \"rangeEnd\": '\"$(date \"+%s\")\"'\n}' | jq .\n\n{\n  \"results\": [\n    {\n      \"id\": \"foo\",\n      \"tags\": [\n        {\n          \"name\": \"city\",\n          \"value\": \"new_york\"\n        },\n        {\n          \"name\": \"endpoint\",\n          \"value\": \"/request\"\n        }\n      ],\n      \"datapoints\": [\n        {\n          \"timestamp\": 1527039389,\n          \"value\": 42.123456789\n        }\n      ]\n    }\n  ],\n  \"exhaustive\": true\n}",
            "title": "M3DB Single Node Deployment"
        },
        {
            "location": "/how_to/single_node/#integrations",
            "text": "Prometheus as a long term storage remote read/write endpoint .",
            "title": "Integrations"
        },
        {
            "location": "/how_to/cluster_hard_way/",
            "text": "M3DB Cluster Deployment, Manually (The Hard Way)\n\n\nIntroduction\n\n\nThis document lists the manual steps involved in deploying a M3DB cluster. In practice, you'd be automating this using Terraform or using Kubernetes rather than doing this by hand; guides for doing so are available under the How-To section.\n\n\nPrimer Architecture\n\n\nA quick primer on M3DB architecture. Here\u2019s what a typical deployment looks like:\n\n\n\n\nA few different things to highlight about the diagram:\n\n\nRole Type\n\n\nThere are three \u2018role types\u2019 for a m3db deployment -\n\n\n\n\n\n\nCoordinator: \nm3coordinator\n serves to coordinate reads and writes across all hosts in the cluster. It\u2019s a lightweight process, and does not store any data. This role would typically be run alongside a Prometheus instance, or be baked into a collector agent.\n\n\n\n\n\n\nStorage Node: \nm3dbnode\n processes running on these hosts are the workhorses of the database, they store data; and serve reads and writes.\n\n\n\n\n\n\nSeed Node: First and foremost, these hosts are storage nodes themselves. In addition to that responsibility, they run an embedded ETCD server. This is to allow the various M3DB processes running across the cluster to reason about the topology/configuration of the cluster in a consistent manner.\n\n\n\n\n\n\nNote: In very large deployments, you\u2019d use a dedicated ETCD cluster, and only use M3DB Storage and Coordinator Nodes\n\n\nProvisioning\n\n\nEnough background, lets get you going with a real cluster! Provision your host (be it VMs from AWS/GCP/etc) or bare-metal servers in your DC with the latest and greatest flavour of Linux you favor. M3DB works on all popular distributions - Ubuntu/RHEL/CentOS, let us know if you run into issues on another platform and we\u2019ll be happy to assist.\n\n\nNetwork\n\n\nIf you\u2019re using AWS or GCP it is highly advised to use static IPs so that if you need to replace a host, you don\u2019t have to update your configuration files on all the hosts, you simply decomission the old seed node and provision a new seed node with the same host ID and static IP that the old seed node had.  For AWS you can use a \nElastic Network Interface\n on a VPC and for GCP you can simply use an \ninternal static IP address\n.\n\n\nIn this example you will be creating three static IP addresses for the three seed nodes.\n\n\nFurther, we assume you have hostnames configured correctly too. i.e. running \nhostname\n on a host in the cluster returns the host ID you'll be using when specifying instance host IDs when creating the M3DB cluster placement. E.g. running \nhostname\n on a node \nm3db001\n should return it's host ID \nm3db001\n.\n\n\nIn GCP the name of your instance when you create it will automatically be it's hostname. When you create an instance click \"Management, disks, networking, SSH keys\" and under \"Networking\" click the default interface and click the \"Primary internal IP\" drop down and select \"Reserve a static internal IP address\" and give it a name, i.e. \nm3db001\n, a description that describes it's a seed node IP address and use \"Assign automatically\".\n\n\nIn AWS it might be simpler to just use whatever the hostname you get for the provisioned VM as your host ID when specifying M3DB placement.  Either that or use the \nenvironment\n host ID resolver and pass your host ID when launching the database process with an environment variable.  You can set to the host ID and specify the environment variable name in config as \nenvVarName: M3DB_HOST_ID\n if you are using an environment variable named \nM3DB_HOST_ID\n.\n\n\nRelevant config snippet:\n\n\nhostID:\n  resolver: environment\n  envVarName: M3DB_HOST_ID\n\n\n\n\nThen start your process with:\n\n\nM3DB_HOST_ID=m3db001 m3dbnode -f config.yml\n\n\n\n\nKernel\n\n\nEnsure you review our \nrecommended kernel configuration\n before running M3DB in production as M3DB may exceed the default limits for some default kernel values.\n\n\nConfig files\n\n\nWe wouldn\u2019t feel right to call this guide, \u201cThe Hard Way\u201d and not require you to change some configs by hand.\n\n\nNote: the steps that follow assume you have the following 3 seed nodes - make necessary adjustment if you have more or are using a dedicated ETCD cluster. Example seed nodes:\n\n\n\n\nm3db001 (Region=us-east1, Zone=us-east1-a, Static IP=10.142.0.1)\n\n\nm3db002 (Region=us-east1, Zone=us-east1-b, Static IP=10.142.0.2)\n\n\nm3db003 (Region=us-east1, Zone=us-east1-c, Static IP=10.142.0.3)\n\n\n\n\nWe\u2019re going to start with the M3DB config template and modify it to work for your cluster. Start by downloading the \nconfig\n. Update the config \u2018service\u2019 and 'seedNodes' sections to read as follows:\n\n\nconfig:\n  service:\n    env: default_env\n    zone: embedded\n    service: m3db\n    cacheDir: /var/lib/m3kv\n    etcdClusters:\n      - zone: embedded\n        endpoints:\n          - 10.142.0.1:2379\n          - 10.142.0.2:2379\n          - 10.142.0.3:2379\n  seedNodes:\n    initialCluster:\n      - hostID: m3db001\n        endpoint: http://10.142.0.1:2380\n      - hostID: m3db002\n        endpoint: http://10.142.0.2:2380\n      - hostID: m3db003\n        endpoint: http://10.142.0.3:2380\n\n\n\n\nStart the seed nodes\n\n\nTransfer the config you just crafted to each host in the cluster. And then starting with the seed nodes, start up the m3dbnode process:\n\n\nm3dbnode -f <config-name.yml>\n\n\n\n\nNote, remember to daemon-ize this using your favourite utility: systemd/init.d/supervisor/etc\n\n\nInitialize Topology\n\n\nM3DB calls its cluster topology \u2018placement\u2019. Run the command below on any of the seed nodes to initialize your first placement.\n\n\nNote: Isolation group specifies how the cluster places shards to avoid more than one replica of a shard appearing in the same replica group. As such you must be using at least as many isolation groups as your replication factor. In this example we use the availibity zones \nus-east1-a\n, \nus-east1-b\n, \nus-east1-c\n as our isolation groups which matches our replication factor of 3.\n\n\ncurl -X POST localhost:7201/api/v1/placement/init -d '{\n    \"num_shards\": 1024,\n    \"replication_factor\": 3,\n    \"instances\": [\n        {\n            \"id\": \"m3db001\",\n            \"isolation_group\": \"us-east1-a\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"10.142.0.1:9000\",\n            \"hostname\": \"m3db001\",\n            \"port\": 9000\n        },\n        {\n            \"id\": \"m3db002\",\n            \"isolation_group\": \"us-east1-b\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"10.142.0.2:9000\",\n            \"hostname\": \"m3db002-us-east\",\n            \"port\": 9000\n        },\n        {\n            \"id\": \"m3db003\",\n            \"isolation_group\": \"us-east1-c\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"10.142.0.3:9000\",\n            \"hostname\": \"m3db003\",\n            \"port\": 9000\n        }\n    ]\n}'\n\n\n\n\nCreate namespace(s)\n\n\nA namespace in M3DB is similar to a table in Cassandra (C*). You can specify retention and a few distinct properties on a namespace.\n\n\nRun the following on any seed node to create a \u2018metrics\u2019 namespace with 30 day retention, 12 hour block sizes, ability to write out of order datapoints into past or future by 1 hour:\n\n\ncurl -X POST localhost:7201/api/v1/namespace -d '{\n  \"name\": \"metrics\",\n  \"options\": {\n    \"bootstrapEnabled\": true,\n    \"flushEnabled\": true,\n    \"writesToCommitLog\": true,\n    \"cleanupEnabled\": true,\n    \"snapshotEnabled\": true,\n    \"repairEnabled\": false,\n    \"retentionOptions\": {\n      \"retentionPeriodDuration\": \"720h\",\n      \"blockSizeDuration\": \"12h\",\n      \"bufferFutureDuration\": \"1h\",\n      \"bufferPastDuration\": \"1h\",\n      \"blockDataExpiry\": true,\n      \"blockDataExpiryAfterNotAccessPeriodDuration\": \"5m\"\n    },\n    \"indexOptions\": {\n      \"enabled\": true,\n      \"blockSizeDuration\": \"12h\"\n    }\n  }\n}'\n\n\n\n\nShortly after, you should see your node complete bootstrapping:\n\n\n20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts\n\n\n\n\nRead more about namespaces and the various knobs in the docs.\n\n\nTest it out\n\n\nNow you can experiment with writing tagged metrics:\n\n\ncurl -sSf -X POST localhost:9003/writetagged -d '{\n  \"namespace\": \"metrics\",\n  \"id\": \"foo\",\n  \"tags\": [\n    {\n      \"name\": \"city\",\n      \"value\": \"new_york\"\n    },\n    {\n      \"name\": \"endpoint\",\n      \"value\": \"/request\"\n    }\n  ],\n  \"datapoint\": {\n    \"timestamp\": '\"$(date \"+%s\")\"',\n    \"value\": 42.123456789\n  }\n}'\n\n\n\n\nAnd reading the metrics you've written:\n\n\ncurl -sSf -X POST http://localhost:9003/query -d '{\n  \"namespace\": \"metrics\",\n  \"query\": {\n    \"regexp\": {\n      \"field\": \"city\",\n      \"regexp\": \".*\"\n    }\n  },\n  \"rangeStart\": 0,\n  \"rangeEnd\": '\"$(date \"+%s\")\"'\n}' | jq .\n\n\n\n\nIntegrations\n\n\nPrometheus as a long term storage remote read/write endpoint\n.",
            "title": "M3DB Cluster Deployment, Manually"
        },
        {
            "location": "/how_to/cluster_hard_way/#m3db-cluster-deployment-manually-the-hard-way",
            "text": "",
            "title": "M3DB Cluster Deployment, Manually (The Hard Way)"
        },
        {
            "location": "/how_to/cluster_hard_way/#introduction",
            "text": "This document lists the manual steps involved in deploying a M3DB cluster. In practice, you'd be automating this using Terraform or using Kubernetes rather than doing this by hand; guides for doing so are available under the How-To section.",
            "title": "Introduction"
        },
        {
            "location": "/how_to/cluster_hard_way/#primer-architecture",
            "text": "A quick primer on M3DB architecture. Here\u2019s what a typical deployment looks like:   A few different things to highlight about the diagram:",
            "title": "Primer Architecture"
        },
        {
            "location": "/how_to/cluster_hard_way/#role-type",
            "text": "There are three \u2018role types\u2019 for a m3db deployment -    Coordinator:  m3coordinator  serves to coordinate reads and writes across all hosts in the cluster. It\u2019s a lightweight process, and does not store any data. This role would typically be run alongside a Prometheus instance, or be baked into a collector agent.    Storage Node:  m3dbnode  processes running on these hosts are the workhorses of the database, they store data; and serve reads and writes.    Seed Node: First and foremost, these hosts are storage nodes themselves. In addition to that responsibility, they run an embedded ETCD server. This is to allow the various M3DB processes running across the cluster to reason about the topology/configuration of the cluster in a consistent manner.    Note: In very large deployments, you\u2019d use a dedicated ETCD cluster, and only use M3DB Storage and Coordinator Nodes",
            "title": "Role Type"
        },
        {
            "location": "/how_to/cluster_hard_way/#provisioning",
            "text": "Enough background, lets get you going with a real cluster! Provision your host (be it VMs from AWS/GCP/etc) or bare-metal servers in your DC with the latest and greatest flavour of Linux you favor. M3DB works on all popular distributions - Ubuntu/RHEL/CentOS, let us know if you run into issues on another platform and we\u2019ll be happy to assist.",
            "title": "Provisioning"
        },
        {
            "location": "/how_to/cluster_hard_way/#network",
            "text": "If you\u2019re using AWS or GCP it is highly advised to use static IPs so that if you need to replace a host, you don\u2019t have to update your configuration files on all the hosts, you simply decomission the old seed node and provision a new seed node with the same host ID and static IP that the old seed node had.  For AWS you can use a  Elastic Network Interface  on a VPC and for GCP you can simply use an  internal static IP address .  In this example you will be creating three static IP addresses for the three seed nodes.  Further, we assume you have hostnames configured correctly too. i.e. running  hostname  on a host in the cluster returns the host ID you'll be using when specifying instance host IDs when creating the M3DB cluster placement. E.g. running  hostname  on a node  m3db001  should return it's host ID  m3db001 .  In GCP the name of your instance when you create it will automatically be it's hostname. When you create an instance click \"Management, disks, networking, SSH keys\" and under \"Networking\" click the default interface and click the \"Primary internal IP\" drop down and select \"Reserve a static internal IP address\" and give it a name, i.e.  m3db001 , a description that describes it's a seed node IP address and use \"Assign automatically\".  In AWS it might be simpler to just use whatever the hostname you get for the provisioned VM as your host ID when specifying M3DB placement.  Either that or use the  environment  host ID resolver and pass your host ID when launching the database process with an environment variable.  You can set to the host ID and specify the environment variable name in config as  envVarName: M3DB_HOST_ID  if you are using an environment variable named  M3DB_HOST_ID .  Relevant config snippet:  hostID:\n  resolver: environment\n  envVarName: M3DB_HOST_ID  Then start your process with:  M3DB_HOST_ID=m3db001 m3dbnode -f config.yml",
            "title": "Network"
        },
        {
            "location": "/how_to/cluster_hard_way/#kernel",
            "text": "Ensure you review our  recommended kernel configuration  before running M3DB in production as M3DB may exceed the default limits for some default kernel values.",
            "title": "Kernel"
        },
        {
            "location": "/how_to/cluster_hard_way/#config-files",
            "text": "We wouldn\u2019t feel right to call this guide, \u201cThe Hard Way\u201d and not require you to change some configs by hand.  Note: the steps that follow assume you have the following 3 seed nodes - make necessary adjustment if you have more or are using a dedicated ETCD cluster. Example seed nodes:   m3db001 (Region=us-east1, Zone=us-east1-a, Static IP=10.142.0.1)  m3db002 (Region=us-east1, Zone=us-east1-b, Static IP=10.142.0.2)  m3db003 (Region=us-east1, Zone=us-east1-c, Static IP=10.142.0.3)   We\u2019re going to start with the M3DB config template and modify it to work for your cluster. Start by downloading the  config . Update the config \u2018service\u2019 and 'seedNodes' sections to read as follows:  config:\n  service:\n    env: default_env\n    zone: embedded\n    service: m3db\n    cacheDir: /var/lib/m3kv\n    etcdClusters:\n      - zone: embedded\n        endpoints:\n          - 10.142.0.1:2379\n          - 10.142.0.2:2379\n          - 10.142.0.3:2379\n  seedNodes:\n    initialCluster:\n      - hostID: m3db001\n        endpoint: http://10.142.0.1:2380\n      - hostID: m3db002\n        endpoint: http://10.142.0.2:2380\n      - hostID: m3db003\n        endpoint: http://10.142.0.3:2380",
            "title": "Config files"
        },
        {
            "location": "/how_to/cluster_hard_way/#start-the-seed-nodes",
            "text": "Transfer the config you just crafted to each host in the cluster. And then starting with the seed nodes, start up the m3dbnode process:  m3dbnode -f <config-name.yml>  Note, remember to daemon-ize this using your favourite utility: systemd/init.d/supervisor/etc",
            "title": "Start the seed nodes"
        },
        {
            "location": "/how_to/cluster_hard_way/#initialize-topology",
            "text": "M3DB calls its cluster topology \u2018placement\u2019. Run the command below on any of the seed nodes to initialize your first placement.  Note: Isolation group specifies how the cluster places shards to avoid more than one replica of a shard appearing in the same replica group. As such you must be using at least as many isolation groups as your replication factor. In this example we use the availibity zones  us-east1-a ,  us-east1-b ,  us-east1-c  as our isolation groups which matches our replication factor of 3.  curl -X POST localhost:7201/api/v1/placement/init -d '{\n    \"num_shards\": 1024,\n    \"replication_factor\": 3,\n    \"instances\": [\n        {\n            \"id\": \"m3db001\",\n            \"isolation_group\": \"us-east1-a\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"10.142.0.1:9000\",\n            \"hostname\": \"m3db001\",\n            \"port\": 9000\n        },\n        {\n            \"id\": \"m3db002\",\n            \"isolation_group\": \"us-east1-b\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"10.142.0.2:9000\",\n            \"hostname\": \"m3db002-us-east\",\n            \"port\": 9000\n        },\n        {\n            \"id\": \"m3db003\",\n            \"isolation_group\": \"us-east1-c\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"10.142.0.3:9000\",\n            \"hostname\": \"m3db003\",\n            \"port\": 9000\n        }\n    ]\n}'",
            "title": "Initialize Topology"
        },
        {
            "location": "/how_to/cluster_hard_way/#create-namespaces",
            "text": "A namespace in M3DB is similar to a table in Cassandra (C*). You can specify retention and a few distinct properties on a namespace.  Run the following on any seed node to create a \u2018metrics\u2019 namespace with 30 day retention, 12 hour block sizes, ability to write out of order datapoints into past or future by 1 hour:  curl -X POST localhost:7201/api/v1/namespace -d '{\n  \"name\": \"metrics\",\n  \"options\": {\n    \"bootstrapEnabled\": true,\n    \"flushEnabled\": true,\n    \"writesToCommitLog\": true,\n    \"cleanupEnabled\": true,\n    \"snapshotEnabled\": true,\n    \"repairEnabled\": false,\n    \"retentionOptions\": {\n      \"retentionPeriodDuration\": \"720h\",\n      \"blockSizeDuration\": \"12h\",\n      \"bufferFutureDuration\": \"1h\",\n      \"bufferPastDuration\": \"1h\",\n      \"blockDataExpiry\": true,\n      \"blockDataExpiryAfterNotAccessPeriodDuration\": \"5m\"\n    },\n    \"indexOptions\": {\n      \"enabled\": true,\n      \"blockSizeDuration\": \"12h\"\n    }\n  }\n}'  Shortly after, you should see your node complete bootstrapping:  20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts  Read more about namespaces and the various knobs in the docs.",
            "title": "Create namespace(s)"
        },
        {
            "location": "/how_to/cluster_hard_way/#test-it-out",
            "text": "Now you can experiment with writing tagged metrics:  curl -sSf -X POST localhost:9003/writetagged -d '{\n  \"namespace\": \"metrics\",\n  \"id\": \"foo\",\n  \"tags\": [\n    {\n      \"name\": \"city\",\n      \"value\": \"new_york\"\n    },\n    {\n      \"name\": \"endpoint\",\n      \"value\": \"/request\"\n    }\n  ],\n  \"datapoint\": {\n    \"timestamp\": '\"$(date \"+%s\")\"',\n    \"value\": 42.123456789\n  }\n}'  And reading the metrics you've written:  curl -sSf -X POST http://localhost:9003/query -d '{\n  \"namespace\": \"metrics\",\n  \"query\": {\n    \"regexp\": {\n      \"field\": \"city\",\n      \"regexp\": \".*\"\n    }\n  },\n  \"rangeStart\": 0,\n  \"rangeEnd\": '\"$(date \"+%s\")\"'\n}' | jq .",
            "title": "Test it out"
        },
        {
            "location": "/how_to/cluster_hard_way/#integrations",
            "text": "Prometheus as a long term storage remote read/write endpoint .",
            "title": "Integrations"
        },
        {
            "location": "/how_to/kubernetes/",
            "text": "M3DB on Kubernetes\n\n\nM3DB on Kubernetes is currently in the alpha phase of development. We currently provide static manifests to bootstrap a\ncluster. In the future, we hope to create an \noperator\n and leverage \ncustom resource\ndefinitions\n (CRDs) to automatically\nhandle operations such as managing cluster topology, but for now, our manifests should be adequate to get started.\n\n\nPrerequisites\n\n\nM3DB performs better when it has access to fast disks. Every incoming write is written to a commit log, which at high\nvolumes of writes can be sensitive to spikes in disk latency. Additionally the random seeks into files when loading cold\nfiles benefit from lower random read latency.\n\n\nBecause of this, the included manifests reference a\n\nStorageClass\n named \nfast\n. Manifests are\nprovided to provide such a StorageClass on AWS / Azure / GCP using the respective cloud provider's premium disk class.\n\n\nIf you do not already have a StorageClass named \nfast\n, create one using one of the provided manifests:\n\n\n# AWS EBS (class io1)\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-aws.yaml\n\n# Azure premium LRS\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-azure.yaml\n\n# GCE Persistent SSD\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-gcp.yaml\n\n\n\n\nIf you wish to use your cloud provider's default remote disk, or another disk class entirely, you'll have to modify them\nmanifests.\n\n\nDeploying\n\n\nApply the following manifest to create your cluster:\n\n\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/bundle.yaml\n\n\n\n\nApplying this bundle will create the following resources:\n\n\n\n\nAn \nm3db\n \nNamespace\n for\n   all M3DB-related resources.\n\n\nA 3-node etcd cluster in the form of a\n   \nStatefulSet\n backed by persistent\n   remote SSDs. This cluster stores the DB topology and other runtime configuration data.\n\n\nA 3-node M3DB cluster in the form of a StatefulSet.\n\n\nHeadless services\n for\n   the etcd and m3db StatefulSets to provide stable DNS hostnames per-pod.\n\n\n\n\nWait until all created pods are listed as ready:\n\n\n$ kubectl -n m3db get po\nNAME         READY     STATUS    RESTARTS   AGE\netcd-0       1/1       Running   0          22m\netcd-1       1/1       Running   0          22m\netcd-2       1/1       Running   0          22m\nm3dbnode-0   1/1       Running   0          22m\nm3dbnode-1   1/1       Running   0          22m\nm3dbnode-2   1/1       Running   0          22m\n\n\n\n\nYou can now proceed to initialize a namespace and placement for the cluster the same as you would for our other how-to\nguides:\n\n\n# Open a local connection to the coordinator service:\n$ kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -> 7201\nForwarding from [::1]:7201 -> 7201\n\n\n\n\n# Create an initial cluster topology\ncurl -sSf -X POST localhost:7201/api/v1/placement/init -d '{\n    \"num_shards\": 1024,\n    \"replication_factor\": 3,\n    \"instances\": [\n        {\n            \"id\": \"m3dbnode-0\",\n            \"isolation_group\": \"pod0\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"m3dbnode-0.m3dbnode:9000\",\n            \"hostname\": \"m3dbnode-0.m3dbnode\",\n            \"port\": 9000\n        },\n        {\n            \"id\": \"m3dbnode-1\",\n            \"isolation_group\": \"pod1\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"m3dbnode-1.m3dbnode:9000\",\n            \"hostname\": \"m3dbnode-1.m3dbnode\",\n            \"port\": 9000\n        },\n        {\n            \"id\": \"m3dbnode-2\",\n            \"isolation_group\": \"pod2\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"m3dbnode-2.m3dbnode:9000\",\n            \"hostname\": \"m3dbnode-2.m3dbnode\",\n            \"port\": 9000\n        }\n    ]\n}'\n\n\n\n\n# Create a namespace to hold your metrics\ncurl -X POST localhost:7201/api/v1/namespace -d '{\n  \"name\": \"default\",\n  \"options\": {\n    \"bootstrapEnabled\": true,\n    \"flushEnabled\": true,\n    \"writesToCommitLog\": true,\n    \"cleanupEnabled\": true,\n    \"snapshotEnabled\": true,\n    \"repairEnabled\": false,\n    \"retentionOptions\": {\n      \"retentionPeriodDuration\": \"720h\",\n      \"blockSizeDuration\": \"12h\",\n      \"bufferFutureDuration\": \"1h\",\n      \"bufferPastDuration\": \"1h\",\n      \"blockDataExpiry\": true,\n      \"blockDataExpiryAfterNotAccessPeriodDuration\": \"5m\"\n    },\n    \"indexOptions\": {\n      \"enabled\": true,\n      \"blockSizeDuration\": \"12h\"\n    }\n  }\n}'\n\n\n\n\nShortly after you should see your nodes finish bootstrapping:\n\n\n$ kubectl -n m3db logs -f m3dbnode-0\n21:36:54.831698[I] cluster database initializing topology\n21:36:54.831732[I] cluster database resolving topology\n21:37:22.821740[I] resolving namespaces with namespace watch\n21:37:22.821813[I] updating database namespaces [{adds [metrics]} {updates []} {removals []}]\n21:37:23.008109[I] node tchannelthrift: listening on 0.0.0.0:9000\n21:37:23.008384[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n21:37:23.217090[I] node httpjson: listening on 0.0.0.0:9002\n21:37:23.217240[I] cluster httpjson: listening on 0.0.0.0:9003\n21:37:23.217526[I] bootstrapping shards for range starting [{run bootstrap-data} {bootstrapper filesystem} ...\n...\n21:37:23.239534[I] bootstrap data fetched now initializing shards with series blocks [{namespace metrics} {numShards 256} {numSeries 0}]\n21:37:23.240778[I] bootstrap finished [{namespace metrics} {duration 23.325194ms}]\n21:37:23.240856[I] bootstrapped\n21:37:29.733025[I] successfully updated topology to 3 hosts\n\n\n\n\nYou can now write and read metrics using the API on the DB nodes:\n\n\n$ kubectl -n m3db port-forward svc/m3dbnode 9003\nForwarding from 127.0.0.1:9003 -> 9003\nForwarding from [::1]:9003 -> 9003\n\n\n\n\ncurl -sSf -X POST localhost:9003/writetagged -d '{\n  \"namespace\": \"default\",\n  \"id\": \"foo\",\n  \"tags\": [\n    {\n      \"name\": \"city\",\n      \"value\": \"new_york\"\n    },\n    {\n      \"name\": \"endpoint\",\n      \"value\": \"/request\"\n    }\n  ],\n  \"datapoint\": {\n    \"timestamp\": '\"$(date \"+%s\")\"',\n    \"value\": 42.123456789\n  }\n}'\n\n\n\n\n$ curl -sSf -X POST http://localhost:9003/query -d '{\n  \"namespace\": \"default\",\n  \"query\": {\n    \"regexp\": {\n      \"field\": \"city\",\n      \"regexp\": \".*\"\n    }\n  },\n  \"rangeStart\": 0,\n  \"rangeEnd\": '\"$(date \"+%s\")\"'\n}' | jq .\n\n{\n  \"results\": [\n    {\n      \"id\": \"foo\",\n      \"tags\": [\n        {\n          \"name\": \"city\",\n          \"value\": \"new_york\"\n        },\n        {\n          \"name\": \"endpoint\",\n          \"value\": \"/request\"\n        }\n      ],\n      \"datapoints\": [\n        {\n          \"timestamp\": 1527630053,\n          \"value\": 42.123456789\n        }\n      ]\n    }\n  ],\n  \"exhaustive\": true\n}\n\n\n\n\nAdding nodes\n\n\nYou can easily scale your M3DB cluster by scaling the StatefulSet and informing the cluster topology of the change:\n\n\nkubectl -n m3db scale --replicas=4 statefulset/m3dbnode\n\n\n\n\nOnce the pod is ready you can modify the cluster topology:\n\n\nkubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -> 7201\nForwarding from [::1]:7201 -> 7201\n\n\n\n\ncurl -sSf -X POST localhost:7201/api/v1/placement -d '{\n    \"instances\": [\n        {\n            \"id\": \"m3dbnode-3\",\n            \"isolation_group\": \"pod3\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"m3dbnode-3.m3dbnode:9000\",\n            \"hostname\": \"m3dbnode-3.m3dbnode\",\n            \"port\": 9000\n        }\n    ]\n}'\n\n\n\n\nIntegrations\n\n\nPrometheus\n\n\nAs mentioned in our integrations \nguide\n, M3DB can be used as a \nremote read/write\nendpoint\n for Prometheus.\n\n\nIf you run Prometheus on your Kubernetes cluster you can easily point it at M3DB in your Prometheus server config:\n\n\nremote_read:\n  - url: \"http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/read\"\n    # To test reading even when local Prometheus has the data\n    read_recent: true\n\nremote_write:\n  - url: \"http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/write\"\n    # To differentiate between local and remote storage we will add a storage label\n    write_relabel_configs:\n      - target_label: metrics_storage\n        replacement: m3db_remote\n\n\n\n\nScheduling\n\n\nIn some cases, you might prefer M3DB to run on certain nodes in your cluster. For example: if your cluster is comprised\nof different instance types and some have more memory than others then you'd like M3DB to run on those nodes if\npossible. To accommodate this, the pods created by the StatefulSets use \npod\naffinities\n and\n\ntolerations\n to prefer to run on\ncertain nodes. Specifically:\n\n\n\n\nThe pods tolerate the taint \n\"dedicated-m3db\"\n to run on nodes that are specifically dedicated to m3db if you so\n   choose.\n\n\nVia \nnodeAffinity\n the pods prefer to run on nodes with the label \nm3db.io/dedicated-m3db=\"true\"\n.",
            "title": "M3DB on Kubernetes"
        },
        {
            "location": "/how_to/kubernetes/#m3db-on-kubernetes",
            "text": "M3DB on Kubernetes is currently in the alpha phase of development. We currently provide static manifests to bootstrap a\ncluster. In the future, we hope to create an  operator  and leverage  custom resource\ndefinitions  (CRDs) to automatically\nhandle operations such as managing cluster topology, but for now, our manifests should be adequate to get started.",
            "title": "M3DB on Kubernetes"
        },
        {
            "location": "/how_to/kubernetes/#prerequisites",
            "text": "M3DB performs better when it has access to fast disks. Every incoming write is written to a commit log, which at high\nvolumes of writes can be sensitive to spikes in disk latency. Additionally the random seeks into files when loading cold\nfiles benefit from lower random read latency.  Because of this, the included manifests reference a StorageClass  named  fast . Manifests are\nprovided to provide such a StorageClass on AWS / Azure / GCP using the respective cloud provider's premium disk class.  If you do not already have a StorageClass named  fast , create one using one of the provided manifests:  # AWS EBS (class io1)\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-aws.yaml\n\n# Azure premium LRS\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-azure.yaml\n\n# GCE Persistent SSD\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-gcp.yaml  If you wish to use your cloud provider's default remote disk, or another disk class entirely, you'll have to modify them\nmanifests.",
            "title": "Prerequisites"
        },
        {
            "location": "/how_to/kubernetes/#deploying",
            "text": "Apply the following manifest to create your cluster:  kubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/bundle.yaml  Applying this bundle will create the following resources:   An  m3db   Namespace  for\n   all M3DB-related resources.  A 3-node etcd cluster in the form of a\n    StatefulSet  backed by persistent\n   remote SSDs. This cluster stores the DB topology and other runtime configuration data.  A 3-node M3DB cluster in the form of a StatefulSet.  Headless services  for\n   the etcd and m3db StatefulSets to provide stable DNS hostnames per-pod.   Wait until all created pods are listed as ready:  $ kubectl -n m3db get po\nNAME         READY     STATUS    RESTARTS   AGE\netcd-0       1/1       Running   0          22m\netcd-1       1/1       Running   0          22m\netcd-2       1/1       Running   0          22m\nm3dbnode-0   1/1       Running   0          22m\nm3dbnode-1   1/1       Running   0          22m\nm3dbnode-2   1/1       Running   0          22m  You can now proceed to initialize a namespace and placement for the cluster the same as you would for our other how-to\nguides:  # Open a local connection to the coordinator service:\n$ kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -> 7201\nForwarding from [::1]:7201 -> 7201  # Create an initial cluster topology\ncurl -sSf -X POST localhost:7201/api/v1/placement/init -d '{\n    \"num_shards\": 1024,\n    \"replication_factor\": 3,\n    \"instances\": [\n        {\n            \"id\": \"m3dbnode-0\",\n            \"isolation_group\": \"pod0\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"m3dbnode-0.m3dbnode:9000\",\n            \"hostname\": \"m3dbnode-0.m3dbnode\",\n            \"port\": 9000\n        },\n        {\n            \"id\": \"m3dbnode-1\",\n            \"isolation_group\": \"pod1\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"m3dbnode-1.m3dbnode:9000\",\n            \"hostname\": \"m3dbnode-1.m3dbnode\",\n            \"port\": 9000\n        },\n        {\n            \"id\": \"m3dbnode-2\",\n            \"isolation_group\": \"pod2\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"m3dbnode-2.m3dbnode:9000\",\n            \"hostname\": \"m3dbnode-2.m3dbnode\",\n            \"port\": 9000\n        }\n    ]\n}'  # Create a namespace to hold your metrics\ncurl -X POST localhost:7201/api/v1/namespace -d '{\n  \"name\": \"default\",\n  \"options\": {\n    \"bootstrapEnabled\": true,\n    \"flushEnabled\": true,\n    \"writesToCommitLog\": true,\n    \"cleanupEnabled\": true,\n    \"snapshotEnabled\": true,\n    \"repairEnabled\": false,\n    \"retentionOptions\": {\n      \"retentionPeriodDuration\": \"720h\",\n      \"blockSizeDuration\": \"12h\",\n      \"bufferFutureDuration\": \"1h\",\n      \"bufferPastDuration\": \"1h\",\n      \"blockDataExpiry\": true,\n      \"blockDataExpiryAfterNotAccessPeriodDuration\": \"5m\"\n    },\n    \"indexOptions\": {\n      \"enabled\": true,\n      \"blockSizeDuration\": \"12h\"\n    }\n  }\n}'  Shortly after you should see your nodes finish bootstrapping:  $ kubectl -n m3db logs -f m3dbnode-0\n21:36:54.831698[I] cluster database initializing topology\n21:36:54.831732[I] cluster database resolving topology\n21:37:22.821740[I] resolving namespaces with namespace watch\n21:37:22.821813[I] updating database namespaces [{adds [metrics]} {updates []} {removals []}]\n21:37:23.008109[I] node tchannelthrift: listening on 0.0.0.0:9000\n21:37:23.008384[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n21:37:23.217090[I] node httpjson: listening on 0.0.0.0:9002\n21:37:23.217240[I] cluster httpjson: listening on 0.0.0.0:9003\n21:37:23.217526[I] bootstrapping shards for range starting [{run bootstrap-data} {bootstrapper filesystem} ...\n...\n21:37:23.239534[I] bootstrap data fetched now initializing shards with series blocks [{namespace metrics} {numShards 256} {numSeries 0}]\n21:37:23.240778[I] bootstrap finished [{namespace metrics} {duration 23.325194ms}]\n21:37:23.240856[I] bootstrapped\n21:37:29.733025[I] successfully updated topology to 3 hosts  You can now write and read metrics using the API on the DB nodes:  $ kubectl -n m3db port-forward svc/m3dbnode 9003\nForwarding from 127.0.0.1:9003 -> 9003\nForwarding from [::1]:9003 -> 9003  curl -sSf -X POST localhost:9003/writetagged -d '{\n  \"namespace\": \"default\",\n  \"id\": \"foo\",\n  \"tags\": [\n    {\n      \"name\": \"city\",\n      \"value\": \"new_york\"\n    },\n    {\n      \"name\": \"endpoint\",\n      \"value\": \"/request\"\n    }\n  ],\n  \"datapoint\": {\n    \"timestamp\": '\"$(date \"+%s\")\"',\n    \"value\": 42.123456789\n  }\n}'  $ curl -sSf -X POST http://localhost:9003/query -d '{\n  \"namespace\": \"default\",\n  \"query\": {\n    \"regexp\": {\n      \"field\": \"city\",\n      \"regexp\": \".*\"\n    }\n  },\n  \"rangeStart\": 0,\n  \"rangeEnd\": '\"$(date \"+%s\")\"'\n}' | jq .\n\n{\n  \"results\": [\n    {\n      \"id\": \"foo\",\n      \"tags\": [\n        {\n          \"name\": \"city\",\n          \"value\": \"new_york\"\n        },\n        {\n          \"name\": \"endpoint\",\n          \"value\": \"/request\"\n        }\n      ],\n      \"datapoints\": [\n        {\n          \"timestamp\": 1527630053,\n          \"value\": 42.123456789\n        }\n      ]\n    }\n  ],\n  \"exhaustive\": true\n}",
            "title": "Deploying"
        },
        {
            "location": "/how_to/kubernetes/#adding-nodes",
            "text": "You can easily scale your M3DB cluster by scaling the StatefulSet and informing the cluster topology of the change:  kubectl -n m3db scale --replicas=4 statefulset/m3dbnode  Once the pod is ready you can modify the cluster topology:  kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -> 7201\nForwarding from [::1]:7201 -> 7201  curl -sSf -X POST localhost:7201/api/v1/placement -d '{\n    \"instances\": [\n        {\n            \"id\": \"m3dbnode-3\",\n            \"isolation_group\": \"pod3\",\n            \"zone\": \"embedded\",\n            \"weight\": 100,\n            \"endpoint\": \"m3dbnode-3.m3dbnode:9000\",\n            \"hostname\": \"m3dbnode-3.m3dbnode\",\n            \"port\": 9000\n        }\n    ]\n}'",
            "title": "Adding nodes"
        },
        {
            "location": "/how_to/kubernetes/#integrations",
            "text": "",
            "title": "Integrations"
        },
        {
            "location": "/how_to/kubernetes/#prometheus",
            "text": "As mentioned in our integrations  guide , M3DB can be used as a  remote read/write\nendpoint  for Prometheus.  If you run Prometheus on your Kubernetes cluster you can easily point it at M3DB in your Prometheus server config:  remote_read:\n  - url: \"http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/read\"\n    # To test reading even when local Prometheus has the data\n    read_recent: true\n\nremote_write:\n  - url: \"http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/write\"\n    # To differentiate between local and remote storage we will add a storage label\n    write_relabel_configs:\n      - target_label: metrics_storage\n        replacement: m3db_remote",
            "title": "Prometheus"
        },
        {
            "location": "/how_to/kubernetes/#scheduling",
            "text": "In some cases, you might prefer M3DB to run on certain nodes in your cluster. For example: if your cluster is comprised\nof different instance types and some have more memory than others then you'd like M3DB to run on those nodes if\npossible. To accommodate this, the pods created by the StatefulSets use  pod\naffinities  and tolerations  to prefer to run on\ncertain nodes. Specifically:   The pods tolerate the taint  \"dedicated-m3db\"  to run on nodes that are specifically dedicated to m3db if you so\n   choose.  Via  nodeAffinity  the pods prefer to run on nodes with the label  m3db.io/dedicated-m3db=\"true\" .",
            "title": "Scheduling"
        },
        {
            "location": "/operational_guide/placement/",
            "text": "Placement\n\n\nOverview\n\n\nNote\n: The words \nplacement\n and \ntopology\n are used interchangeably throughout the M3DB documentation and codebase.\n\n\nA M3DB cluster has exactly one Placement. That placement maps the cluster's shard replicas to nodes. A cluster also has 0 or more namespaces (analogous to tables in other databases), and each node serves every namespace for the shards it owns. In other words, if the cluster topology states that node A owns shards 1, 2, and 3 then node A will own shards 1, 2, 3 for all configured namespaces in the cluster.\n\n\nM3DB stores its placement (mapping of which NODES are responsible for which shards) in \netcd\n. There are three possible states that each node/shard pair can be in:\n\n\n\n\nInitializing\n\n\nAvailable\n\n\nLeaving\n\n\n\n\nNote that these states are not a reflection of the current status of an M3DB node, but an indication of whether a given node has ever successfully bootstrapped and taken ownership of a given shard (achieved goal state). For example, in a new cluster all the nodes will begin with all of their shards in the \nInitializing\n state. Once all the nodes finish bootstrapping, they will mark all of their shards as \nAvailable\n. If all the M3DB nodes are stopped at the same time, the cluster placement will still show all of the shards for all of the nodes as \nAvailable\n.\n\n\nInitializing State\n\n\nThe \nInitializing\n state is the state in which all new node/shard combinations begin. For example, upon creating a new placement all the node/shard pairs will begin in the \nInitializing\n state and only once they have successfully bootstrapped will they transition to the \nAvailable\n state.\n\n\nThe \nInitializing\n state is not limited to new placement, however, as it can also occur during placement changes. For example, during a node add/replace the new node will begin with all of its shards in the \nInitializing\n state until it can stream the data it is missing from its peers. During a node removal, all of the nodes who receive new shards (as a result of taking over the responsibilities of the node that is leaving) will begin with those shards marked as \nInitializing\n until they can stream in the data from the node leaving the cluster, or one of its peers.\n\n\nAvailable State\n\n\nOnce a node with a shard in the \nInitializing\n state successfully bootstraps all of the data for that shard, it will mark that shard as \nAvailable\n (for the single node) in the cluster placement.\n\n\nLeaving State\n\n\nThe \nLeaving\n state indicates that a node has been marked for removal from the cluster. The purpose of this state is to allow the node to remain in the cluster long enough for the nodes that are taking over its responsibilities to stream data from it.\n\n\nSample Cluster State Transitions - Node Add\n\n\nNode adds are performed by adding the new node to the placement. Some portion of the existing shards will be assigned to the new node based on its weight, and they will begin in the \nInitializing\n state. Similarly, the shards will be marked as \nLeaving\n on the node that are destined to lose ownership of them. Once the new node finishes bootstrapping the shards, it will update the placement to indicate that the shards it owns are \nAvailable\n and that the \nLeaving\n node should no longer own that shard in the placement.\n\n\nReplication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 1: Leaving     \u2502 \u2502 \u2502   Shard 1: Available  \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502Shard 1: Initializing \u2502 \u2502\n\u2502   2) Begin Node Add      \u2502 \u2502    Shard 2: Available   \u2502 \u2502 \u2502   Shard 2: Leaving    \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502Shard 2: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 3: Available   \u2502 \u2502 \u2502   Shard 3: Available  \u2502 \u2502 \u2502  Shard 3: Leaving    \u2502\u2502\u2502Shard 3: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  3) Complete Node Add    \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nSample Cluster State Transitions - Node Remove\n\n\nNode removes are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as \nLeaving\n and those shards are distributed to the remaining nodes (based on their weight) and assigned a state of \nInitializing\n. Once the existing nodes that are taking ownership of the leaving nodes shards finish bootstrapping, they will update the placement to indicate that the shards that they just acquired are \nAvailable\n and that the leaving node should no longer own those shards in the placement.\n\n\nReplication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502  Shard 1: Available   \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Initializing \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502\u2502  Shard 2: Available   \u2502\u2502\u2502   Shard 1: Leaving   \u2502 \u2502\n\u2502  2) Begin Node Remove    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Initializing\u2502 \u2502\u2502  Shard 3: Initializing\u2502\u2502\u2502   Shard 2: Leaving   \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502\u2502                       \u2502\u2502\u2502   Shard 3: Leaving   \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502    Shard 1: Avaiable    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  3) Complete Node Add    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nSample Cluster State Transitions - Node Replace\n\n\nNode replaces are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as \nLeaving\n and those shards are all added to the node that is being added and assigned a state of \nInitializing\n. Once the replacement node finishes bootstrapping, it will update the placement to indicate that the shards that it acquired are \nAvailable\n and that the leaving node should no longer own those shards in the placement.\n\n\nReplication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502\u2502   Shard 1: Leaving    \u2502\u2502\u2502Shard 1: Initializing \u2502 \u2502\n\u2502  2) Begin Node Remove    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502\u2502   Shard 2: Leaving    \u2502\u2502\u2502Shard 2: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502\u2502   Shard 3: Leaving    \u2502\u2502\u2502Shard 3: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                         \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 1: Avaiable    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502                         \u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  3) Complete Node Add    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502                         \u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502                         \u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                         \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nCluster State Transitions - Placement Updates Initiation\n\n\nThe diagram below depicts the sequence of events that happen during a node replace and illustrates which entity is performing the placement update (in etcd) at each step.\n\n\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502             Node A             \u2502\n \u2502                                \u2502\n \u2502       Shard 1: Available       \u2502\n \u2502       Shard 2: Available       \u2502     Operator performs node replace by\n \u2502       Shard 3: Available       \u2502      updating placement in etcd such\n \u2502                                \u2502     that shards on node A are marked\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     Leaving and shards on node B are\n                                  \u2502            marked Initializing\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u25bc\n                                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                   \u2502             Node A             \u2502\n                                                   \u2502                                \u2502\n                                                   \u2502        Shard 1: Leaving        \u2502\n                                                   \u2502        Shard 2: Leaving        \u2502\n                                                   \u2502        Shard 3: Leaving        \u2502\n                                                   \u2502                                \u2502\n                                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                   \u2502             Node B             \u2502\n                                                   \u2502                                \u2502\n                                                   \u2502     Shard 1: Initializing      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502     Shard 2: Initializing      \u2502\n\u2502                                \u2502                 \u2502     Shard 3: Initializing      \u2502\n\u2502                                \u2502                 \u2502                                \u2502\n\u2502             Node A             \u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                                \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n                                                                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502             Node B             \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2502       Shard 1: Available       \u2502   Node B completes bootstrapping and\n\u2502       Shard 2: Available       \u2502\u25c0\u2500\u2500\u2500\u2500updates placement (via etcd) to\n\u2502       Shard 3: Available       \u2502    indicate shard state is Available and\n\u2502                                \u2502    that Node A should no longer own any shards\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Placement/Topology"
        },
        {
            "location": "/operational_guide/placement/#placement",
            "text": "",
            "title": "Placement"
        },
        {
            "location": "/operational_guide/placement/#overview",
            "text": "Note : The words  placement  and  topology  are used interchangeably throughout the M3DB documentation and codebase.  A M3DB cluster has exactly one Placement. That placement maps the cluster's shard replicas to nodes. A cluster also has 0 or more namespaces (analogous to tables in other databases), and each node serves every namespace for the shards it owns. In other words, if the cluster topology states that node A owns shards 1, 2, and 3 then node A will own shards 1, 2, 3 for all configured namespaces in the cluster.  M3DB stores its placement (mapping of which NODES are responsible for which shards) in  etcd . There are three possible states that each node/shard pair can be in:   Initializing  Available  Leaving   Note that these states are not a reflection of the current status of an M3DB node, but an indication of whether a given node has ever successfully bootstrapped and taken ownership of a given shard (achieved goal state). For example, in a new cluster all the nodes will begin with all of their shards in the  Initializing  state. Once all the nodes finish bootstrapping, they will mark all of their shards as  Available . If all the M3DB nodes are stopped at the same time, the cluster placement will still show all of the shards for all of the nodes as  Available .",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/placement/#initializing-state",
            "text": "The  Initializing  state is the state in which all new node/shard combinations begin. For example, upon creating a new placement all the node/shard pairs will begin in the  Initializing  state and only once they have successfully bootstrapped will they transition to the  Available  state.  The  Initializing  state is not limited to new placement, however, as it can also occur during placement changes. For example, during a node add/replace the new node will begin with all of its shards in the  Initializing  state until it can stream the data it is missing from its peers. During a node removal, all of the nodes who receive new shards (as a result of taking over the responsibilities of the node that is leaving) will begin with those shards marked as  Initializing  until they can stream in the data from the node leaving the cluster, or one of its peers.",
            "title": "Initializing State"
        },
        {
            "location": "/operational_guide/placement/#available-state",
            "text": "Once a node with a shard in the  Initializing  state successfully bootstraps all of the data for that shard, it will mark that shard as  Available  (for the single node) in the cluster placement.",
            "title": "Available State"
        },
        {
            "location": "/operational_guide/placement/#leaving-state",
            "text": "The  Leaving  state indicates that a node has been marked for removal from the cluster. The purpose of this state is to allow the node to remain in the cluster long enough for the nodes that are taking over its responsibilities to stream data from it.",
            "title": "Leaving State"
        },
        {
            "location": "/operational_guide/placement/#sample-cluster-state-transitions-node-add",
            "text": "Node adds are performed by adding the new node to the placement. Some portion of the existing shards will be assigned to the new node based on its weight, and they will begin in the  Initializing  state. Similarly, the shards will be marked as  Leaving  on the node that are destined to lose ownership of them. Once the new node finishes bootstrapping the shards, it will update the placement to indicate that the shards it owns are  Available  and that the  Leaving  node should no longer own that shard in the placement.  Replication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 1: Leaving     \u2502 \u2502 \u2502   Shard 1: Available  \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502Shard 1: Initializing \u2502 \u2502\n\u2502   2) Begin Node Add      \u2502 \u2502    Shard 2: Available   \u2502 \u2502 \u2502   Shard 2: Leaving    \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502Shard 2: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 3: Available   \u2502 \u2502 \u2502   Shard 3: Available  \u2502 \u2502 \u2502  Shard 3: Leaving    \u2502\u2502\u2502Shard 3: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  3) Complete Node Add    \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Sample Cluster State Transitions - Node Add"
        },
        {
            "location": "/operational_guide/placement/#sample-cluster-state-transitions-node-remove",
            "text": "Node removes are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as  Leaving  and those shards are distributed to the remaining nodes (based on their weight) and assigned a state of  Initializing . Once the existing nodes that are taking ownership of the leaving nodes shards finish bootstrapping, they will update the placement to indicate that the shards that they just acquired are  Available  and that the leaving node should no longer own those shards in the placement.  Replication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502  Shard 1: Available   \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Initializing \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502\u2502  Shard 2: Available   \u2502\u2502\u2502   Shard 1: Leaving   \u2502 \u2502\n\u2502  2) Begin Node Remove    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Initializing\u2502 \u2502\u2502  Shard 3: Initializing\u2502\u2502\u2502   Shard 2: Leaving   \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502\u2502                       \u2502\u2502\u2502   Shard 3: Leaving   \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502    Shard 1: Avaiable    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  3) Complete Node Add    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Sample Cluster State Transitions - Node Remove"
        },
        {
            "location": "/operational_guide/placement/#sample-cluster-state-transitions-node-replace",
            "text": "Node replaces are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as  Leaving  and those shards are all added to the node that is being added and assigned a state of  Initializing . Once the replacement node finishes bootstrapping, it will update the placement to indicate that the shards that it acquired are  Available  and that the leaving node should no longer own those shards in the placement.  Replication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502\u2502   Shard 1: Leaving    \u2502\u2502\u2502Shard 1: Initializing \u2502 \u2502\n\u2502  2) Begin Node Remove    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502\u2502   Shard 2: Leaving    \u2502\u2502\u2502Shard 2: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502\u2502   Shard 3: Leaving    \u2502\u2502\u2502Shard 3: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                         \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 1: Avaiable    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502                         \u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  3) Complete Node Add    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502                         \u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502                         \u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                         \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Sample Cluster State Transitions - Node Replace"
        },
        {
            "location": "/operational_guide/placement/#cluster-state-transitions-placement-updates-initiation",
            "text": "The diagram below depicts the sequence of events that happen during a node replace and illustrates which entity is performing the placement update (in etcd) at each step.   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502             Node A             \u2502\n \u2502                                \u2502\n \u2502       Shard 1: Available       \u2502\n \u2502       Shard 2: Available       \u2502     Operator performs node replace by\n \u2502       Shard 3: Available       \u2502      updating placement in etcd such\n \u2502                                \u2502     that shards on node A are marked\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     Leaving and shards on node B are\n                                  \u2502            marked Initializing\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u25bc\n                                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                   \u2502             Node A             \u2502\n                                                   \u2502                                \u2502\n                                                   \u2502        Shard 1: Leaving        \u2502\n                                                   \u2502        Shard 2: Leaving        \u2502\n                                                   \u2502        Shard 3: Leaving        \u2502\n                                                   \u2502                                \u2502\n                                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                   \u2502             Node B             \u2502\n                                                   \u2502                                \u2502\n                                                   \u2502     Shard 1: Initializing      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502     Shard 2: Initializing      \u2502\n\u2502                                \u2502                 \u2502     Shard 3: Initializing      \u2502\n\u2502                                \u2502                 \u2502                                \u2502\n\u2502             Node A             \u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                                \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n                                                                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502             Node B             \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2502       Shard 1: Available       \u2502   Node B completes bootstrapping and\n\u2502       Shard 2: Available       \u2502\u25c0\u2500\u2500\u2500\u2500updates placement (via etcd) to\n\u2502       Shard 3: Available       \u2502    indicate shard state is Available and\n\u2502                                \u2502    that Node A should no longer own any shards\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Cluster State Transitions - Placement Updates Initiation"
        },
        {
            "location": "/operational_guide/placement_configuration/",
            "text": "Placement Configuration\n\n\nOverview\n\n\nM3DB was designed from the ground up to be a distributed (clustered) database that is availability zone or rack aware (by using isolation groups). Clusters will seamlessly scale with your data, and you can start with a small number of nodes and grow it to a size of several hundred nodes with no downtime or expensive migrations.\n\n\nBefore reading the rest of this document, we recommend familiarizing yourself with the \nM3DB placement documentation\n\n\nNote\n: The primary limiting factor for the maximum size of an M3DB cluster is the number of shards. Picking an appropriate number of shards is more of an art than a science, but our recommendation is as follows:\n\n\n\n\nResource Constrained / Development clusters: \n64 shards\n\n\nProduction clusters: \n1024 shards\n\n\nProduction clusters with high-resource nodes (Over 128GiB of ram, etc) and an expected cluster size of several hundred nodes: \n4096 shards\n\n\n\n\nAfter performing any of the instructions documented below a new placement will automatically be generated to distribute the shards among the M3DB nodes such that the isolation group and replication factor constraints are met.\n\n\nIf the constraints cannot be met, because there are not enough nodes to calculate a new placement such that each shard is replicated on the desired number of nodes with none of the nodes owning the same shard existing in the same isolation group, then the operation will fail.\n\n\nIn other words, all you have to do is issue the desired instruction and the M3 stack will take care of making sure that your data is distributed with appropriate replication and isolation.\n\n\nIn the case of the M3DB nodes, nodes that have received new shards will immediately begin receiving writes (but not serving reads) for the new shards that they are responsible for. They will also begin streaming in all the data for their newly acquired shards from the peers that already have data for those shards. Once the nodes have finished streaming in the data for the shards that they have acquired, they will mark their status for those shards as \nAvailable\n in the placement and begin accepting writes. Simultaneously, the nodes that are losing ownership of any shards will mark their status for those shards as \nLeaving\n. Once all the nodes accepting ownership of the new shards have finished streaming data from them, they will relinquish ownership of those shards and remove all the data associated with the shards they lost from memory and from disk.\n\n\nM3Coordinator nodes will also pickup the new placement from etcd and alter which M3DB nodes they issue writes and reads to appropriately.\n\n\nUnderstanding the Placement Configuration\n\n\nThe placement configuration contains a few core values that control how the placement behaves.\n\n\nID\n\n\nThis is the identifier for a node in the placement and can be any value that uniquely identifies an M3DB node.\n\n\nIsolation Group\n\n\nThis value controls how nodes that own the same M3DB shards are isolated from each other. For example, in a single datacenter configuration this value could be set to the rack that the M3DB node lives on. As a result, the placement will guarantee that nodes that exist on the same rack do not share any shards, allowing the cluster to survive the failure of an entire rack. Alternatively, if M3DB was deployed in an AWS region, the isolation group could be set to the region's availability zone and that would ensure that the cluster would survive the loss of an entire availability zone.\n\n\nZone\n\n\nThis value controls what etcd zone the M3DB node belongs to.\n\n\nWeight\n\n\nThis value should be an integer and controls how the cluster will weigh the number of shards that an individual node will own. If you're running the M3DB cluster on homogenous hardware, then you probably want to assign all M3DB nodes the same weight so that shards are distributed evenly. On the otherhand, if you're running the cluster on heterogenous hardware, then this value should be higher for nodes with higher resources for whatever the limiting factor is in your cluster setup. For example, if disk space (as opposed to memory or CPU) is the limiting factor in how many shards any given node in your cluster can tolerate, then you could assign a higher value to nodes in your cluster that have larger disks and the placement calculations would assign them a higher number of shards.\n\n\nEndpoint\n\n\nThis value should be in the form of \n:\n and identifies how network requests should be routed to this particular node in the placement.\n\n\nHostname\n\n\nThis value should be in the form of \n and identifies the address / host name of the M3DB node.\n\n\nPort\n\n\nThis value should be in the form of \n and identifies the port over which this M3DB node expects to receive traffic (defaults to 9000).\n\n\nPlacement Operations\n\n\nThe instructions below all contain sample curl commands, but you can always review the API documentation by navigating to\n\n\nhttp://<M3_COORDINATOR_HOST_NAME>:<CONFIGURED_PORT(default 7201)>/api/v1/openapi\n or our \nonline API documentation\n.\n\n\nNote\n: The \npeers bootstrapper\n must be configured on all nodes in the M3DB cluster for placement changes to work. The \npeers\n bootstrapper is enabled by default, so you only need to worry about this if you modified the default bootstrapping configuration\n\n\nPlacement Initialization\n\n\nSend a POST request to the \n/api/v1/services/m3db/placement/init\n endpoint\n\n\ncurl -X POST localhost:7201/api/v1/services/m3db/placement/init -d '{\n    \"num_shards\": <DESIRED_NUMBER_OF_SHARDS>,\n    \"replication_factor\": <DESIRED_REPLICATION_FACTOR>(recommended 3),\n    \"instances\": [\n        {\n            \"id\": \"<NODE_1_ID>\",\n            \"isolation_group\": \"<NODE_1_ISOLATION_GROUP>\",\n            \"zone\": \"<ETCD_ZONE>\",\n            \"weight\": <NODE_WEIGHT>,\n            \"endpoint\": \"<NODE_1_HOST_NAME>:<NODE_1_PORT>\",\n            \"hostname\": \"<NODE_1_HOST_NAME>\",\n            \"port\": <NODE_1_PORT>\n        },\n        {\n            \"id\": \"<NODE_2_ID>\",\n            \"isolation_group\": \"<NODE_2_ISOLATION_GROUP>\",\n            \"zone\": \"<ETCD_ZONE>\",\n            \"weight\": <NODE_WEIGHT>,\n            \"endpoint\": \"<NODE_2_HOST_NAME>:<NODE_2_PORT>\",\n            \"hostname\": \"<NODE_2_HOST_NAME>\",\n            \"port\": <NODE_2_PORT>\n        },\n        {\n            \"id\": \"<NODE_3_ID>\",\n            \"isolation_group\": \"<NODE_3_ISOLATION_GROUP>\",\n            \"zone\": \"<ETCD_ZONE>\",\n            \"weight\": <NODE_WEIGHT>,\n            \"endpoint\": \"<NODE_3_HOST_NAME>:<NODE_3_PORT>\",\n            \"hostname\": \"<NODE_3_HOST_NAME>\",\n            \"port\": <NODE_3_PORT>\n        },\n    ]\n}'\n\n\n\n\nAdding a Node\n\n\nSend a POST request to the \n/api/v1/services/m3db/placement\n endpoint\n\n\ncurl -X POST <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT(default 7201)>/api/v1/services/m3db/placement -d '{\n  \"instances\": [\n    {\n      \"id\": \"<NEW_NODE_ID>\",\n      \"isolationGroup\": \"<NEW_NODE_ISOLATION_GROUP>\",\n      \"zone\": \"<ETCD_ZONE>\",\n      \"weight\": <NODE_WEIGHT>,\n      \"endpoint\": \"<NEW_NODE_HOST_NAME>:<NEW_NODE_PORT>(default 9000)\",\n      \"hostname\": \"<NEW_NODE_HOST_NAME>\",\n      \"port\": <NEW_NODE_PORT>\n    }\n  ]\n}'\n\n\n\n\nAfter sending the add command you will need to wait for the M3DB cluster to reach the new desired state. You'll know that this has been achieved when the placement shows that all shards for all hosts are in the \nAvailable\n state.\n\n\nRemoving a Node\n\n\nSend a DELETE request to the \n/api/v1/services/m3db/placement/<NODE_ID>\n endpoint.\n\n\ncurl -X DELETE <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT(default 7201)>/api/v1/services/m3db/placement/<NODE_ID>\n\n\n\n\nAfter sending the delete command you will need to wait for the M3DB cluster to reach the new desired state. You'll know that this has been achieved when the placement shows that all shards for all hosts are in the \nAvailable\n state.\n\n\nReplacing a Node\n\n\nSend a POST request to the \n/api/v1/services/m3db/placement/replace\n endpoint containing hosts to replace and candidates to replace it with.\n\n\ncurl -X POST <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT(default 7201)>/api/v1/services/m3db/placement/replace -d '{\n    \"leavingInstanceIDs\": [\"<OLD_NODE_ID>\"],\n    \"candidates\": [\n        {\n          \"id\": \"<NEW_NODE_ID>\",\n          \"isolationGroup\": \"<NEW_NODE_ISOLATION_GROUP>\",\n          \"zone\": \"<ETCD_ZONE>\",\n          \"weight\": <NODE_WEIGHT>,\n          \"endpoint\": \"<NEW_NODE_HOST_NAME>:<NEW_NODE_PORT>(default 9000)\",\n          \"hostname\": \"<NEW_NODE_HOST_NAME>\",\n          \"port\": <NEW_NODE_PORT>\n        }\n    ]\n}'",
            "title": "Placement/Topology Configuration"
        },
        {
            "location": "/operational_guide/placement_configuration/#placement-configuration",
            "text": "",
            "title": "Placement Configuration"
        },
        {
            "location": "/operational_guide/placement_configuration/#overview",
            "text": "M3DB was designed from the ground up to be a distributed (clustered) database that is availability zone or rack aware (by using isolation groups). Clusters will seamlessly scale with your data, and you can start with a small number of nodes and grow it to a size of several hundred nodes with no downtime or expensive migrations.  Before reading the rest of this document, we recommend familiarizing yourself with the  M3DB placement documentation  Note : The primary limiting factor for the maximum size of an M3DB cluster is the number of shards. Picking an appropriate number of shards is more of an art than a science, but our recommendation is as follows:   Resource Constrained / Development clusters:  64 shards  Production clusters:  1024 shards  Production clusters with high-resource nodes (Over 128GiB of ram, etc) and an expected cluster size of several hundred nodes:  4096 shards   After performing any of the instructions documented below a new placement will automatically be generated to distribute the shards among the M3DB nodes such that the isolation group and replication factor constraints are met.  If the constraints cannot be met, because there are not enough nodes to calculate a new placement such that each shard is replicated on the desired number of nodes with none of the nodes owning the same shard existing in the same isolation group, then the operation will fail.  In other words, all you have to do is issue the desired instruction and the M3 stack will take care of making sure that your data is distributed with appropriate replication and isolation.  In the case of the M3DB nodes, nodes that have received new shards will immediately begin receiving writes (but not serving reads) for the new shards that they are responsible for. They will also begin streaming in all the data for their newly acquired shards from the peers that already have data for those shards. Once the nodes have finished streaming in the data for the shards that they have acquired, they will mark their status for those shards as  Available  in the placement and begin accepting writes. Simultaneously, the nodes that are losing ownership of any shards will mark their status for those shards as  Leaving . Once all the nodes accepting ownership of the new shards have finished streaming data from them, they will relinquish ownership of those shards and remove all the data associated with the shards they lost from memory and from disk.  M3Coordinator nodes will also pickup the new placement from etcd and alter which M3DB nodes they issue writes and reads to appropriately.",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/placement_configuration/#understanding-the-placement-configuration",
            "text": "The placement configuration contains a few core values that control how the placement behaves.",
            "title": "Understanding the Placement Configuration"
        },
        {
            "location": "/operational_guide/placement_configuration/#id",
            "text": "This is the identifier for a node in the placement and can be any value that uniquely identifies an M3DB node.",
            "title": "ID"
        },
        {
            "location": "/operational_guide/placement_configuration/#isolation-group",
            "text": "This value controls how nodes that own the same M3DB shards are isolated from each other. For example, in a single datacenter configuration this value could be set to the rack that the M3DB node lives on. As a result, the placement will guarantee that nodes that exist on the same rack do not share any shards, allowing the cluster to survive the failure of an entire rack. Alternatively, if M3DB was deployed in an AWS region, the isolation group could be set to the region's availability zone and that would ensure that the cluster would survive the loss of an entire availability zone.",
            "title": "Isolation Group"
        },
        {
            "location": "/operational_guide/placement_configuration/#zone",
            "text": "This value controls what etcd zone the M3DB node belongs to.",
            "title": "Zone"
        },
        {
            "location": "/operational_guide/placement_configuration/#weight",
            "text": "This value should be an integer and controls how the cluster will weigh the number of shards that an individual node will own. If you're running the M3DB cluster on homogenous hardware, then you probably want to assign all M3DB nodes the same weight so that shards are distributed evenly. On the otherhand, if you're running the cluster on heterogenous hardware, then this value should be higher for nodes with higher resources for whatever the limiting factor is in your cluster setup. For example, if disk space (as opposed to memory or CPU) is the limiting factor in how many shards any given node in your cluster can tolerate, then you could assign a higher value to nodes in your cluster that have larger disks and the placement calculations would assign them a higher number of shards.",
            "title": "Weight"
        },
        {
            "location": "/operational_guide/placement_configuration/#endpoint",
            "text": "This value should be in the form of  :  and identifies how network requests should be routed to this particular node in the placement.",
            "title": "Endpoint"
        },
        {
            "location": "/operational_guide/placement_configuration/#hostname",
            "text": "This value should be in the form of   and identifies the address / host name of the M3DB node.",
            "title": "Hostname"
        },
        {
            "location": "/operational_guide/placement_configuration/#port",
            "text": "This value should be in the form of   and identifies the port over which this M3DB node expects to receive traffic (defaults to 9000).",
            "title": "Port"
        },
        {
            "location": "/operational_guide/placement_configuration/#placement-operations",
            "text": "The instructions below all contain sample curl commands, but you can always review the API documentation by navigating to  http://<M3_COORDINATOR_HOST_NAME>:<CONFIGURED_PORT(default 7201)>/api/v1/openapi  or our  online API documentation .  Note : The  peers bootstrapper  must be configured on all nodes in the M3DB cluster for placement changes to work. The  peers  bootstrapper is enabled by default, so you only need to worry about this if you modified the default bootstrapping configuration",
            "title": "Placement Operations"
        },
        {
            "location": "/operational_guide/placement_configuration/#placement-initialization",
            "text": "Send a POST request to the  /api/v1/services/m3db/placement/init  endpoint  curl -X POST localhost:7201/api/v1/services/m3db/placement/init -d '{\n    \"num_shards\": <DESIRED_NUMBER_OF_SHARDS>,\n    \"replication_factor\": <DESIRED_REPLICATION_FACTOR>(recommended 3),\n    \"instances\": [\n        {\n            \"id\": \"<NODE_1_ID>\",\n            \"isolation_group\": \"<NODE_1_ISOLATION_GROUP>\",\n            \"zone\": \"<ETCD_ZONE>\",\n            \"weight\": <NODE_WEIGHT>,\n            \"endpoint\": \"<NODE_1_HOST_NAME>:<NODE_1_PORT>\",\n            \"hostname\": \"<NODE_1_HOST_NAME>\",\n            \"port\": <NODE_1_PORT>\n        },\n        {\n            \"id\": \"<NODE_2_ID>\",\n            \"isolation_group\": \"<NODE_2_ISOLATION_GROUP>\",\n            \"zone\": \"<ETCD_ZONE>\",\n            \"weight\": <NODE_WEIGHT>,\n            \"endpoint\": \"<NODE_2_HOST_NAME>:<NODE_2_PORT>\",\n            \"hostname\": \"<NODE_2_HOST_NAME>\",\n            \"port\": <NODE_2_PORT>\n        },\n        {\n            \"id\": \"<NODE_3_ID>\",\n            \"isolation_group\": \"<NODE_3_ISOLATION_GROUP>\",\n            \"zone\": \"<ETCD_ZONE>\",\n            \"weight\": <NODE_WEIGHT>,\n            \"endpoint\": \"<NODE_3_HOST_NAME>:<NODE_3_PORT>\",\n            \"hostname\": \"<NODE_3_HOST_NAME>\",\n            \"port\": <NODE_3_PORT>\n        },\n    ]\n}'",
            "title": "Placement Initialization"
        },
        {
            "location": "/operational_guide/placement_configuration/#adding-a-node",
            "text": "Send a POST request to the  /api/v1/services/m3db/placement  endpoint  curl -X POST <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT(default 7201)>/api/v1/services/m3db/placement -d '{\n  \"instances\": [\n    {\n      \"id\": \"<NEW_NODE_ID>\",\n      \"isolationGroup\": \"<NEW_NODE_ISOLATION_GROUP>\",\n      \"zone\": \"<ETCD_ZONE>\",\n      \"weight\": <NODE_WEIGHT>,\n      \"endpoint\": \"<NEW_NODE_HOST_NAME>:<NEW_NODE_PORT>(default 9000)\",\n      \"hostname\": \"<NEW_NODE_HOST_NAME>\",\n      \"port\": <NEW_NODE_PORT>\n    }\n  ]\n}'  After sending the add command you will need to wait for the M3DB cluster to reach the new desired state. You'll know that this has been achieved when the placement shows that all shards for all hosts are in the  Available  state.",
            "title": "Adding a Node"
        },
        {
            "location": "/operational_guide/placement_configuration/#removing-a-node",
            "text": "Send a DELETE request to the  /api/v1/services/m3db/placement/<NODE_ID>  endpoint.  curl -X DELETE <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT(default 7201)>/api/v1/services/m3db/placement/<NODE_ID>  After sending the delete command you will need to wait for the M3DB cluster to reach the new desired state. You'll know that this has been achieved when the placement shows that all shards for all hosts are in the  Available  state.",
            "title": "Removing a Node"
        },
        {
            "location": "/operational_guide/placement_configuration/#replacing-a-node",
            "text": "Send a POST request to the  /api/v1/services/m3db/placement/replace  endpoint containing hosts to replace and candidates to replace it with.  curl -X POST <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT(default 7201)>/api/v1/services/m3db/placement/replace -d '{\n    \"leavingInstanceIDs\": [\"<OLD_NODE_ID>\"],\n    \"candidates\": [\n        {\n          \"id\": \"<NEW_NODE_ID>\",\n          \"isolationGroup\": \"<NEW_NODE_ISOLATION_GROUP>\",\n          \"zone\": \"<ETCD_ZONE>\",\n          \"weight\": <NODE_WEIGHT>,\n          \"endpoint\": \"<NEW_NODE_HOST_NAME>:<NEW_NODE_PORT>(default 9000)\",\n          \"hostname\": \"<NEW_NODE_HOST_NAME>\",\n          \"port\": <NEW_NODE_PORT>\n        }\n    ]\n}'",
            "title": "Replacing a Node"
        },
        {
            "location": "/operational_guide/namespace_configuration/",
            "text": "Namespace Configuration\n\n\nIntroduction\n\n\nNamespaces in M3DB are analogous to tables in other databases. Each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize. For more information about namespaces, read our \nstorage engine documentation\n.\n\n\nNamespace Attributes\n\n\nbootstrapEnabled\n\n\nThis controls whether M3DB will attempt to \nbootstrap\n the namespace on startup. This value should always be set to \ntrue\n unless you have a very good reason to change it as setting it to \nfalse\n can cause data loss when restarting nodes.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nflushEnabled\n\n\nThis controls whether M3DB will periodically flush blocks to disk once they become immutable. This value should always be set to \ntrue\n unless you have a very good reason to change it as setting it to \nfalse\n will cause increased memory utilization and potential data loss when restarting nodes.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nwritesToCommitlog\n\n\nThis controls whether M3DB will includes writes to this namespace in the commitlog. This value should always be set to \ntrue\n unless you have a very good reason to change it as setting it to \nfalse\n will cause potential data loss when restarting nodes.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nsnapshotEnabled\n\n\nThis controls whether M3DB will periodically write out \nsnapshot files\n for this namespace which act as compacted commitlog files. This value should always be set to \ntrue\n unless you have a very good reason to change it as setting it to \nfalse\n will increasing bootstrapping times (reading commitlog files is slower than reading snapshot files) and increase disk utilization (snapshot files are compressed but commitlog files are uncompressed).\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nrepairEnabled\n\n\nIf enabled, the M3DB nodes will attempt to compare the data they own with the data of their peers and emit metrics about any discrepancies. This feature is experimental and we do not recommend enabling it under any circumstances.\n\n\nretentionOptions\n\n\nretentionPeriod\n\n\nThis controls the duration of time that M3DB will retain data for the namespace. For example, if this is set to 30 days, then data within this namespace will be available for querying up to 30 days after it is written. Note that this retention operates at the block level, not the write level, so its possible for individual datapoints to only be available for less than the specified retention. For example, if the blockSize was set to 24 hour and the retention was set to 30 days then a write that arrived at the very end of a 24 hour block would only be available for 29 days, but the node itself would always support querying the last 30 days worth of data.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nblockSize\n\n\nThis is the most important value to consider when tuning the performance of an M3DB namespace. Read the \nstorage engine documentation\n for more details, but the basic idea is that larger blockSizes will use more memory, but achieve higher compression. Similarly, smaller blockSizes will use less memory, but have worse compression.\n\n\nCan be modified without creating a new namespace: \nno\n\n\nbufferFuture and bufferPast\n\n\nThese values control how far into the future and the past (compared to the system time on an M3DB node) writes for the namespace will be accepted. For example, consider the following configuration:\n\n\nbufferPast: 10m\nbufferFuture: 20m\ncurrentSystemTime: 2:35:00PM\n\n\n\n\nNow consider the following writes (all of which arrive at 2:35:00PM system time, but include datapoints with the specified timestamps):\n\n\n2:25:00PM - Accepted, within the 10m bufferPast\n\n2:24:59PM - Rejected, outside the 10m bufferPast\n\n2:55:00PM - Accepted, within the 20m bufferFuture\n\n2:55:01PM - Rejected, outside the 20m bufferFuture\n\n\n\n\nWhile it may be tempting to configure \nbufferPast\n and \nbufferFuture\n to very large values to prevent writes from being rejected, this may cause performance issues. M3DB is a timeseries database that is optimized for realtime data. Out of order writes, as well as writes for times that are very far into the future or past are much more expensive and will cause additional CPU / memory pressure. In addition, M3DB cannot evict a block from memory until it is no longer mutable and large \nbufferPast\n and \nbufferFuture\n values effectively increase the amount of time that a block is mutable for which means that it must be kept in memory for a longer period of time.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nIndex Options\n\n\nTODO\n\n\nNamespace Operations\n\n\nThe operations below include sample CURLs, but you can always review the API documentation by navigating to\n\n\nhttp://<M3_COORDINATOR_HOST_NAME>:<CONFIGURED_PORT(default 7201)>/api/v1/openapi\n or our \nonline API documentation\n.\n\n\nAdding a Namespace\n\n\nAdding a namespace is a simple as using the \nPOST\n \napi/v1/namespace\n API on an M3Coordinator instance.\n\n\ncurl -X POST <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT(default 7201)>api/v1/namespace -d '{\n  \"name\": \"default_unaggregated\",\n  \"options\": {\n    \"bootstrapEnabled\": true,\n    \"flushEnabled\": true,\n    \"writesToCommitLog\": true,\n    \"cleanupEnabled\": true,\n    \"snapshotEnabled\": true,\n    \"repairEnabled\": false,\n    \"retentionOptions\": {\n      \"retentionPeriodDuration\": \"2d\",\n      \"blockSizeDuration\": \"2h\",\n      \"bufferFutureDuratiom\": \"10m\",\n      \"bufferPastDuration\": \"10m\",\n      \"blockDataExpiry\": true,\n      \"blockDataExpiryAfterNotAccessPeriodDuration\": \"5m\"\n    },\n    \"indexOptions\": {\n      \"enabled\": true,\n      \"blockSizeDuration\": \"4h\"\n    }\n  }\n}'\n\n\n\n\nAdding a namespace does not require restarting M3DB, but will require modifying the M3Coordinator configuration to include the new namespace, and then restarting it.\n\n\nDeleting a Namespace\n\n\nDeleting a namespace is a simple as using the \nDELETE\n \n/api/v1/namespace\n API on an M3Coordinator instance.\n\n\ncurl -X DELETE <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT(default 7201)>/api/v1/namespace/<NAMESPACE_NAME>\n\n\nNote that deleting a namespace will not have any effect on the M3DB nodes until they are all restarted. In addition, the namespace will need to be removed from the M3Coordinator configuration and then the M3Coordinator node will need to be restarted.\n\n\nModifying a Namespace\n\n\nThere is currently no atomic namespace modification endpoint. Instead, you will need to delete a namespace and then add it back again with the same name, but modified settings. Review the individual namespace settings above to determine whether or not a given setting is safe to modify. For example,it is never safe to modify the blockSize of a namespace.\n\n\nAlso, be very careful not to restart the M3DB nodes after deleting the namespace, but before adding it back. If you do this, the M3DB nodes may detect the existing data files on disk and delete them since they are not configured to retain that namespace.",
            "title": "Namespace Configuration"
        },
        {
            "location": "/operational_guide/namespace_configuration/#namespace-configuration",
            "text": "",
            "title": "Namespace Configuration"
        },
        {
            "location": "/operational_guide/namespace_configuration/#introduction",
            "text": "Namespaces in M3DB are analogous to tables in other databases. Each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize. For more information about namespaces, read our  storage engine documentation .",
            "title": "Introduction"
        },
        {
            "location": "/operational_guide/namespace_configuration/#namespace-attributes",
            "text": "",
            "title": "Namespace Attributes"
        },
        {
            "location": "/operational_guide/namespace_configuration/#bootstrapenabled",
            "text": "This controls whether M3DB will attempt to  bootstrap  the namespace on startup. This value should always be set to  true  unless you have a very good reason to change it as setting it to  false  can cause data loss when restarting nodes.  Can be modified without creating a new namespace:  yes",
            "title": "bootstrapEnabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#flushenabled",
            "text": "This controls whether M3DB will periodically flush blocks to disk once they become immutable. This value should always be set to  true  unless you have a very good reason to change it as setting it to  false  will cause increased memory utilization and potential data loss when restarting nodes.  Can be modified without creating a new namespace:  yes",
            "title": "flushEnabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#writestocommitlog",
            "text": "This controls whether M3DB will includes writes to this namespace in the commitlog. This value should always be set to  true  unless you have a very good reason to change it as setting it to  false  will cause potential data loss when restarting nodes.  Can be modified without creating a new namespace:  yes",
            "title": "writesToCommitlog"
        },
        {
            "location": "/operational_guide/namespace_configuration/#snapshotenabled",
            "text": "This controls whether M3DB will periodically write out  snapshot files  for this namespace which act as compacted commitlog files. This value should always be set to  true  unless you have a very good reason to change it as setting it to  false  will increasing bootstrapping times (reading commitlog files is slower than reading snapshot files) and increase disk utilization (snapshot files are compressed but commitlog files are uncompressed).  Can be modified without creating a new namespace:  yes",
            "title": "snapshotEnabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#repairenabled",
            "text": "If enabled, the M3DB nodes will attempt to compare the data they own with the data of their peers and emit metrics about any discrepancies. This feature is experimental and we do not recommend enabling it under any circumstances.",
            "title": "repairEnabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#retentionoptions",
            "text": "",
            "title": "retentionOptions"
        },
        {
            "location": "/operational_guide/namespace_configuration/#retentionperiod",
            "text": "This controls the duration of time that M3DB will retain data for the namespace. For example, if this is set to 30 days, then data within this namespace will be available for querying up to 30 days after it is written. Note that this retention operates at the block level, not the write level, so its possible for individual datapoints to only be available for less than the specified retention. For example, if the blockSize was set to 24 hour and the retention was set to 30 days then a write that arrived at the very end of a 24 hour block would only be available for 29 days, but the node itself would always support querying the last 30 days worth of data.  Can be modified without creating a new namespace:  yes",
            "title": "retentionPeriod"
        },
        {
            "location": "/operational_guide/namespace_configuration/#blocksize",
            "text": "This is the most important value to consider when tuning the performance of an M3DB namespace. Read the  storage engine documentation  for more details, but the basic idea is that larger blockSizes will use more memory, but achieve higher compression. Similarly, smaller blockSizes will use less memory, but have worse compression.  Can be modified without creating a new namespace:  no",
            "title": "blockSize"
        },
        {
            "location": "/operational_guide/namespace_configuration/#bufferfuture-and-bufferpast",
            "text": "These values control how far into the future and the past (compared to the system time on an M3DB node) writes for the namespace will be accepted. For example, consider the following configuration:  bufferPast: 10m\nbufferFuture: 20m\ncurrentSystemTime: 2:35:00PM  Now consider the following writes (all of which arrive at 2:35:00PM system time, but include datapoints with the specified timestamps):  2:25:00PM - Accepted, within the 10m bufferPast\n\n2:24:59PM - Rejected, outside the 10m bufferPast\n\n2:55:00PM - Accepted, within the 20m bufferFuture\n\n2:55:01PM - Rejected, outside the 20m bufferFuture  While it may be tempting to configure  bufferPast  and  bufferFuture  to very large values to prevent writes from being rejected, this may cause performance issues. M3DB is a timeseries database that is optimized for realtime data. Out of order writes, as well as writes for times that are very far into the future or past are much more expensive and will cause additional CPU / memory pressure. In addition, M3DB cannot evict a block from memory until it is no longer mutable and large  bufferPast  and  bufferFuture  values effectively increase the amount of time that a block is mutable for which means that it must be kept in memory for a longer period of time.  Can be modified without creating a new namespace:  yes",
            "title": "bufferFuture and bufferPast"
        },
        {
            "location": "/operational_guide/namespace_configuration/#index-options",
            "text": "TODO",
            "title": "Index Options"
        },
        {
            "location": "/operational_guide/namespace_configuration/#namespace-operations",
            "text": "The operations below include sample CURLs, but you can always review the API documentation by navigating to  http://<M3_COORDINATOR_HOST_NAME>:<CONFIGURED_PORT(default 7201)>/api/v1/openapi  or our  online API documentation .",
            "title": "Namespace Operations"
        },
        {
            "location": "/operational_guide/namespace_configuration/#adding-a-namespace",
            "text": "Adding a namespace is a simple as using the  POST   api/v1/namespace  API on an M3Coordinator instance.  curl -X POST <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT(default 7201)>api/v1/namespace -d '{\n  \"name\": \"default_unaggregated\",\n  \"options\": {\n    \"bootstrapEnabled\": true,\n    \"flushEnabled\": true,\n    \"writesToCommitLog\": true,\n    \"cleanupEnabled\": true,\n    \"snapshotEnabled\": true,\n    \"repairEnabled\": false,\n    \"retentionOptions\": {\n      \"retentionPeriodDuration\": \"2d\",\n      \"blockSizeDuration\": \"2h\",\n      \"bufferFutureDuratiom\": \"10m\",\n      \"bufferPastDuration\": \"10m\",\n      \"blockDataExpiry\": true,\n      \"blockDataExpiryAfterNotAccessPeriodDuration\": \"5m\"\n    },\n    \"indexOptions\": {\n      \"enabled\": true,\n      \"blockSizeDuration\": \"4h\"\n    }\n  }\n}'  Adding a namespace does not require restarting M3DB, but will require modifying the M3Coordinator configuration to include the new namespace, and then restarting it.",
            "title": "Adding a Namespace"
        },
        {
            "location": "/operational_guide/namespace_configuration/#deleting-a-namespace",
            "text": "Deleting a namespace is a simple as using the  DELETE   /api/v1/namespace  API on an M3Coordinator instance.  curl -X DELETE <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT(default 7201)>/api/v1/namespace/<NAMESPACE_NAME>  Note that deleting a namespace will not have any effect on the M3DB nodes until they are all restarted. In addition, the namespace will need to be removed from the M3Coordinator configuration and then the M3Coordinator node will need to be restarted.",
            "title": "Deleting a Namespace"
        },
        {
            "location": "/operational_guide/namespace_configuration/#modifying-a-namespace",
            "text": "There is currently no atomic namespace modification endpoint. Instead, you will need to delete a namespace and then add it back again with the same name, but modified settings. Review the individual namespace settings above to determine whether or not a given setting is safe to modify. For example,it is never safe to modify the blockSize of a namespace.  Also, be very careful not to restart the M3DB nodes after deleting the namespace, but before adding it back. If you do this, the M3DB nodes may detect the existing data files on disk and delete them since they are not configured to retain that namespace.",
            "title": "Modifying a Namespace"
        },
        {
            "location": "/operational_guide/bootstrapping/",
            "text": "Bootstrapping\n\n\nIntroduction\n\n\nWe recommend reading the \nplacement operational guide\n before reading the rest of this document.\n\n\nWhen an M3DB node is turned on (goes through a placement change) it needs to go through a bootstrapping process to determine the integrity of data that it has, replay writes from the commit log, and/or stream missing data from its peers. In most cases, as long as you're running with the default and recommended bootstrapper configuration of: \nfilesystem,commitlog,peers,uninitialized_topology\n then you should not need to worry about the bootstrapping process at all and M3DB will take care of doing the right thing such that you don't lose data and consistency guarantees are met. Note that the order of the configured bootstrappers \ndoes\n matter.\n\n\nGenerally speaking, we recommend that operators do not modify the bootstrappers configuration, but in the rare case that you to, this document is designed to help you understand the implications of doing so.\n\n\nM3DB currently supports 5 different bootstrappers:\n\n\n\n\nfilesystem\n\n\ncommitlog\n\n\npeers\n\n\nuninitialized_topology\n\n\nnoop_all\n\n\n\n\nWhen the bootstrapping process begins, M3DB nodes need to determine two things:\n\n\n\n\nWhat shards the bootstrapping node should bootstrap, which can be determined from the cluster placement.\n\n\nWhat time-ranges the bootstrapping node needs to bootstrap those shards for, which can be determined from the namespace retention.\n\n\n\n\nFor example, imagine a M3DB node that is responsible for shards 1, 5, 13, and 25 according to the cluster placement. In addition, it has a single namespace called \"metrics\" with a retention of 48 hours. When the M3DB node is started, the node will determine that it needs to bootstrap shards 1, 5, 13, and 25 for the time range starting at the current time and ending 48 hours ago. In order to obtain all this data, it will run the configured bootstrappers in the specified order. Every bootstrapper will notify the bootstrapping process of which shard/ranges it was able to bootstrap and the bootstrapping process will continue working its way through the list of bootstrappers until all the shards/ranges required have been marked as fulfilled. Otherwise the M3DB node will fail to start.\n\n\nBootstrappers\n\n\nFilesystem Bootstrapper\n\n\nThe \nfilesystem\n bootstrapper's responsibility is to determine which immutable \nFileset files\n exist on disk, and if so, mark them as fulfilled. The \nfilesystem\n bootstrapper achieves this by scanning M3DB's directory structure and determining which Fileset files exist on disk. Unlike the other bootstrappers, the \nfilesystem\n bootstrapper does not need to load any data into memory, it simply verifies the checksums of the data on disk and other components of the M3DB node will handle reading (and caching) the data dynamically once it begins to serve reads.\n\n\nCommitlog Bootstrapper\n\n\nThe \ncommitlog\n bootstrapper's responsibility is to read the commitlog and snapshot (compacted commitlogs) files on disk and recover any data that has not yet been written out as an immutable Fileset file. Unlike the \nfilesystem\n bootstrapper, the commit log bootstrapper cannot simply check which files are on disk in order to determine if it can satisfy a bootstrap request. Instead, the \ncommitlog\n bootstrapper determines whether it can satisfy a bootstrap request using a simple heuristic.\n\n\nOn a shard-by-shard basis, the \ncommitlog\n bootstrapper will consult the cluster placement to see if the node it is running on has ever achieved the \nAvailable\n status for the specified shard. If so, then the commit log bootstrapper should have all the data since the last Fileset file was flushed and will return that it can satisfy any time range for that shard. In other words, the commit log bootstrapper is all-or-nothing for a given shard: it will either return that it can satisfy any time range for a given shard or none at all. In addition, the \ncommitlog\n bootstrapper \nassumes\n it is running after the \nfilesystem\n bootstrapper. M3DB will not allow you to run with a configuration where the \nfilesystem\n bootstrapper is placed after the \ncommitlog\n bootstrapper, but it will allow you to run the \ncommitlog\n bootstrapper without the \nfilesystem\n bootstrapper which can result in loss of data, depending on the workload.\n\n\nPeers Bootstrapper\n\n\nThe \npeers\n bootstrapper's responsibility is to stream in data for shard/ranges from other M3DB nodes (peers) in the cluster. This bootstrapper is only useful in M3DB clusters with more than a single node \nand\n where the replication factor is set to a value larger than 1. The \npeers\n bootstrapper will determine whether or not it can satisfy a bootstrap request on a shard-by-shard basis by consulting the cluster placement and determining if there are enough peers to satisfy the bootstrap request. For example, imagine the following M3DB placement where node A is trying to perform a peer bootstrap:\n\n\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502  Shard 1: Initializing  \u2502   \u2502 Shard 1: Initializing \u2502   \u2502  Shard 1: Available  \u2502\n\u2502  Shard 2: Initializing  \u2502   \u2502 Shard 2: Initializing \u2502   \u2502  Shard 2: Available  \u2502\n\u2502  Shard 3: Initializing  \u2502   \u2502 Shard 3: Initializing \u2502   \u2502  Shard 3: Available  \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn this case, the \npeers\n bootstrapper running on node A will not be able to fullfill any requests because node B is in the \nInitializing\n state for all of its shards and cannot fulfill bootstrap requests. This means that node A's \npeers\n bootstrapper cannot meet its default consistency level of majority for bootstrapping (1 < 2 which is majority with a replication factor of 3). On the other hand, node A would be able to peer bootstrap its shards in the following placement because its peers (nodes B/C) have sufficient replicas of the shards it needs in the \nAvailable\n state:\n\n\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502  Shard 1: Initializing  \u2502   \u2502 Shard 1: Available    \u2502   \u2502  Shard 1: Available  \u2502\n\u2502  Shard 2: Initializing  \u2502   \u2502 Shard 2: Available    \u2502   \u2502  Shard 2: Available  \u2502\n\u2502  Shard 3: Initializing  \u2502   \u2502 Shard 3: Available    \u2502   \u2502  Shard 3: Available  \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nNote that a bootstrap consistency level of \nmajority\n is the default value, but can be modified by changing the value of the key \nm3db.client.bootstrap-consistency-level\n in \netcd\n to one of: \nnone\n, \none\n, \nunstrict_majority\n (attempt to read from majority, but settle for less if any errors occur), \nmajority\n (strict majority), and \nall\n. For example, if an entire cluster with a replication factor of 3 was restarted simultaneously, all the nodes would get stuck in an infinite loop trying to peer bootstrap from each other and not achieving majority until an operator modified this value. Note that this can happen even if all the shards were in the \nAvailable\n state because M3DB nodes will reject all read requests for a shard until they have bootstrapped that shard (which has to happen everytime the node is restarted).\n\n\nNote\n: Any bootstrappers configuration that does not include the \npeers\n bootstrapper will be unable to handle dynamic placement changes of any kind.\n\n\nUninitialized Topology Bootstrapper\n\n\nThe purpose of the \nuninitialized_topology\n bootstrapper is to succeed bootstraps for all time ranges for shards that have never been completely bootstrapped (at a cluster level). This allows us to run the default bootstrapper configuration of: \nfilesystem,commitlog,peers,topology_uninitialized\n such that the \nfilesystem\n and \ncommitlog\n bootstrappers are used by default in node restarts, the \npeers\n bootstrapper is used for node adds/removes/replaces, and bootstraps still succeed for brand new placement where both the \ncommitlog\n and \npeers\n bootstrappers will be unable to succeed any bootstraps. In other words, the \nuninitialized_topology\n bootstrapper allows us to place the \ncommitlog\n bootstrapper \nbefore\n the \npeers\n bootstrapper and still succeed bootstraps with brand new placements without resorting to using the noop-all bootstrapper which suceeds bootstraps for all shard/time-ranges regardless of the status of the placement.\n\n\nThe \nuninitialized_topology\n bootstrapper determines whether a placement is \"new\" for a given shard by counting the number of nodes in the \nInitializing\n state and \nLeaving\n states and there are more \nInitializing\n than \nLeaving\n, then it succeeds the bootstrap because that means the placement has never reached a state where all nodes are \nAvailable\n.\n\n\nNo Operational All Bootstrapper\n\n\nThe \nnoop_all\n bootstrapper succeeds all bootstraps regardless of requests shards/time ranges.\n\n\nBootstrappers Configuration\n\n\nNow that we've gone over the various bootstrappers, let's consider how M3DB will behave in different configurations. Note that we include \nuninitialized_topology\n at the end of all the lists of bootstrappers because its required to get a new placement up and running in the first place, but is not required after that (although leaving it in has no detrimental effects). Also note that any configuration that does not include the \npeers\n bootstrapper will not be able to handle dynamic placement changes like node adds/removes/replaces.\n\n\nfilesystem,commitlog,peers,uninitialized_topology (default)\n\n\nThis is the default bootstrappers configuration for M3DB and will behave \"as expected\" in the sense that it will maintain M3DB's consistency guarantees at all times, handle node adds/replaces/removes correctly, and still work with brand new placements / topologies. \nThis is the only configuration that we recommend using in production\n.\n\n\nIn the general case, the node will use only the \nfilesystem\n and \ncommitlog\n bootstrappers on node startup. However, in the case of a node add/remove/replace, the \ncommitlog\n bootstrapper will detect that it is unable to fulfill the bootstrap request (because the node has never reached the \nAvailable\n state) and defer to the \npeers\n bootstrapper to stream in the data.\n\n\nAdditionally, if it is a brand new placement where even the \npeers\n bootstrapper cannot fulfill the bootstrap, this will be detected by the \nuninitialized_topology\n bootstrapper which will succeed the bootstrap.\n\n\nfilesystem,peers,uninitialized_topology (default)\n\n\nEverytime a node is restarted it will attempt to stream in all of the the data for any blocks that it has never flushed, which is generally the currently active block and possibly the previous block as well. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of any unflushed blocks. This mode can lead to violations of M3DB's consistency guarantees due to the fact that commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see \npeers\n bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.\n\n\npeers,uninitialized_topology\n\n\nEvery time a node is restarted, it will attempt to stream in \nall\n of the data that it is responsible for from its peers, completely ignoring the immutable Fileset files it already has on disk. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of all data on an individual node. This mode can lead to violations of M3DB's consistency guarantees due to the fact that the commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see \npeers\n bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.\n\n\nInvalid bootstrappers configuration\n\n\nFor the sake of completeness, we've included a short discussion below of some bootstrapping configurations that we consider \"invalid\" in that they are likely to lose data / violate M3DB's consistency guarantees and/or not handle placement changes in a correct way.\n\n\nfilesystem,commitlog,uninitialized_topology\n\n\nThis bootstrapping configuration will work just fine if nodes are never added/replaced/removed, but will fail when attempting a node add/replace/remove.\n\n\nfilesystem,uninitialized_topology\n\n\nEvery time a node is restarted it will utilize the immutable Fileset files its already written out to disk, but any data that it had received since it wrote out the last set of immutable files will be lost.\n\n\ncommitlog,uninitialized_topology\n\n\nEvery time a node is restarted it will read all the commit log and snapshot files it has on disk, but it will ignore all the data in the immutable Fileset files that it has already written.",
            "title": "Bootstrapping"
        },
        {
            "location": "/operational_guide/bootstrapping/#bootstrapping",
            "text": "",
            "title": "Bootstrapping"
        },
        {
            "location": "/operational_guide/bootstrapping/#introduction",
            "text": "We recommend reading the  placement operational guide  before reading the rest of this document.  When an M3DB node is turned on (goes through a placement change) it needs to go through a bootstrapping process to determine the integrity of data that it has, replay writes from the commit log, and/or stream missing data from its peers. In most cases, as long as you're running with the default and recommended bootstrapper configuration of:  filesystem,commitlog,peers,uninitialized_topology  then you should not need to worry about the bootstrapping process at all and M3DB will take care of doing the right thing such that you don't lose data and consistency guarantees are met. Note that the order of the configured bootstrappers  does  matter.  Generally speaking, we recommend that operators do not modify the bootstrappers configuration, but in the rare case that you to, this document is designed to help you understand the implications of doing so.  M3DB currently supports 5 different bootstrappers:   filesystem  commitlog  peers  uninitialized_topology  noop_all   When the bootstrapping process begins, M3DB nodes need to determine two things:   What shards the bootstrapping node should bootstrap, which can be determined from the cluster placement.  What time-ranges the bootstrapping node needs to bootstrap those shards for, which can be determined from the namespace retention.   For example, imagine a M3DB node that is responsible for shards 1, 5, 13, and 25 according to the cluster placement. In addition, it has a single namespace called \"metrics\" with a retention of 48 hours. When the M3DB node is started, the node will determine that it needs to bootstrap shards 1, 5, 13, and 25 for the time range starting at the current time and ending 48 hours ago. In order to obtain all this data, it will run the configured bootstrappers in the specified order. Every bootstrapper will notify the bootstrapping process of which shard/ranges it was able to bootstrap and the bootstrapping process will continue working its way through the list of bootstrappers until all the shards/ranges required have been marked as fulfilled. Otherwise the M3DB node will fail to start.",
            "title": "Introduction"
        },
        {
            "location": "/operational_guide/bootstrapping/#bootstrappers",
            "text": "",
            "title": "Bootstrappers"
        },
        {
            "location": "/operational_guide/bootstrapping/#filesystem-bootstrapper",
            "text": "The  filesystem  bootstrapper's responsibility is to determine which immutable  Fileset files  exist on disk, and if so, mark them as fulfilled. The  filesystem  bootstrapper achieves this by scanning M3DB's directory structure and determining which Fileset files exist on disk. Unlike the other bootstrappers, the  filesystem  bootstrapper does not need to load any data into memory, it simply verifies the checksums of the data on disk and other components of the M3DB node will handle reading (and caching) the data dynamically once it begins to serve reads.",
            "title": "Filesystem Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping/#commitlog-bootstrapper",
            "text": "The  commitlog  bootstrapper's responsibility is to read the commitlog and snapshot (compacted commitlogs) files on disk and recover any data that has not yet been written out as an immutable Fileset file. Unlike the  filesystem  bootstrapper, the commit log bootstrapper cannot simply check which files are on disk in order to determine if it can satisfy a bootstrap request. Instead, the  commitlog  bootstrapper determines whether it can satisfy a bootstrap request using a simple heuristic.  On a shard-by-shard basis, the  commitlog  bootstrapper will consult the cluster placement to see if the node it is running on has ever achieved the  Available  status for the specified shard. If so, then the commit log bootstrapper should have all the data since the last Fileset file was flushed and will return that it can satisfy any time range for that shard. In other words, the commit log bootstrapper is all-or-nothing for a given shard: it will either return that it can satisfy any time range for a given shard or none at all. In addition, the  commitlog  bootstrapper  assumes  it is running after the  filesystem  bootstrapper. M3DB will not allow you to run with a configuration where the  filesystem  bootstrapper is placed after the  commitlog  bootstrapper, but it will allow you to run the  commitlog  bootstrapper without the  filesystem  bootstrapper which can result in loss of data, depending on the workload.",
            "title": "Commitlog Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping/#peers-bootstrapper",
            "text": "The  peers  bootstrapper's responsibility is to stream in data for shard/ranges from other M3DB nodes (peers) in the cluster. This bootstrapper is only useful in M3DB clusters with more than a single node  and  where the replication factor is set to a value larger than 1. The  peers  bootstrapper will determine whether or not it can satisfy a bootstrap request on a shard-by-shard basis by consulting the cluster placement and determining if there are enough peers to satisfy the bootstrap request. For example, imagine the following M3DB placement where node A is trying to perform a peer bootstrap:      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502  Shard 1: Initializing  \u2502   \u2502 Shard 1: Initializing \u2502   \u2502  Shard 1: Available  \u2502\n\u2502  Shard 2: Initializing  \u2502   \u2502 Shard 2: Initializing \u2502   \u2502  Shard 2: Available  \u2502\n\u2502  Shard 3: Initializing  \u2502   \u2502 Shard 3: Initializing \u2502   \u2502  Shard 3: Available  \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In this case, the  peers  bootstrapper running on node A will not be able to fullfill any requests because node B is in the  Initializing  state for all of its shards and cannot fulfill bootstrap requests. This means that node A's  peers  bootstrapper cannot meet its default consistency level of majority for bootstrapping (1 < 2 which is majority with a replication factor of 3). On the other hand, node A would be able to peer bootstrap its shards in the following placement because its peers (nodes B/C) have sufficient replicas of the shards it needs in the  Available  state:      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502  Shard 1: Initializing  \u2502   \u2502 Shard 1: Available    \u2502   \u2502  Shard 1: Available  \u2502\n\u2502  Shard 2: Initializing  \u2502   \u2502 Shard 2: Available    \u2502   \u2502  Shard 2: Available  \u2502\n\u2502  Shard 3: Initializing  \u2502   \u2502 Shard 3: Available    \u2502   \u2502  Shard 3: Available  \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  Note that a bootstrap consistency level of  majority  is the default value, but can be modified by changing the value of the key  m3db.client.bootstrap-consistency-level  in  etcd  to one of:  none ,  one ,  unstrict_majority  (attempt to read from majority, but settle for less if any errors occur),  majority  (strict majority), and  all . For example, if an entire cluster with a replication factor of 3 was restarted simultaneously, all the nodes would get stuck in an infinite loop trying to peer bootstrap from each other and not achieving majority until an operator modified this value. Note that this can happen even if all the shards were in the  Available  state because M3DB nodes will reject all read requests for a shard until they have bootstrapped that shard (which has to happen everytime the node is restarted).  Note : Any bootstrappers configuration that does not include the  peers  bootstrapper will be unable to handle dynamic placement changes of any kind.",
            "title": "Peers Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping/#uninitialized-topology-bootstrapper",
            "text": "The purpose of the  uninitialized_topology  bootstrapper is to succeed bootstraps for all time ranges for shards that have never been completely bootstrapped (at a cluster level). This allows us to run the default bootstrapper configuration of:  filesystem,commitlog,peers,topology_uninitialized  such that the  filesystem  and  commitlog  bootstrappers are used by default in node restarts, the  peers  bootstrapper is used for node adds/removes/replaces, and bootstraps still succeed for brand new placement where both the  commitlog  and  peers  bootstrappers will be unable to succeed any bootstraps. In other words, the  uninitialized_topology  bootstrapper allows us to place the  commitlog  bootstrapper  before  the  peers  bootstrapper and still succeed bootstraps with brand new placements without resorting to using the noop-all bootstrapper which suceeds bootstraps for all shard/time-ranges regardless of the status of the placement.  The  uninitialized_topology  bootstrapper determines whether a placement is \"new\" for a given shard by counting the number of nodes in the  Initializing  state and  Leaving  states and there are more  Initializing  than  Leaving , then it succeeds the bootstrap because that means the placement has never reached a state where all nodes are  Available .",
            "title": "Uninitialized Topology Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping/#no-operational-all-bootstrapper",
            "text": "The  noop_all  bootstrapper succeeds all bootstraps regardless of requests shards/time ranges.",
            "title": "No Operational All Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping/#bootstrappers-configuration",
            "text": "Now that we've gone over the various bootstrappers, let's consider how M3DB will behave in different configurations. Note that we include  uninitialized_topology  at the end of all the lists of bootstrappers because its required to get a new placement up and running in the first place, but is not required after that (although leaving it in has no detrimental effects). Also note that any configuration that does not include the  peers  bootstrapper will not be able to handle dynamic placement changes like node adds/removes/replaces.",
            "title": "Bootstrappers Configuration"
        },
        {
            "location": "/operational_guide/bootstrapping/#filesystemcommitlogpeersuninitialized_topology-default",
            "text": "This is the default bootstrappers configuration for M3DB and will behave \"as expected\" in the sense that it will maintain M3DB's consistency guarantees at all times, handle node adds/replaces/removes correctly, and still work with brand new placements / topologies.  This is the only configuration that we recommend using in production .  In the general case, the node will use only the  filesystem  and  commitlog  bootstrappers on node startup. However, in the case of a node add/remove/replace, the  commitlog  bootstrapper will detect that it is unable to fulfill the bootstrap request (because the node has never reached the  Available  state) and defer to the  peers  bootstrapper to stream in the data.  Additionally, if it is a brand new placement where even the  peers  bootstrapper cannot fulfill the bootstrap, this will be detected by the  uninitialized_topology  bootstrapper which will succeed the bootstrap.",
            "title": "filesystem,commitlog,peers,uninitialized_topology (default)"
        },
        {
            "location": "/operational_guide/bootstrapping/#filesystempeersuninitialized_topology-default",
            "text": "Everytime a node is restarted it will attempt to stream in all of the the data for any blocks that it has never flushed, which is generally the currently active block and possibly the previous block as well. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of any unflushed blocks. This mode can lead to violations of M3DB's consistency guarantees due to the fact that commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see  peers  bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.",
            "title": "filesystem,peers,uninitialized_topology (default)"
        },
        {
            "location": "/operational_guide/bootstrapping/#peersuninitialized_topology",
            "text": "Every time a node is restarted, it will attempt to stream in  all  of the data that it is responsible for from its peers, completely ignoring the immutable Fileset files it already has on disk. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of all data on an individual node. This mode can lead to violations of M3DB's consistency guarantees due to the fact that the commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see  peers  bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.",
            "title": "peers,uninitialized_topology"
        },
        {
            "location": "/operational_guide/bootstrapping/#invalid-bootstrappers-configuration",
            "text": "For the sake of completeness, we've included a short discussion below of some bootstrapping configurations that we consider \"invalid\" in that they are likely to lose data / violate M3DB's consistency guarantees and/or not handle placement changes in a correct way.",
            "title": "Invalid bootstrappers configuration"
        },
        {
            "location": "/operational_guide/bootstrapping/#filesystemcommitloguninitialized_topology",
            "text": "This bootstrapping configuration will work just fine if nodes are never added/replaced/removed, but will fail when attempting a node add/replace/remove.",
            "title": "filesystem,commitlog,uninitialized_topology"
        },
        {
            "location": "/operational_guide/bootstrapping/#filesystemuninitialized_topology",
            "text": "Every time a node is restarted it will utilize the immutable Fileset files its already written out to disk, but any data that it had received since it wrote out the last set of immutable files will be lost.",
            "title": "filesystem,uninitialized_topology"
        },
        {
            "location": "/operational_guide/bootstrapping/#commitloguninitialized_topology",
            "text": "Every time a node is restarted it will read all the commit log and snapshot files it has on disk, but it will ignore all the data in the immutable Fileset files that it has already written.",
            "title": "commitlog,uninitialized_topology"
        },
        {
            "location": "/operational_guide/kernel_configuration/",
            "text": "Kernel Configuration\n\n\nThis document lists the Kernel tweaks M3DB needs to run well.\n\n\nvm.max_map_count\n\n\nM3DB uses a lot of mmap-ed files for performance, as a result, you might need to bump \nvm.max_map_count\n. We suggest setting this value to \n262144\n, so you don\u2019t have to come back and debug issues later.\n\n\nOn Linux, you can increase the limits by running the following command as root:\n\n\nsysctl -w vm.max_map_count=262144\n\n\n\n\nTo set this value permanently, update the \nvm.max_map_count\n setting in \n/etc/sysctl.conf\n.\n\n\nvm.swappiness\n\n\nvm.swappiness\n controls how much the virtual memory subsystem will try to swap to disk. By default, the kernel configures this value to \n60\n, and will try to swap out items in memory even when there is plenty of RAM available to the system.\n\n\nWe recommend sizing clusters such that M3DB is running on a substrate (hosts/containers) such that no-swapping is necessary, i.e. the process is only using 30-50% of the maximum available memory. And therefore recommend setting the value of \nvm.swappiness\n to \n1\n. This tells the kernel to swap as little as possible, without altogether disabling swapping.\n\n\nOn Linux, you can configure this by running the following as root:\n\n\nsysctl -w vm.swappiness=1\n\n\n\n\nTo set this value permanently, update the \nvm.swappiness\n setting in \n/etc/sysctl.conf\n.\n\n\nrlimits\n\n\nM3DB also can use a high number of files and we suggest setting a high max open number of files due to per partition fileset volumes.\n\n\nOn Linux you can set a high limit for maximum number of open files in \n/etc/security/limits.conf\n:\n\n\nyour_m3db_user        hard nofile 500000\nyour_m3db_user        soft nofile 500000\n\n\n\n\nAlternatively, if you wish to have M3DB run under \nsystemd\n you can use our \nservice example\n which will set sane defaults.\n\n\nBefore running the process make sure the limits are set, if running manually you can raise the limit for the current user with \nulimit -n 500000\n.",
            "title": "Kernel Configuration"
        },
        {
            "location": "/operational_guide/kernel_configuration/#kernel-configuration",
            "text": "This document lists the Kernel tweaks M3DB needs to run well.",
            "title": "Kernel Configuration"
        },
        {
            "location": "/operational_guide/kernel_configuration/#vmmax_map_count",
            "text": "M3DB uses a lot of mmap-ed files for performance, as a result, you might need to bump  vm.max_map_count . We suggest setting this value to  262144 , so you don\u2019t have to come back and debug issues later.  On Linux, you can increase the limits by running the following command as root:  sysctl -w vm.max_map_count=262144  To set this value permanently, update the  vm.max_map_count  setting in  /etc/sysctl.conf .",
            "title": "vm.max_map_count"
        },
        {
            "location": "/operational_guide/kernel_configuration/#vmswappiness",
            "text": "vm.swappiness  controls how much the virtual memory subsystem will try to swap to disk. By default, the kernel configures this value to  60 , and will try to swap out items in memory even when there is plenty of RAM available to the system.  We recommend sizing clusters such that M3DB is running on a substrate (hosts/containers) such that no-swapping is necessary, i.e. the process is only using 30-50% of the maximum available memory. And therefore recommend setting the value of  vm.swappiness  to  1 . This tells the kernel to swap as little as possible, without altogether disabling swapping.  On Linux, you can configure this by running the following as root:  sysctl -w vm.swappiness=1  To set this value permanently, update the  vm.swappiness  setting in  /etc/sysctl.conf .",
            "title": "vm.swappiness"
        },
        {
            "location": "/operational_guide/kernel_configuration/#rlimits",
            "text": "M3DB also can use a high number of files and we suggest setting a high max open number of files due to per partition fileset volumes.  On Linux you can set a high limit for maximum number of open files in  /etc/security/limits.conf :  your_m3db_user        hard nofile 500000\nyour_m3db_user        soft nofile 500000  Alternatively, if you wish to have M3DB run under  systemd  you can use our  service example  which will set sane defaults.  Before running the process make sure the limits are set, if running manually you can raise the limit for the current user with  ulimit -n 500000 .",
            "title": "rlimits"
        },
        {
            "location": "/integrations/prometheus/",
            "text": "Prometheus\n\n\nThis document is a getting started guide to integrating M3DB with Prometheus.\n\n\nM3 Coordinator configuration\n\n\nTo write to a remote M3DB cluster the simplest configuration is to run \nm3coordinator\n as a sidecar alongside Prometheus.\n\n\nStart by downloading the \nconfig template\n. Update the \nnamespaces\n and the \nclient\n section for a new cluster to match your cluster's configuration.\n\n\nYou'll need to specify the static IPs or hostnames of your M3DB seed nodes, and the name and retention values of the namespace you set up.  You can leave the namespace storage metrics type as \nunaggregated\n since it's required by default to have a cluster that receives all Prometheus metrics unaggregated.  In the future you might also want to aggregate and downsample metrics for longer retention, and you can come back and update the config once you've setup those clusters.\n\n\nIt should look something like:\n\n\nlistenAddress:\n  type: \"config\"\n  value: \"0.0.0.0:7201\"\n\nmetrics:\n  scope:\n    prefix: \"coordinator\"\n  prometheus:\n    handlerPath: /metrics\n    listenAddress: 0.0.0.0:7203 # until https://github.com/m3db/m3/issues/682 is resolved\n  sanitization: prometheus\n  samplingRate: 1.0\n  extended: none\n\nclusters:\n   - namespaces:\n# We created a namespace called \"default\" and had set it to retention \"48h\".\n       - namespace: default\n         retention: 48h\n         type: unaggregated\n     client:\n       config:\n         service:\n           env: default_env\n           zone: embedded\n           service: m3db\n           cacheDir: /var/lib/m3kv\n           etcdClusters:\n             - zone: embedded\n               endpoints:\n# We have five M3DB nodes but only three are seed nodes, they are listed here.\n                 - M3DB_NODE_01_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_02_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_03_STATIC_IP_ADDRESS:2379\n       writeConsistencyLevel: majority\n       readConsistencyLevel: unstrict_majority\n       writeTimeout: 10s\n       fetchTimeout: 15s\n       connectTimeout: 20s\n       writeRetry:\n         initialBackoff: 500ms\n         backoffFactor: 3\n         maxRetries: 2\n         jitter: true\n       fetchRetry:\n         initialBackoff: 500ms\n         backoffFactor: 2\n         maxRetries: 3\n         jitter: true\n       backgroundHealthCheckFailLimit: 4\n       backgroundHealthCheckFailThrottleFactor: 0.5\n\n\n\n\n\nNow start the process up:\n\n\nm3coordinator -f <config-name.yml>\n\n\n\n\nOr, use the docker container:\n\n\ndocker pull quay.io/m3/m3coordinator:latest\ndocker run -p 7201:7201 --name m3coordinator -v <config-name.yml>:/etc/m3coordinator/m3coordinator.yml quay.io/m3/m3coordinator:latest\n\n\n\n\nPrometheus configuration\n\n\nAdd to your Prometheus configuration the \nm3coordinator\n sidecar remote read/write endpoints, something like:\n\n\nremote_read:\n  - url: \"http://localhost:7201/api/v1/prom/remote/read\"\n    # To test reading even when local Prometheus has the data\n    read_recent: true\nremote_write:\n  - url: \"http://localhost:7201/api/v1/prom/remote/write\"",
            "title": "Prometheus"
        },
        {
            "location": "/integrations/prometheus/#prometheus",
            "text": "This document is a getting started guide to integrating M3DB with Prometheus.",
            "title": "Prometheus"
        },
        {
            "location": "/integrations/prometheus/#m3-coordinator-configuration",
            "text": "To write to a remote M3DB cluster the simplest configuration is to run  m3coordinator  as a sidecar alongside Prometheus.  Start by downloading the  config template . Update the  namespaces  and the  client  section for a new cluster to match your cluster's configuration.  You'll need to specify the static IPs or hostnames of your M3DB seed nodes, and the name and retention values of the namespace you set up.  You can leave the namespace storage metrics type as  unaggregated  since it's required by default to have a cluster that receives all Prometheus metrics unaggregated.  In the future you might also want to aggregate and downsample metrics for longer retention, and you can come back and update the config once you've setup those clusters.  It should look something like:  listenAddress:\n  type: \"config\"\n  value: \"0.0.0.0:7201\"\n\nmetrics:\n  scope:\n    prefix: \"coordinator\"\n  prometheus:\n    handlerPath: /metrics\n    listenAddress: 0.0.0.0:7203 # until https://github.com/m3db/m3/issues/682 is resolved\n  sanitization: prometheus\n  samplingRate: 1.0\n  extended: none\n\nclusters:\n   - namespaces:\n# We created a namespace called \"default\" and had set it to retention \"48h\".\n       - namespace: default\n         retention: 48h\n         type: unaggregated\n     client:\n       config:\n         service:\n           env: default_env\n           zone: embedded\n           service: m3db\n           cacheDir: /var/lib/m3kv\n           etcdClusters:\n             - zone: embedded\n               endpoints:\n# We have five M3DB nodes but only three are seed nodes, they are listed here.\n                 - M3DB_NODE_01_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_02_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_03_STATIC_IP_ADDRESS:2379\n       writeConsistencyLevel: majority\n       readConsistencyLevel: unstrict_majority\n       writeTimeout: 10s\n       fetchTimeout: 15s\n       connectTimeout: 20s\n       writeRetry:\n         initialBackoff: 500ms\n         backoffFactor: 3\n         maxRetries: 2\n         jitter: true\n       fetchRetry:\n         initialBackoff: 500ms\n         backoffFactor: 2\n         maxRetries: 3\n         jitter: true\n       backgroundHealthCheckFailLimit: 4\n       backgroundHealthCheckFailThrottleFactor: 0.5  Now start the process up:  m3coordinator -f <config-name.yml>  Or, use the docker container:  docker pull quay.io/m3/m3coordinator:latest\ndocker run -p 7201:7201 --name m3coordinator -v <config-name.yml>:/etc/m3coordinator/m3coordinator.yml quay.io/m3/m3coordinator:latest",
            "title": "M3 Coordinator configuration"
        },
        {
            "location": "/integrations/prometheus/#prometheus-configuration",
            "text": "Add to your Prometheus configuration the  m3coordinator  sidecar remote read/write endpoints, something like:  remote_read:\n  - url: \"http://localhost:7201/api/v1/prom/remote/read\"\n    # To test reading even when local Prometheus has the data\n    read_recent: true\nremote_write:\n  - url: \"http://localhost:7201/api/v1/prom/remote/write\"",
            "title": "Prometheus configuration"
        },
        {
            "location": "/troubleshooting/",
            "text": "Troubleshooting\n\n\nSome common problems and resolutions\n\n\nPorts 9001-9004 aren't open after starting m3db.\n\n\nThese ports will not open until a namespace and placement have been created and the nodes have bootstrapped.\n\n\nBootstrapping is slow\n\n\nDouble check your configuration against the \nbootstrapping guide\n. The nodes will log what bootstrapper they are using and what time range they are using it for.\n\n\nIf you're using the commitlog bootstrapper, and it seems to be slow, ensure that snapshotting is enabled for your namespace. Enabling snapshotting will require a node restart to take effect.\n\n\nIf an m3db node hasn't been able to snapshot for awhile, or is stuck in the commitlog bootstrapping phase for a long time due to accumulating a large number of commitlogs, consider using the peers bootstrapper. In situations where a large number of commitlogs need to be read, the peers bootstrapper will outperform the commitlog bootstrapper (faster and less memory usage) due to the fact that it will receive already-compressed data from its peers. Keep in mind that this will only work with a replication factor of 3 or larger and if the nodes peers are healthy and bootstrapped. Review the \nbootstrapping guide\n for more information.\n\n\nNodes a crashing with memory allocation errors, but there's plenty of available memory\n\n\nEnsure you've set \nvm.max_map_count\n to something like 262,144 using sysctl. Find out more in the \nClustering the Hard Way\n document.",
            "title": "Troubleshooting"
        },
        {
            "location": "/troubleshooting/#troubleshooting",
            "text": "Some common problems and resolutions",
            "title": "Troubleshooting"
        },
        {
            "location": "/troubleshooting/#ports-9001-9004-arent-open-after-starting-m3db",
            "text": "These ports will not open until a namespace and placement have been created and the nodes have bootstrapped.",
            "title": "Ports 9001-9004 aren't open after starting m3db."
        },
        {
            "location": "/troubleshooting/#bootstrapping-is-slow",
            "text": "Double check your configuration against the  bootstrapping guide . The nodes will log what bootstrapper they are using and what time range they are using it for.  If you're using the commitlog bootstrapper, and it seems to be slow, ensure that snapshotting is enabled for your namespace. Enabling snapshotting will require a node restart to take effect.  If an m3db node hasn't been able to snapshot for awhile, or is stuck in the commitlog bootstrapping phase for a long time due to accumulating a large number of commitlogs, consider using the peers bootstrapper. In situations where a large number of commitlogs need to be read, the peers bootstrapper will outperform the commitlog bootstrapper (faster and less memory usage) due to the fact that it will receive already-compressed data from its peers. Keep in mind that this will only work with a replication factor of 3 or larger and if the nodes peers are healthy and bootstrapped. Review the  bootstrapping guide  for more information.",
            "title": "Bootstrapping is slow"
        },
        {
            "location": "/troubleshooting/#nodes-a-crashing-with-memory-allocation-errors-but-theres-plenty-of-available-memory",
            "text": "Ensure you've set  vm.max_map_count  to something like 262,144 using sysctl. Find out more in the  Clustering the Hard Way  document.",
            "title": "Nodes a crashing with memory allocation errors, but there's plenty of available memory"
        },
        {
            "location": "/faqs/",
            "text": "FAQs",
            "title": "FAQs"
        },
        {
            "location": "/faqs/#faqs",
            "text": "",
            "title": "FAQs"
        }
    ]
}