=== RUN   TestAdminSessionFetchBlocksFromPeers
2019-04-29T09:47:37.682-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:47:37.682-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:37.682-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:37.682-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:37.687-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:47:37.687-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:47:37.687-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:47:37.704-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestAdminSessionFetchBlocksFromPeers
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/admin_session_fetch_blocks_test.go:59
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:47:37.704-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestAdminSessionFetchBlocksFromPeers
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/admin_session_fetch_blocks_test.go:59
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:47:38.482-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:47:38.482-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:47:38.483-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:47:38.483-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:47:38.483-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:47:38.483-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:47:38.483-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:47:38.483-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:47:38.537-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556458.7278886,"msg":"successfully updated topology","numHosts":1}
{"level":"info","ts":1556556469.472922,"msg":"successfully updated topology","numHosts":1}
--- PASS: TestAdminSessionFetchBlocksFromPeers (13.40s)
=== RUN   TestBootstrapAfterBufferRotation
2019-04-29T09:47:51.843-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:47:51.843-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:51.843-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:51.843-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:51.845-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:47:51.846-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:47:51.846-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:47:51.896-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestBootstrapAfterBufferRotation
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/bootstrap_after_buffer_rotation_regression_test.go:182
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:47:53.540-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:47:53.541-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:47:53.541-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:47:53.541-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:47:53.541-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:47:53.542-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "test-bootstrapper", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:47:53.542-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
{"level":"info","ts":1556556473.9262793,"msg":"successfully updated topology","numHosts":1}
2019-04-29T09:47:55.050-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "test-bootstrapper", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "2h5m1s", "numSeries": 0}
2019-04-29T09:47:55.050-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:47:55.053-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:47:55.053-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "89.596µs"}
2019-04-29T09:47:55.053-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 1, "datapointsRead": 0}
2019-04-29T09:47:55.053-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:47:55.054-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "took": "2h5m1s"}
2019-04-29T09:47:55.054-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:47:55.054-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "test-bootstrapper", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:47:55.054-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "test-bootstrapper", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:47:55.054-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:47:55.055-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:47:55.055-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "131.598µs"}
2019-04-29T09:47:55.056-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 0, "datapointsRead": 1}
2019-04-29T09:47:55.056-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 1}
2019-04-29T09:47:55.056-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:47:55.056-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 1}
2019-04-29T09:47:55.056-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "2h5m1s"}
2019-04-29T09:47:55.085-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556475.243939,"msg":"successfully updated topology","numHosts":1}
--- PASS: TestBootstrapAfterBufferRotation (5.14s)
=== RUN   TestBootstrapBeforeBufferRotationNoTick
2019-04-29T09:47:55.900-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T11:05:01.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:47:55.900-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T11:05:01.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:47:56.059-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:47:56.060-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:56.060-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:56.060-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:47:56.061-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:47:56.061-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:47:56.061-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:47:56.086-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestBootstrapBeforeBufferRotationNoTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/bootstrap_before_buffer_rotation_no_tick_regression_test.go:195
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:47:58.511-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:47:58.515-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:47:58.516-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:47:58.516-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:47:58.516-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:47:58.516-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "test-bootstrapper", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:47:58.516-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:00.656-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "test-bootstrapper", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "2h5m1s", "numSeries": 0}
2019-04-29T09:48:00.658-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:48:00.664-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:00.668-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "4.770923ms"}
2019-04-29T09:48:00.669-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 1, "datapointsRead": 0}
2019-04-29T09:48:00.669-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:00.669-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "took": "2h5m1s"}
2019-04-29T09:48:00.669-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:00.669-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "test-bootstrapper", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:00.669-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "test-bootstrapper", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:00.670-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:00.679-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:00.679-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "235.834µs"}
2019-04-29T09:48:00.679-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 0, "datapointsRead": 1}
2019-04-29T09:48:00.679-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 1}
2019-04-29T09:48:00.679-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:00.679-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 1}
2019-04-29T09:48:00.680-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "2h5m1s"}
2019-04-29T09:48:00.762-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556482.3292065,"msg":"successfully updated topology","numHosts":1}
--- PASS: TestBootstrapBeforeBufferRotationNoTick (7.92s)
=== RUN   TestClusterAddOneNodeCommitlog
2019-04-29T09:48:04.059-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T11:05:01.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 11:05:01 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:04.059-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T11:05:01.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
{"level":"info","ts":1556556484.5593443,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556484.5593944,"msg":"initial topology / placement value received"}
2019-04-29T09:48:04.652-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:04.652-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:04.652-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:04.652-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:04.653-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:04.653-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:04.653-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 0, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:04.666-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.testClusterAddOneNode
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_test.go:206
github.com/m3db/m3/src/dbnode/integration.TestClusterAddOneNodeCommitlog
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_commitlog_test.go:34
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:05.068-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9000"}
2019-04-29T09:48:05.068-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9002"}
2019-04-29T09:48:05.068-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9001"}
2019-04-29T09:48:05.068-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9003"}
2019-04-29T09:48:05.068-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:05.069-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:05.085-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 12}
{"level":"info","ts":1556556485.085183,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556485.085207,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
2019-04-29T09:48:05.132-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:05.132-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:05.132-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:05.148-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
{"level":"info","ts":1556556485.1488426,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556485.1488564,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
2019-04-29T09:48:05.162-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:05.162-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:05.162-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:48:05.171-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:05.186-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:48:05.186-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:05.186-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:05.186-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:05.186-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:05.187-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:05.187-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:05.187-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 1, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:05.197-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.testClusterAddOneNode
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_test.go:210
github.com/m3db/m3/src/dbnode/integration.TestClusterAddOneNodeCommitlog
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_commitlog_test.go:34
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:05.657-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9005"}
2019-04-29T09:48:05.657-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9007"}
2019-04-29T09:48:05.657-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9006"}
2019-04-29T09:48:05.658-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9008"}
2019-04-29T09:48:05.658-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:05.715-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:48:05.715-0700	DEBUG	servers are now up
2019-04-29T09:48:05.715-0700	DEBUG	resharding to initialize shards on second node
2019-04-29T09:48:05.715-0700	INFO	received update from kv topology watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:05.715-0700	INFO	received update from kv topology watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:05.715-0700	DEBUG	waiting for shards to be bootstrapped
2019-04-29T09:48:05.715-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:05.715-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:05.716-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6}
2019-04-29T09:48:05.716-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6, "took": "0s", "numSeries": 0}
2019-04-29T09:48:05.716-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "peers", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 6}
2019-04-29T09:48:05.716-0700	INFO	peers bootstrapper resolving block retriever	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
2019-04-29T09:48:06.956-0700	INFO	successfully updated topology	{"cache-policy": "recently_read", "instance": 1, "numHosts": 2}
2019-04-29T09:48:07.105-0700	INFO	peers bootstrapper bootstrapping shards for ranges	{"cache-policy": "recently_read", "instance": 1, "shards": 6, "concurrency": 2, "shouldPersist": true}
2019-04-29T09:48:07.154-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "peers", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 6, "took": "2s", "numSeries": 0}
2019-04-29T09:48:07.154-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "2s"}
2019-04-29T09:48:07.154-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:07.158-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6}
2019-04-29T09:48:07.158-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6, "took": "0s", "numSeries": 0}
2019-04-29T09:48:07.158-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "peers", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 6}
2019-04-29T09:48:07.158-0700	INFO	peers bootstrapper bootstrapping shards for ranges	{"cache-policy": "recently_read", "instance": 1, "shards": 6, "concurrency": 4, "shouldPersist": false}
2019-04-29T09:48:07.272-0700	INFO	peer bootstrapped shard	{"cache-policy": "recently_read", "instance": 1, "shard": 8, "numSeries": 1, "blockStart": "2019-04-29T09:00:00.000-0700"}
2019-04-29T09:48:07.272-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "peers", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 6, "took": "0s", "numSeries": 1}
2019-04-29T09:48:07.272-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:07.272-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "numShards": 6, "numSeries": 1}
2019-04-29T09:48:07.279-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "2s"}
2019-04-29T09:48:07.717-0700	DEBUG	waiting for background writes to complete
{"level":"info","ts":1556556487.9590964,"msg":"successfully updated topology","numHosts":2}
2019-04-29T09:48:08.191-0700	DEBUG	waiting for shards to be marked initialized
2019-04-29T09:48:16.198-0700	INFO	cluster db successfully marked shards as available	{"cache-policy": "recently_read", "instance": 1, "shards": [6, 7, 8, 9, 10, 11]}
2019-04-29T09:48:16.233-0700	DEBUG	all shards marked as initialized
2019-04-29T09:48:16.233-0700	DEBUG	resharding to shed shards from first node
{"level":"info","ts":1556556496.233363,"msg":"received update for topology"}
{"level":"info","ts":1556556496.233461,"msg":"successfully updated topology","numHosts":2}
2019-04-29T09:48:16.233-0700	INFO	received update from kv topology watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:16.233-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:16.233-0700	INFO	received update from kv topology watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:16.233-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:16.233-0700	INFO	received update for topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:16.233-0700	INFO	successfully updated topology	{"cache-policy": "recently_read", "instance": 1, "numHosts": 2}
2019-04-29T09:48:17.233-0700	DEBUG	verifying data in servers matches expected data set
2019-04-29T09:48:17.430-0700	INFO	starting server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556497.4303105,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556497.4303255,"msg":"initial topology / placement value received"}
2019-04-29T09:48:17.430-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:17.430-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:17.430-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:17.432-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:17.432-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:17.432-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 1, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:17.432-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.startServerWithNewInspection
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_commitlog_mixed_mode_read_write_test.go:175
github.com/m3db/m3/src/dbnode/integration.testClusterAddOneNode
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_test.go:347
github.com/m3db/m3/src/dbnode/integration.TestClusterAddOneNodeCommitlog
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_commitlog_test.go:34
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:17.813-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9005"}
2019-04-29T09:48:17.813-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9007"}
2019-04-29T09:48:17.814-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9006"}
2019-04-29T09:48:17.814-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9008"}
2019-04-29T09:48:17.814-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:17.814-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9009", "error": "listen tcp 127.0.0.1:9009: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:17.825-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 6}
{"level":"info","ts":1556556497.8256917,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556497.825703,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":6}
2019-04-29T09:48:17.829-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 6, "took": "0s", "numSeries": 0}
2019-04-29T09:48:17.829-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:17.829-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:17.832-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6}
2019-04-29T09:48:17.832-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6, "took": "0s", "numSeries": 0}
2019-04-29T09:48:17.832-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 6}
2019-04-29T09:48:17.841-0700	INFO	starting merge...	{"cache-policy": "recently_read", "instance": 1, "bootstrapper": "commitlog"}
2019-04-29T09:48:17.846-0700	INFO	done merging...	{"cache-policy": "recently_read", "instance": 1, "bootstrapper": "commitlog", "took": "5.39015ms"}
2019-04-29T09:48:17.846-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "instance": 1, "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 0, "datapointsRead": 0}
2019-04-29T09:48:17.846-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 6, "took": "0s", "numSeries": 3}
2019-04-29T09:48:17.846-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:17.846-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "numShards": 6, "numSeries": 3}
2019-04-29T09:48:17.848-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:17.947-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:48:18.052-0700	DEBUG	servers are now down
--- PASS: TestClusterAddOneNodeCommitlog (14.41s)
=== RUN   TestClusterAddOneNode
2019-04-29T09:48:19.752-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:17.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:17 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:17 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:17 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:17 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:17 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:19.752-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:17.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:20.286-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:18.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:18 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:18 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:18 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:18 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:18 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:20.287-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:18.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
{"level":"info","ts":1556556502.2540956,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556502.2541306,"msg":"initial topology / placement value received"}
2019-04-29T09:48:22.443-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:22.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:22 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:22 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:22 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:22 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:22 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:22.443-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:22.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:22.521-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:22.521-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:22.521-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:22.521-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:22.525-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:22.525-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:22.525-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 0, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:22.595-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.testClusterAddOneNode
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_test.go:206
github.com/m3db/m3/src/dbnode/integration.TestClusterAddOneNode
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_test.go:53
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:24.202-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9000"}
2019-04-29T09:48:24.202-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9002"}
2019-04-29T09:48:24.204-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9001"}
2019-04-29T09:48:24.204-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9003"}
2019-04-29T09:48:24.205-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:24.205-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:24.227-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 12}
{"level":"info","ts":1556556504.2278342,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556504.2278538,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
2019-04-29T09:48:24.301-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:24.301-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:24.301-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:24.324-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
{"level":"info","ts":1556556504.3242936,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556504.3243635,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
2019-04-29T09:48:24.352-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:24.353-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:24.353-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:48:24.369-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:24.535-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:48:24.535-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:24.535-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:24.535-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:24.535-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:24.537-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:24.537-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:24.537-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 1, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:24.568-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.testClusterAddOneNode
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_test.go:210
github.com/m3db/m3/src/dbnode/integration.TestClusterAddOneNode
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/cluster_add_one_node_test.go:53
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:25.947-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9005"}
2019-04-29T09:48:25.948-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9007"}
2019-04-29T09:48:25.948-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9006"}
2019-04-29T09:48:25.949-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9008"}
2019-04-29T09:48:25.950-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:25.950-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9009", "error": "listen tcp 127.0.0.1:9009: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:26.048-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:48:26.048-0700	DEBUG	servers are now up
2019-04-29T09:48:26.048-0700	DEBUG	resharding to initialize shards on second node
2019-04-29T09:48:26.049-0700	DEBUG	waiting for shards to be bootstrapped
2019-04-29T09:48:26.053-0700	INFO	received update from kv topology watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:26.053-0700	INFO	received update from kv topology watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:48:26.053-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:26.053-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:26.054-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6}
2019-04-29T09:48:26.054-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6, "took": "0s", "numSeries": 0}
2019-04-29T09:48:26.054-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "peers", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 6}
2019-04-29T09:48:26.054-0700	INFO	peers bootstrapper resolving block retriever	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
2019-04-29T09:48:27.837-0700	INFO	successfully updated topology	{"cache-policy": "recently_read", "instance": 1, "numHosts": 2}
2019-04-29T09:48:28.448-0700	INFO	peers bootstrapper bootstrapping shards for ranges	{"cache-policy": "recently_read", "instance": 1, "shards": 6, "concurrency": 2, "shouldPersist": true}
2019-04-29T09:48:28.589-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "peers", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "shards": 6, "took": "4s", "numSeries": 0}
2019-04-29T09:48:28.590-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "4s"}
2019-04-29T09:48:28.590-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:28.595-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6}
2019-04-29T09:48:28.595-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 6, "took": "0s", "numSeries": 0}
2019-04-29T09:48:28.596-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "peers", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 6}
2019-04-29T09:48:28.596-0700	INFO	peers bootstrapper bootstrapping shards for ranges	{"cache-policy": "recently_read", "instance": 1, "shards": 6, "concurrency": 4, "shouldPersist": false}
{"level":"info","ts":1556556508.6730485,"msg":"successfully updated topology","numHosts":2}
2019-04-29T09:48:28.732-0700	INFO	peer bootstrapped shard	{"cache-policy": "recently_read", "instance": 1, "shard": 8, "numSeries": 1, "blockStart": "2019-04-29T09:00:00.000-0700"}
2019-04-29T09:48:28.732-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "peers", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 6, "took": "0s", "numSeries": 1}
2019-04-29T09:48:28.732-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 6, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:28.732-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "numShards": 6, "numSeries": 1}
2019-04-29T09:48:28.735-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "4s"}
2019-04-29T09:48:29.052-0700	DEBUG	waiting for background writes to complete
2019-04-29T09:48:29.075-0700	DEBUG	waiting for shards to be marked initialized
2019-04-29T09:48:35.570-0700	INFO	cluster db successfully marked shards as available	{"cache-policy": "recently_read", "instance": 1, "shards": [6, 7, 8, 9, 10, 11]}
2019-04-29T09:48:35.615-0700	DEBUG	all shards marked as initialized
2019-04-29T09:48:35.615-0700	DEBUG	resharding to shed shards from first node
2019-04-29T09:48:35.615-0700	INFO	received update from kv topology watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:48:35.615-0700	INFO	received update for topology	{"cache-policy": "recently_read", "instance": 1}
{"level":"info","ts":1556556515.615816,"msg":"received update for topology"}
2019-04-29T09:48:35.615-0700	INFO	successfully updated topology	{"cache-policy": "recently_read", "instance": 1, "numHosts": 2}
2019-04-29T09:48:35.615-0700	INFO	received update from kv topology watch	{"cache-policy": "recently_read", "instance": 0}
{"level":"info","ts":1556556515.6159196,"msg":"successfully updated topology","numHosts":2}
2019-04-29T09:48:35.615-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:35.616-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:36.615-0700	DEBUG	verifying data in servers matches expected data set
2019-04-29T09:48:36.749-0700	DEBUG	servers are now down
--- PASS: TestClusterAddOneNode (18.71s)
=== RUN   TestCommitLogIndexBootstrap
2019-04-29T09:48:37.648-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:11.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:11 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:11 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:11 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:11 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:11 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:37.650-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:11.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:38.654-0700	INFO	commit log bootstrap test	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.654-0700	INFO	generating data	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.655-0700	INFO	writing data	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.663-0700	INFO	finished writing data	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.708-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.708-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.708-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.708-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.710-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.710-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:38.710-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:38.734-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogIndexBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_index_test.go:139
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:38.734-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogIndexBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_index_test.go:139
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:39.201-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:48:39.202-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:48:39.202-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:48:39.202-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:48:39.202-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:48:39.202-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:39.202-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:48:39.215-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:39.215-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "99.02µs"}
2019-04-29T09:48:39.215-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 5, "datapointsSkipped": 365, "datapointsRead": 0}
2019-04-29T09:48:39.215-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:39.215-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "took": "0s"}
2019-04-29T09:48:39.215-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:39.216-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:39.229-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:39.229-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "475.214µs"}
2019-04-29T09:48:39.230-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 5, "datapointsSkipped": 0, "datapointsRead": 365}
2019-04-29T09:48:39.230-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 4}
2019-04-29T09:48:39.230-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:39.230-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T05:00:00.000-0700", "range": "8h0m0s"}
2019-04-29T09:48:39.230-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T05:00:00.000-0700", "range": "8h0m0s", "shards": 12}
2019-04-29T09:48:39.253-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T05:00:00.000-0700", "range": "8h0m0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:48:39.253-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T05:00:00.000-0700", "range": "8h0m0s", "took": "0s"}
2019-04-29T09:48:39.253-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T05:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "8h0m0s"}
2019-04-29T09:48:39.253-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T05:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "8h0m0s", "shards": 12}
2019-04-29T09:48:39.268-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T05:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "8h0m0s", "shards": 12, "took": "0s", "numBlocks": 2, "numSegments": 2}
2019-04-29T09:48:39.268-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T05:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "8h0m0s", "took": "0s"}
2019-04-29T09:48:39.268-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 4}
2019-04-29T09:48:39.269-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:39.269-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:48:39.269-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:48:39.279-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:39.280-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "156.528µs"}
2019-04-29T09:48:39.280-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 5, "datapointsSkipped": 365, "datapointsRead": 0}
2019-04-29T09:48:39.280-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:39.280-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "took": "0s"}
2019-04-29T09:48:39.280-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:39.280-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:39.292-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:39.292-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "492.649µs"}
2019-04-29T09:48:39.292-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 5, "datapointsSkipped": 0, "datapointsRead": 365}
2019-04-29T09:48:39.292-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 4}
2019-04-29T09:48:39.292-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:39.292-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T05:00:00.000-0700", "range": "8h0m0s"}
2019-04-29T09:48:39.292-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T05:00:00.000-0700", "range": "8h0m0s", "shards": 12}
2019-04-29T09:48:39.301-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T05:00:00.000-0700", "range": "8h0m0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:48:39.301-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T05:00:00.000-0700", "range": "8h0m0s", "took": "0s"}
2019-04-29T09:48:39.301-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T05:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "8h0m0s"}
2019-04-29T09:48:39.301-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T05:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "8h0m0s", "shards": 12}
2019-04-29T09:48:39.320-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T05:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "8h0m0s", "shards": 12, "took": "0s", "numBlocks": 2, "numSegments": 2}
2019-04-29T09:48:39.320-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T05:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "8h0m0s", "took": "0s"}
2019-04-29T09:48:39.320-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 4}
2019-04-29T09:48:39.321-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:48:39.348-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556519.5809531,"msg":"successfully updated topology","numHosts":1}
2019-04-29T09:48:39.703-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:16.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:16 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:16 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:16 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:16 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:16 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:39.703-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:16.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:39.735-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:39.735-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
--- PASS: TestCommitLogIndexBootstrap (3.20s)
=== RUN   TestCommitLogAndFSMergeBootstrap
2019-04-29T09:48:40.393-0700	INFO	commit log + fs merge bootstrap test	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.393-0700	INFO	generating data	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.393-0700	INFO	generated data	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.393-0700	INFO	writing filesets	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.806-0700	INFO	writing commit logs	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.885-0700	INFO	moving time forward and starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.885-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.885-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.885-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.885-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.886-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.886-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:40.886-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:40.897-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogAndFSMergeBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_merge_test.go:155
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:41.653-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:48:41.657-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:48:41.657-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:48:41.658-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:48:41.658-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:41.658-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:48:41.689-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "10h0m0s", "shards": 12}
{"level":"info","ts":1556556521.689968,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556521.6899922,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
2019-04-29T09:48:41.799-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "filesystem", "namespace": "testNs1", "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:41.799-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T13:00:00.000-0700", "range": "10h0m0s", "took": "0s"}
2019-04-29T09:48:41.799-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T13:00:00.000-0700", "to": "2019-04-29T17:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:41.841-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:48:41.841-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:41.842-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T13:00:00.000-0700", "to": "2019-04-29T17:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:41.845-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:41.846-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "224.965µs"}
2019-04-29T09:48:41.846-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 119, "datapointsRead": 71}
2019-04-29T09:48:41.846-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T13:00:00.000-0700", "to": "2019-04-29T17:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 2}
2019-04-29T09:48:41.846-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T13:00:00.000-0700", "to": "2019-04-29T17:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:41.846-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 2}
2019-04-29T09:48:41.865-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:42.018-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.018-0700	INFO	validating bootstrapped data	{"cache-policy": "recently_read"}
--- PASS: TestCommitLogAndFSMergeBootstrap (2.17s)
=== RUN   TestCommitLogBootstrapMultipleNamespaces
2019-04-29T09:48:42.439-0700	WARN	unable to find a single blockSize from known retention periods	{"cache-policy": "recently_read", "guessing": "1h0m0s"}
github.com/m3db/m3/src/dbnode/integration.newTestSetup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:279
github.com/m3db/m3/src/dbnode/integration.TestCommitLogBootstrapMultipleNamespaces
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_multi_ns_test.go:59
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:42.457-0700	INFO	generating data - ns1	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.457-0700	INFO	writing data - ns1	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.466-0700	INFO	written data - ns1	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.466-0700	INFO	generating data - ns2	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.466-0700	INFO	writing data - ns2	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.480-0700	INFO	written data - ns2	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.538-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.538-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.538-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.538-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.540-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.540-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:42.540-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:42.569-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogBootstrapMultipleNamespaces
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_multi_ns_test.go:102
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:42.569-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogBootstrapMultipleNamespaces
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_multi_ns_test.go:102
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:42.974-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T15:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:42.974-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T15:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:42.986-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:48:42.986-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:48:42.987-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:48:42.987-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:48:42.987-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-27T13:00:00.000-0700", "to": "2019-04-29T12:00:00.000-0700", "range": "47h0m0s"}
2019-04-29T09:48:42.987-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:42.987-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-27T13:00:00.000-0700", "to": "2019-04-29T12:00:00.000-0700", "range": "47h0m0s", "shards": 12}
2019-04-29T09:48:43.001-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:43.001-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "728.796µs"}
2019-04-29T09:48:43.001-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 8, "datapointsSkipped": 95, "datapointsRead": 166}
2019-04-29T09:48:43.001-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-27T13:00:00.000-0700", "to": "2019-04-29T12:00:00.000-0700", "range": "47h0m0s", "shards": 12, "took": "0s", "numSeries": 3}
2019-04-29T09:48:43.002-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-27T13:00:00.000-0700", "to": "2019-04-29T12:00:00.000-0700", "range": "47h0m0s", "took": "0s"}
2019-04-29T09:48:43.002-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T12:00:00.000-0700", "to": "2019-04-29T14:00:00.000-0700", "range": "2h0m0s"}
2019-04-29T09:48:43.002-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T12:00:00.000-0700", "to": "2019-04-29T14:00:00.000-0700", "range": "2h0m0s", "shards": 12}
2019-04-29T09:48:43.012-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:43.012-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "265.457µs"}
2019-04-29T09:48:43.012-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 8, "datapointsSkipped": 166, "datapointsRead": 95}
2019-04-29T09:48:43.012-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T12:00:00.000-0700", "to": "2019-04-29T14:00:00.000-0700", "range": "2h0m0s", "shards": 12, "took": "0s", "numSeries": 2}
2019-04-29T09:48:43.012-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T12:00:00.000-0700", "to": "2019-04-29T14:00:00.000-0700", "range": "2h0m0s", "took": "0s"}
2019-04-29T09:48:43.012-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 5}
2019-04-29T09:48:43.013-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:43.013-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-27T13:00:00.000-0700", "to": "2019-04-29T12:30:00.000-0700", "range": "47h30m0s"}
2019-04-29T09:48:43.013-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-27T13:00:00.000-0700", "to": "2019-04-29T12:30:00.000-0700", "range": "47h30m0s", "shards": 12}
2019-04-29T09:48:43.024-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:43.025-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "1.002339ms"}
2019-04-29T09:48:43.025-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 6, "datapointsSkipped": 0, "datapointsRead": 452}
2019-04-29T09:48:43.025-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-27T13:00:00.000-0700", "to": "2019-04-29T12:30:00.000-0700", "range": "47h30m0s", "shards": 12, "took": "0s", "numSeries": 8}
2019-04-29T09:48:43.026-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-27T13:00:00.000-0700", "to": "2019-04-29T12:30:00.000-0700", "range": "47h30m0s", "took": "0s"}
2019-04-29T09:48:43.026-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T12:30:00.000-0700", "to": "2019-04-29T13:30:00.000-0700", "range": "1h0m0s"}
2019-04-29T09:48:43.026-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T12:30:00.000-0700", "to": "2019-04-29T13:30:00.000-0700", "range": "1h0m0s", "shards": 12}
2019-04-29T09:48:43.041-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:43.041-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "338.294µs"}
2019-04-29T09:48:43.042-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 6, "datapointsSkipped": 452, "datapointsRead": 0}
2019-04-29T09:48:43.042-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T12:30:00.000-0700", "to": "2019-04-29T13:30:00.000-0700", "range": "1h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:43.042-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T12:30:00.000-0700", "to": "2019-04-29T13:30:00.000-0700", "range": "1h0m0s", "took": "0s"}
2019-04-29T09:48:43.042-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 8}
2019-04-29T09:48:43.042-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:48:43.080-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:48:43.080-0700	INFO	waiting until data is bootstrapped	{"cache-policy": "recently_read"}
2019-04-29T09:48:43.080-0700	INFO	data bootstrapped	{"cache-policy": "recently_read"}
2019-04-29T09:48:43.080-0700	INFO	verifying ns1 data	{"cache-policy": "recently_read"}
2019-04-29T09:48:43.084-0700	INFO	verified ns1 data	{"cache-policy": "recently_read"}
2019-04-29T09:48:43.084-0700	INFO	verifying ns2 data	{"cache-policy": "recently_read"}
2019-04-29T09:48:43.088-0700	INFO	verified ns2 data	{"cache-policy": "recently_read"}
2019-04-29T09:48:43.570-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T13:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 13:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 13:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 13:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 13:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 13:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:43.570-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T13:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
--- PASS: TestCommitLogBootstrapMultipleNamespaces (2.10s)
=== RUN   TestCommitLogBootstrapOnlyReadsRequiredFiles
--- SKIP: TestCommitLogBootstrapOnlyReadsRequiredFiles (0.00s)
=== RUN   TestCommitLogBootstrap
2019-04-29T09:48:44.617-0700	INFO	commit log bootstrap test	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.617-0700	INFO	generating data	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.623-0700	INFO	writing data	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.642-0700	INFO	finished writing data	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.694-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.694-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.694-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.694-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.696-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.696-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:44.696-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:44.730-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_test.go:77
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:44.731-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_test.go:77
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:45.239-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:48:45.239-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:48:45.240-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:48:45.240-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:48:45.240-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:48:45.240-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:48:45.240-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:45.261-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:45.263-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "1.857217ms"}
2019-04-29T09:48:45.263-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 2540, "datapointsRead": 3753}
2019-04-29T09:48:45.263-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 68}
2019-04-29T09:48:45.263-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "took": "0s"}
2019-04-29T09:48:45.263-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:45.263-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:45.286-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:45.288-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "1.384423ms"}
2019-04-29T09:48:45.288-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 3753, "datapointsRead": 2540}
2019-04-29T09:48:45.288-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 44}
2019-04-29T09:48:45.288-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:45.288-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 112}
2019-04-29T09:48:45.289-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:45.289-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:48:45.289-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:48:45.308-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:45.308-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "132.172µs"}
2019-04-29T09:48:45.308-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 112, "datapointsSkipped": 0, "datapointsRead": 0}
2019-04-29T09:48:45.308-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:45.308-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "took": "0s"}
2019-04-29T09:48:45.308-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:45.309-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:45.330-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:45.330-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "96.276µs"}
2019-04-29T09:48:45.330-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 110, "datapointsSkipped": 0, "datapointsRead": 0}
2019-04-29T09:48:45.330-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:45.330-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:45.330-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:48:45.330-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:48:45.353-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556525.5638416,"msg":"successfully updated topology","numHosts":1}
{"level":"error","ts":1556556527.419952,"msg":"stream blocks returned more blocks than expected","id":"HOveyhkUVAHwWIYrfVgTVArErFlSwhbbinVfcywQLnVfkuCOJenXSXgTjEDBHxvmrrFWcwHTFQMzggJqqcmIyuVbyQHRXNzrOaOn","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.419994,"msg":"stream blocks returned more blocks than expected","id":"GjqcUPfInbSnxcDZDHOgKSWrMWtsxyQLUHnbRVRJzMxhSTagjgENbnDQboQqxfoCfEsAbndwoGFejQxQamsSSybLFffHBAEqcMtS","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.4200068,"msg":"stream blocks returned more blocks than expected","id":"jDqnzeZAKEXbsGhRhFqQcCEzCMWwBiDNlpezJuJUAUxYFJAGFVJfawlIHBCcvJosxkoOkrzlfGjhOORMRHjJkEXquzbHwKvqDhRg","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.4200408,"msg":"stream blocks returned more blocks than expected","id":"TGcoJwtsrhKyLgerJWLRSbVBAchGHfeAffoHvKnSqNVLAnSNRRftpqSifLxoGIGfvnVIVftrQNoNUKEKtYxowOZIVBSryZLggARy","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.519093,"msg":"stream blocks returned more blocks than expected","id":"HOveyhkUVAHwWIYrfVgTVArErFlSwhbbinVfcywQLnVfkuCOJenXSXgTjEDBHxvmrrFWcwHTFQMzggJqqcmIyuVbyQHRXNzrOaOn","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.5191536,"msg":"stream blocks returned more blocks than expected","id":"jDqnzeZAKEXbsGhRhFqQcCEzCMWwBiDNlpezJuJUAUxYFJAGFVJfawlIHBCcvJosxkoOkrzlfGjhOORMRHjJkEXquzbHwKvqDhRg","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.519189,"msg":"stream blocks returned more blocks than expected","id":"GjqcUPfInbSnxcDZDHOgKSWrMWtsxyQLUHnbRVRJzMxhSTagjgENbnDQboQqxfoCfEsAbndwoGFejQxQamsSSybLFffHBAEqcMtS","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.5192409,"msg":"stream blocks returned more blocks than expected","id":"TGcoJwtsrhKyLgerJWLRSbVBAchGHfeAffoHvKnSqNVLAnSNRRftpqSifLxoGIGfvnVIVftrQNoNUKEKtYxowOZIVBSryZLggARy","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.620047,"msg":"stream blocks returned more blocks than expected","id":"HOveyhkUVAHwWIYrfVgTVArErFlSwhbbinVfcywQLnVfkuCOJenXSXgTjEDBHxvmrrFWcwHTFQMzggJqqcmIyuVbyQHRXNzrOaOn","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.620088,"msg":"stream blocks returned more blocks than expected","id":"TGcoJwtsrhKyLgerJWLRSbVBAchGHfeAffoHvKnSqNVLAnSNRRftpqSifLxoGIGfvnVIVftrQNoNUKEKtYxowOZIVBSryZLggARy","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.620101,"msg":"stream blocks returned more blocks than expected","id":"jDqnzeZAKEXbsGhRhFqQcCEzCMWwBiDNlpezJuJUAUxYFJAGFVJfawlIHBCcvJosxkoOkrzlfGjhOORMRHjJkEXquzbHwKvqDhRg","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.6201105,"msg":"stream blocks returned more blocks than expected","id":"GjqcUPfInbSnxcDZDHOgKSWrMWtsxyQLUHnbRVRJzMxhSTagjgENbnDQboQqxfoCfEsAbndwoGFejQxQamsSSybLFffHBAEqcMtS","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.72024,"msg":"stream blocks returned more blocks than expected","id":"HOveyhkUVAHwWIYrfVgTVArErFlSwhbbinVfcywQLnVfkuCOJenXSXgTjEDBHxvmrrFWcwHTFQMzggJqqcmIyuVbyQHRXNzrOaOn","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.7202814,"msg":"stream blocks returned more blocks than expected","id":"TGcoJwtsrhKyLgerJWLRSbVBAchGHfeAffoHvKnSqNVLAnSNRRftpqSifLxoGIGfvnVIVftrQNoNUKEKtYxowOZIVBSryZLggARy","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.7202947,"msg":"stream blocks returned more blocks than expected","id":"GjqcUPfInbSnxcDZDHOgKSWrMWtsxyQLUHnbRVRJzMxhSTagjgENbnDQboQqxfoCfEsAbndwoGFejQxQamsSSybLFffHBAEqcMtS","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.720307,"msg":"stream blocks returned more blocks than expected","id":"jDqnzeZAKEXbsGhRhFqQcCEzCMWwBiDNlpezJuJUAUxYFJAGFVJfawlIHBCcvJosxkoOkrzlfGjhOORMRHjJkEXquzbHwKvqDhRg","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.8217154,"msg":"stream blocks returned more blocks than expected","id":"HOveyhkUVAHwWIYrfVgTVArErFlSwhbbinVfcywQLnVfkuCOJenXSXgTjEDBHxvmrrFWcwHTFQMzggJqqcmIyuVbyQHRXNzrOaOn","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.8217475,"msg":"stream blocks returned more blocks than expected","id":"GjqcUPfInbSnxcDZDHOgKSWrMWtsxyQLUHnbRVRJzMxhSTagjgENbnDQboQqxfoCfEsAbndwoGFejQxQamsSSybLFffHBAEqcMtS","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.8217611,"msg":"stream blocks returned more blocks than expected","id":"jDqnzeZAKEXbsGhRhFqQcCEzCMWwBiDNlpezJuJUAUxYFJAGFVJfawlIHBCcvJosxkoOkrzlfGjhOORMRHjJkEXquzbHwKvqDhRg","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.8218014,"msg":"stream blocks returned more blocks than expected","id":"TGcoJwtsrhKyLgerJWLRSbVBAchGHfeAffoHvKnSqNVLAnSNRRftpqSifLxoGIGfvnVIVftrQNoNUKEKtYxowOZIVBSryZLggARy","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.9216342,"msg":"stream blocks returned more blocks than expected","id":"TGcoJwtsrhKyLgerJWLRSbVBAchGHfeAffoHvKnSqNVLAnSNRRftpqSifLxoGIGfvnVIVftrQNoNUKEKtYxowOZIVBSryZLggARy","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.921712,"msg":"stream blocks returned more blocks than expected","id":"HOveyhkUVAHwWIYrfVgTVArErFlSwhbbinVfcywQLnVfkuCOJenXSXgTjEDBHxvmrrFWcwHTFQMzggJqqcmIyuVbyQHRXNzrOaOn","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.9217458,"msg":"stream blocks returned more blocks than expected","id":"GjqcUPfInbSnxcDZDHOgKSWrMWtsxyQLUHnbRVRJzMxhSTagjgENbnDQboQqxfoCfEsAbndwoGFejQxQamsSSybLFffHBAEqcMtS","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556527.921784,"msg":"stream blocks returned more blocks than expected","id":"jDqnzeZAKEXbsGhRhFqQcCEzCMWwBiDNlpezJuJUAUxYFJAGFVJfawlIHBCcvJosxkoOkrzlfGjhOORMRHjJkEXquzbHwKvqDhRg","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556528.0233653,"msg":"stream blocks returned more blocks than expected","id":"jDqnzeZAKEXbsGhRhFqQcCEzCMWwBiDNlpezJuJUAUxYFJAGFVJfawlIHBCcvJosxkoOkrzlfGjhOORMRHjJkEXquzbHwKvqDhRg","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556528.0234263,"msg":"stream blocks returned more blocks than expected","id":"TGcoJwtsrhKyLgerJWLRSbVBAchGHfeAffoHvKnSqNVLAnSNRRftpqSifLxoGIGfvnVIVftrQNoNUKEKtYxowOZIVBSryZLggARy","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556528.0234427,"msg":"stream blocks returned more blocks than expected","id":"HOveyhkUVAHwWIYrfVgTVArErFlSwhbbinVfcywQLnVfkuCOJenXSXgTjEDBHxvmrrFWcwHTFQMzggJqqcmIyuVbyQHRXNzrOaOn","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556528.0234544,"msg":"stream blocks returned more blocks than expected","id":"GjqcUPfInbSnxcDZDHOgKSWrMWtsxyQLUHnbRVRJzMxhSTagjgENbnDQboQqxfoCfEsAbndwoGFejQxQamsSSybLFffHBAEqcMtS","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556528.1236432,"msg":"stream blocks returned more blocks than expected","id":"jDqnzeZAKEXbsGhRhFqQcCEzCMWwBiDNlpezJuJUAUxYFJAGFVJfawlIHBCcvJosxkoOkrzlfGjhOORMRHjJkEXquzbHwKvqDhRg","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556528.1236672,"msg":"stream blocks returned more blocks than expected","id":"GjqcUPfInbSnxcDZDHOgKSWrMWtsxyQLUHnbRVRJzMxhSTagjgENbnDQboQqxfoCfEsAbndwoGFejQxQamsSSybLFffHBAEqcMtS","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556528.1236923,"msg":"stream blocks returned more blocks than expected","id":"HOveyhkUVAHwWIYrfVgTVArErFlSwhbbinVfcywQLnVfkuCOJenXSXgTjEDBHxvmrrFWcwHTFQMzggJqqcmIyuVbyQHRXNzrOaOn","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556528.1237397,"msg":"stream blocks returned more blocks than expected","id":"TGcoJwtsrhKyLgerJWLRSbVBAchGHfeAffoHvKnSqNVLAnSNRRftpqSifLxoGIGfvnVIVftrQNoNUKEKtYxowOZIVBSryZLggARy","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
2019-04-29T09:48:49.471-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
--- PASS: TestCommitLogBootstrap (5.31s)
=== RUN   TestCommitLogBootstrapWithSnapshots
2019-04-29T09:48:49.886-0700	INFO	commit log bootstrap test	{"cache-policy": "recently_read"}
2019-04-29T09:48:49.886-0700	INFO	generating data	{"cache-policy": "recently_read"}
2019-04-29T09:48:49.890-0700	INFO	writing data	{"cache-policy": "recently_read"}
2019-04-29T09:48:50.103-0700	INFO	finished writing data	{"cache-policy": "recently_read"}
2019-04-29T09:48:50.155-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:50.155-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:50.155-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:50.155-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:50.157-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:50.157-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:50.157-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:50.184-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogBootstrapWithSnapshots
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_with_snapshots_test.go:110
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:50.184-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestCommitLogBootstrapWithSnapshots
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/commitlog_bootstrap_with_snapshots_test.go:110
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:50.664-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:48:50.666-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:48:50.666-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:48:50.667-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:48:50.667-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:48:50.667-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:48:50.667-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:48:50.707-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:50.732-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "24.384958ms"}
2019-04-29T09:48:50.732-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 2925, "datapointsRead": 2562}
2019-04-29T09:48:50.732-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 47}
2019-04-29T09:48:50.732-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "took": "0s"}
2019-04-29T09:48:50.732-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:50.732-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:50.767-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:50.778-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "11.440169ms"}
2019-04-29T09:48:50.778-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 2562, "datapointsRead": 2925}
2019-04-29T09:48:50.778-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 69}
2019-04-29T09:48:50.778-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:50.778-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 116}
2019-04-29T09:48:50.779-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:48:50.779-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s"}
2019-04-29T09:48:50.779-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12}
2019-04-29T09:48:50.795-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:50.796-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "123.511µs"}
2019-04-29T09:48:50.796-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 106, "datapointsSkipped": 0, "datapointsRead": 0}
2019-04-29T09:48:50.796-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:50.796-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-28T21:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "10h0m0s", "took": "0s"}
2019-04-29T09:48:50.796-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:48:50.796-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12}
2019-04-29T09:48:50.815-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:48:50.815-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "301.97µs"}
2019-04-29T09:48:50.815-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 104, "datapointsSkipped": 0, "datapointsRead": 0}
2019-04-29T09:48:50.815-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs2", "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:48:50.815-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs2", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:48:50.815-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:48:50.816-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:48:50.915-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556532.0567496,"msg":"successfully updated topology","numHosts":1}
{"level":"error","ts":1556556534.5316653,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.5317059,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.5317206,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.531745,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.53178,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.6315825,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.6316223,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.6316595,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.6316907,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.6317196,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.7312703,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.7313244,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.7313435,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.731365,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.7313776,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.831271,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.8315883,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.831671,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.831729,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.831784,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.9338052,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.9338691,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.9338908,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.933939,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556534.933975,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.033094,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.0331566,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.033174,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.0331943,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.0332077,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.1336076,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.133667,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.133687,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.1337073,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.133734,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.23679,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.2368307,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.2369587,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.237016,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.2370977,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.3352556,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.3355265,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.3355684,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.3355827,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.3355997,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.4382336,"msg":"stream blocks returned more blocks than expected","id":"MGCEFXFNdFSBqfWXxmBwHJejVJXCdfKVuipQnXXZQpBsfFIExaYprcSvjeaurHbzbGLCzCCuqXxWpREnTuqhOrgfdRZwjXclqZBf","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.4382765,"msg":"stream blocks returned more blocks than expected","id":"VTuZsyIWAxRPfJFJZKasXWBHeMSPyyLdGRlWsJVPUBTKiQRZXHceeXvYaVDbgkUmEPTzDMYoUsmSXzOlEQAKraCAtTDrBjMoloUa","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.4382896,"msg":"stream blocks returned more blocks than expected","id":"WwHXnjwUJCEatRwzTysUHfEThhlVeRYMWmMiHqoQSHMIBMyqvZxjIVPhFkDioGBvQDFvZXlJszMBwgRTsnbKMghqNRWEMgksEHQH","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.438314,"msg":"stream blocks returned more blocks than expected","id":"xmVuEpEPuLKXoQrtFcIQrKUNcAVPOXWEYUYwXtCVJfBDVIqfeUIFbZDnzFXdTmPliaNolcMVhfqaWTFJxjcIrsVcvNByaKjYmZbA","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
{"level":"error","ts":1556556535.4383283,"msg":"stream blocks returned more blocks than expected","id":"zfFUpfKmfdeATyxqtQgpfQquBXWNBCMEqoUnAUyQFLjtuskDuFYCmNawTshgxQNoqUbsGGAvxaKmrwObJHgKdONtnlkVwKaCMqvQ","expectedStarts":[1556539200],"actualStarts":[1556539200,1556539200],"peer":"Host<ID=testhost, Address=127.0.0.1:9003>"}
--- PASS: TestCommitLogBootstrapWithSnapshots (8.81s)
=== RUN   TestDiskCleansupInactiveDirectories
2019-04-29T09:48:58.541-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:58.542-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:48:59.471-0700	INFO	disk cleanup directories test	{"cache-policy": "recently_read"}
2019-04-29T09:48:59.471-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:48:59.471-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:59.472-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:59.472-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:48:59.477-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:59.477-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:48:59.478-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:48:59.579-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskCleansupInactiveDirectories
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_cleanup_deletes_inactive_directories_test.go:50
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:48:59.579-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskCleansupInactiveDirectories
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_cleanup_deletes_inactive_directories_test.go:50
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:01.620-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:49:01.622-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:49:01.623-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:49:01.623-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:49:01.623-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:01.624-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:49:01.624-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:49:01.624-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:01.624-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:49:01.750-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:49:01.750-0700	INFO	blocking until file cleanup is received	{"cache-policy": "recently_read"}
2019-04-29T09:49:01.750-0700	INFO	blocking until namespaces have reset and deleted	{"cache-policy": "recently_read"}
2019-04-29T09:49:01.750-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:49:01.751-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:49:02.586-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:49:02.586-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:49:03.931-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:49:03.931-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:03.931-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:03.931-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:03.936-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:03.936-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:03.936-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:49:03.994-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.waitUntilNamespacesHaveReset
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_cleanup_helpers.go:187
github.com/m3db/m3/src/dbnode/integration.TestDiskCleansupInactiveDirectories.func2
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_cleanup_deletes_inactive_directories_test.go:85
2019-04-29T09:49:05.875-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:49:05.875-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:49:05.878-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:49:05.879-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:49:05.879-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:05.880-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:49:05.880-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:49:05.971-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:49:05.971-0700	INFO	blocking until the namespace cleanup is received	{"cache-policy": "recently_read"}
2019-04-29T09:49:06.002-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:49:06.002-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
--- PASS: TestDiskCleansupInactiveDirectories (8.76s)
=== RUN   TestDiskCleanupIndex
2019-04-29T09:49:08.170-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:49:08.171-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:08.171-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:08.171-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:08.172-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:08.173-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:08.173-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:49:08.225-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskCleanupIndex
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_cleanup_index_test.go:70
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:09.834-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:49:09.835-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:49:09.835-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:49:09.835-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:49:09.835-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:09.836-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:49:09.836-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:49:09.881-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:49:20.912-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-05-01T11:00:00.000-0700", "error": "encountered errors when cleaning up snapshot and commitlog files: commit log is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:49:20.912-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-05-01T11:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
--- PASS: TestDiskCleanupIndex (15.03s)
=== RUN   TestDiskCleanup
2019-04-29T09:49:25.111-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:49:25.111-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:25.111-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:25.111-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:25.115-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:25.115-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:25.115-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:49:25.157-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskCleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_cleanup_test.go:77
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:25.157-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskCleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_cleanup_test.go:77
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:25.674-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:49:25.675-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:49:25.675-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:49:25.675-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:49:25.675-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:25.676-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:49:25.678-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:49:25.678-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:25.678-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:49:25.679-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556566.7759478,"msg":"successfully updated topology","numHosts":1}
--- PASS: TestDiskCleanup (5.06s)
=== RUN   TestDiskFlushMultipleNamespace
2019-04-29T09:49:27.439-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T19:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 19:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 19:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 19:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 19:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 19:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:49:27.440-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T19:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:49:29.872-0700	WARN	unable to find a single blockSize from known retention periods	{"cache-policy": "recently_read", "guessing": "6h0m0s"}
github.com/m3db/m3/src/dbnode/integration.newTestSetup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:279
github.com/m3db/m3/src/dbnode/integration.TestDiskFlushMultipleNamespace
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_flush_multi_ns_test.go:60
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:29.900-0700	INFO	disk flush multiple namespaces test	{"cache-policy": "recently_read"}
2019-04-29T09:49:29.900-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:49:29.900-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:29.900-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:29.901-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:29.902-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:29.902-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:29.902-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:49:29.941-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskFlushMultipleNamespace
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_flush_multi_ns_test.go:73
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:29.941-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskFlushMultipleNamespace
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_flush_multi_ns_test.go:73
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:30.498-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:49:30.499-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:49:30.499-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:49:30.499-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:49:30.499-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:30.500-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:49:30.500-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:30.500-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:49:30.500-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:49:30.574-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:49:30.574-0700	INFO	server is now up	{"cache-policy": "recently_read"}
2019-04-29T09:49:30.574-0700	INFO	generating test data	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556570.8025868,"msg":"successfully updated topology","numHosts":1}
2019-04-29T09:49:31.041-0700	INFO	wrote ns1 for time	{"cache-policy": "recently_read", "start": "2019-04-29T05:00:00.000-0700"}
2019-04-29T09:49:31.041-0700	INFO	wrote ns2 for time	{"cache-policy": "recently_read", "start": "2019-04-29T05:00:00.000-0700"}
2019-04-29T09:49:31.134-0700	INFO	wrote ns1 for time	{"cache-policy": "recently_read", "start": "2019-04-29T07:00:00.000-0700"}
2019-04-29T09:49:31.134-0700	INFO	test data written successfully	{"cache-policy": "recently_read"}
2019-04-29T09:49:31.134-0700	INFO	waiting until data is flushed	{"cache-policy": "recently_read"}
2019-04-29T09:49:33.152-0700	INFO	data has been flushed	{"cache-policy": "recently_read"}
2019-04-29T09:49:33.152-0700	INFO	verifying flushed data	{"cache-policy": "recently_read"}
2019-04-29T09:49:33.161-0700	INFO	flushed data verified	{"cache-policy": "recently_read"}
2019-04-29T09:49:33.269-0700	INFO	server is now down	{"cache-policy": "recently_read"}
2019-04-29T09:49:33.441-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T13:00:00.000-0700", "error": "error rotating commitlog in mediator tick: commit log is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
--- PASS: TestDiskFlushMultipleNamespace (6.82s)
=== RUN   TestDiskFlushSimple
2019-04-29T09:49:34.865-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:49:34.865-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:34.865-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:34.865-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:34.866-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:34.866-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:34.866-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:49:34.905-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskFlushSimple
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_flush_test.go:53
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:34.905-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskFlushSimple
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_flush_test.go:53
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:35.538-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:49:35.539-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:49:35.540-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:49:35.540-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:49:35.540-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:35.541-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:49:35.541-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:35.541-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:49:35.541-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:49:35.641-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556575.99178,"msg":"successfully updated topology","numHosts":1}
--- PASS: TestDiskFlushSimple (4.82s)
=== RUN   TestDiskSnapshotSimple
2019-04-29T09:49:39.161-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:49:39.161-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:39.161-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:39.161-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:39.163-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:39.163-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:39.163-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:49:39.197-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskSnapshotSimple
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_snapshot_test.go:72
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:39.197-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDiskSnapshotSimple
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/disk_snapshot_test.go:72
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:39.777-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:49:39.777-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:49:39.778-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:49:39.778-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:49:39.778-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:39.778-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:49:39.778-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:49:39.778-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:39.779-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:49:39.831-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556580.0204175,"msg":"successfully updated topology","numHosts":1}
2019-04-29T09:49:40.391-0700	INFO	waiting for snapshot files to flush	{"cache-policy": "recently_read"}
2019-04-29T09:49:49.697-0700	INFO	verifying snapshot files	{"cache-policy": "recently_read"}
2019-04-29T09:49:49.739-0700	INFO	waiting for snapshot files to flush	{"cache-policy": "recently_read"}
2019-04-29T09:49:50.199-0700	INFO	verifying snapshot files	{"cache-policy": "recently_read"}
2019-04-29T09:49:50.230-0700	INFO	waiting for new snapshot files to be written out	{"cache-policy": "recently_read"}
2019-04-29T09:49:54.785-0700	INFO	waiting for old snapshot files to be deleted	{"cache-policy": "recently_read"}
2019-04-29T09:49:56.003-0700	INFO	waiting for new snapshot files to be written out	{"cache-policy": "recently_read"}
2019-04-29T09:49:56.005-0700	INFO	waiting for old snapshot files to be deleted	{"cache-policy": "recently_read"}
--- PASS: TestDiskSnapshotSimple (18.33s)
=== RUN   TestDynamicNamespaceAdd
2019-04-29 09:49:57.263817 I | embed: listening for peers on http://localhost:2380
2019-04-29 09:49:57.267050 I | embed: listening for client requests on localhost:2379
2019-04-29 09:49:57.272101 I | etcdserver: name = default
2019-04-29 09:49:57.272124 I | etcdserver: data dir = /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/etcd.dir174785813
2019-04-29 09:49:57.272135 I | etcdserver: member dir = /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/etcd.dir174785813/member
2019-04-29 09:49:57.272142 I | etcdserver: heartbeat = 100ms
2019-04-29 09:49:57.272148 I | etcdserver: election = 1000ms
2019-04-29 09:49:57.272154 I | etcdserver: snapshot count = 100000
2019-04-29 09:49:57.272177 I | etcdserver: advertise client URLs = http://localhost:2379
2019-04-29 09:49:57.272201 I | etcdserver: initial advertise peer URLs = http://localhost:2380
2019-04-29 09:49:57.272217 I | etcdserver: initial cluster = default=http://localhost:2380
2019-04-29 09:49:57.442068 I | etcdserver: starting member 8e9e05c52164694d in cluster cdf818194e3a8c32
2019-04-29 09:49:57.442156 I | raft: 8e9e05c52164694d became follower at term 0
2019-04-29 09:49:57.442201 I | raft: newRaft 8e9e05c52164694d [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]
2019-04-29 09:49:57.442228 I | raft: 8e9e05c52164694d became follower at term 1
2019-04-29 09:49:57.510592 W | auth: simple token is not cryptographically signed
2019-04-29 09:49:57.511336 I | etcdserver: starting server... [version: 3.2.10, cluster version: to_be_decided]
2019-04-29 09:49:57.511439 E | etcdserver: cannot monitor file descriptor usage (cannot get FDUsage on darwin)
2019-04-29 09:49:57.511934 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
2019-04-29 09:49:58.409029 I | raft: 8e9e05c52164694d is starting a new election at term 1
2019-04-29 09:49:58.409152 I | raft: 8e9e05c52164694d became candidate at term 2
2019-04-29 09:49:58.409195 I | raft: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 2
2019-04-29 09:49:58.409239 I | raft: 8e9e05c52164694d became leader at term 2
2019-04-29 09:49:58.409254 I | raft: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 2
2019-04-29 09:49:58.409407 I | etcdserver: setting up the initial cluster version to 3.2
2019-04-29 09:49:58.438406 N | etcdserver/membership: set the initial cluster version to 3.2
2019-04-29 09:49:58.438464 I | etcdserver/api: enabled capabilities for version 3.2
2019-04-29 09:49:58.438503 I | etcdserver: published {Name:default ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32
2019-04-29 09:49:58.438862 I | embed: ready to serve client requests
2019-04-29 09:49:58.439096 I | etcdserver/api/v3rpc: dialing to target with scheme: ""
2019-04-29 09:49:58.439110 I | etcdserver/api/v3rpc: could not get resolver for scheme: ""
2019-04-29 09:49:58.439254 N | embed: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged!
2019-04-29 09:49:58.452877 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: IDLE, 0xcbd8a40960
2019-04-29 09:49:58.452899 I | etcdserver/api/v3rpc: dialing to target with scheme: ""
2019-04-29 09:49:58.452907 I | etcdserver/api/v3rpc: could not get resolver for scheme: ""
2019-04-29 09:49:58.452956 I | etcdserver/api/v3rpc: balancerWrapper: is pickfirst: false
2019-04-29 09:49:58.452985 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:49:58.453009 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:49:58.453056 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbd898b8e0, CONNECTING
2019-04-29 09:49:58.453084 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:49:58.454541 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbd898b8e0, READY
2019-04-29 09:49:58.454571 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: READY, 0xcbd8a40960
2019-04-29 09:49:58.454613 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:49:58.687-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:49:58.687-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:58.687-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:58.687-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:49:58.689-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556598.6891937,"msg":"waiting for dynamic namespace registry initialization, if this takes a long time, make sure that a namespace is configured"}
{"level":"info","ts":1556556598.6904542,"msg":"initial namespace value received"}
2019-04-29T09:49:58.690-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:58.690-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:49:58.700-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDynamicNamespaceAdd
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/dynamic_namespace_add_test.go:86
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:49:59.155-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:49:59.156-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:49:59.156-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:49:59.156-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:49:59.156-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:49:59.157-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:49:59.157-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:49:59.213-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:49:59.213-0700	INFO	test data is now generated	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556599.4230468,"msg":"successfully updated topology","numHosts":1}
2019-04-29T09:49:59.715-0700	INFO	new namespace added to kv	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556599.716034,"msg":"dynamic namespace registry updated to version","version":2}
2019-04-29T09:49:59.716-0700	INFO	received update from kv namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:49:59.716-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:49:59.725-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.(*dbNamespaceWatch).startWatch
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/namespace_watch.go:146
2019-04-29T09:49:59.725-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.(*dbNamespaceWatch).startWatch
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/namespace_watch.go:146
2019-04-29T09:49:59.816-0700	INFO	new namespace available in testSetup	{"cache-policy": "recently_read"}
2019-04-29T09:49:59.986-0700	INFO	test data is now written	{"cache-policy": "recently_read"}
2019-04-29T09:50:00.726-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:50:00.726-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:50:00.726-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
{"level":"info","ts":1556556610.1783834,"msg":"successfully updated topology","numHosts":1}
2019-04-29T09:50:11.002-0700	INFO	reading data from testSetup	{"cache-policy": "recently_read"}
2019-04-29T09:50:11.004-0700	INFO	data is verified	{"cache-policy": "recently_read"}
2019-04-29T09:50:11.106-0700	INFO	server is now down	{"cache-policy": "recently_read"}
2019-04-29 09:50:11.205356 I | etcdserver: skipped leadership transfer for single member cluster
2019-04-29T09:50:11.219-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T15:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:50:11.219-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T15:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:50:11.285706 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbd898b8e0, TRANSIENT_FAILURE
2019-04-29 09:50:11.285750 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:11.285762 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbd898b8e0, CONNECTING
2019-04-29 09:50:11.285769 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:11.285790 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:11.285842 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:11.286283 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:11.286307 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbd898b8e0, TRANSIENT_FAILURE
2019-04-29 09:50:11.286347 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:11.287981 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp [::1]:2379: connect: connection refused"; Reconnecting to {localhost:2379 0  <nil>}
2019-04-29 09:50:12.286192 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbd898b8e0, CONNECTING
2019-04-29 09:50:12.286249 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:12.286894 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:12.286931 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbd898b8e0, TRANSIENT_FAILURE
2019-04-29 09:50:12.286946 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:12.290409 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp [::1]:2379: connect: connection refused"; Reconnecting to {localhost:2379 0  <nil>}
2019-04-29 09:50:13.454771 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:13.454802 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:13.454832 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:13.454845 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbd898b8e0, SHUTDOWN
2019-04-29 09:50:13.454858 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:13.454867 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:13.454886 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:13.454895 I | etcdserver/api/v3rpc: grpc: addrConn.transportMonitor exits due to: context canceled
2019-04-29 09:50:13.454905 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7e88da20, CONNECTING
2019-04-29 09:50:13.454911 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:13.970549 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:13.970581 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7e88da20, TRANSIENT_FAILURE
2019-04-29 09:50:13.970592 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:14.053805 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp [::1]:2379: connect: connection refused"; Reconnecting to {localhost:2379 0  <nil>}
2019-04-29 09:50:14.455921 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7e88da20, CONNECTING
2019-04-29 09:50:14.455956 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:16.462417 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:16.462448 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:16.462515 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:16.462533 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:16.462734 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7e88da20, SHUTDOWN
2019-04-29 09:50:16.462773 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:16.462972 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:16.463052 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ec1bad0, CONNECTING
2019-04-29 09:50:16.463071 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:18.054233 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:18.054301 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:18.286578 I | etcdserver/api/v3rpc: transport: http2Server.HandleStreams failed to read frame: read tcp 127.0.0.1:2379->127.0.0.1:55678: use of closed network connection
2019-04-29 09:50:18.286637 I | etcdserver: skipped leadership transfer for single member cluster
2019-04-29 09:50:18.286733 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: operation was canceled"; Reconnecting to {localhost:2379 0  <nil>}
2019-04-29 09:50:18.286746 I | etcdserver/api/v3rpc: grpc: addrConn.transportMonitor exits due to: grpc: the connection is closing
--- PASS: TestDynamicNamespaceAdd (21.04s)
=== RUN   TestDynamicNamespaceDelete
2019-04-29 09:50:18.290925 I | embed: listening for peers on http://localhost:2380
2019-04-29 09:50:18.291816 I | embed: listening for client requests on localhost:2379
2019-04-29 09:50:18.293234 I | etcdserver: name = default
2019-04-29 09:50:18.293264 I | etcdserver: data dir = /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/etcd.dir341380111
2019-04-29 09:50:18.293278 I | etcdserver: member dir = /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/etcd.dir341380111/member
2019-04-29 09:50:18.293297 I | etcdserver: heartbeat = 100ms
2019-04-29 09:50:18.293316 I | etcdserver: election = 1000ms
2019-04-29 09:50:18.293328 I | etcdserver: snapshot count = 100000
2019-04-29 09:50:18.293341 I | etcdserver: advertise client URLs = http://localhost:2379
2019-04-29 09:50:18.293348 I | etcdserver: initial advertise peer URLs = http://localhost:2380
2019-04-29 09:50:18.293357 I | etcdserver: initial cluster = default=http://localhost:2380
2019-04-29 09:50:18.395505 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ec1bad0, READY
2019-04-29 09:50:18.395537 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: READY, 0xcbd8a40960
2019-04-29 09:50:18.395561 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:18.513729 I | etcdserver: starting member 8e9e05c52164694d in cluster cdf818194e3a8c32
2019-04-29 09:50:18.513784 I | raft: 8e9e05c52164694d became follower at term 0
2019-04-29 09:50:18.513799 I | raft: newRaft 8e9e05c52164694d [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]
2019-04-29 09:50:18.513830 I | raft: 8e9e05c52164694d became follower at term 1
2019-04-29 09:50:18.517429 W | auth: simple token is not cryptographically signed
2019-04-29 09:50:18.518285 I | etcdserver: starting server... [version: 3.2.10, cluster version: to_be_decided]
2019-04-29 09:50:18.518359 E | etcdserver: cannot monitor file descriptor usage (cannot get FDUsage on darwin)
2019-04-29 09:50:18.518843 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
2019-04-29 09:50:19.316491 I | raft: 8e9e05c52164694d is starting a new election at term 1
2019-04-29 09:50:19.316535 I | raft: 8e9e05c52164694d became candidate at term 2
2019-04-29 09:50:19.316549 I | raft: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 2
2019-04-29 09:50:19.316572 I | raft: 8e9e05c52164694d became leader at term 2
2019-04-29 09:50:19.316581 I | raft: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 2
2019-04-29 09:50:19.316738 I | etcdserver: setting up the initial cluster version to 3.2
2019-04-29 09:50:19.330440 N | etcdserver/membership: set the initial cluster version to 3.2
2019-04-29 09:50:19.330486 I | etcdserver: published {Name:default ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32
2019-04-29 09:50:19.330522 I | embed: ready to serve client requests
2019-04-29 09:50:19.330700 I | etcdserver/api/v3rpc: dialing to target with scheme: ""
2019-04-29 09:50:19.330712 I | etcdserver/api/v3rpc: could not get resolver for scheme: ""
2019-04-29 09:50:19.330827 N | embed: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged!
2019-04-29 09:50:19.331043 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: IDLE, 0xcc7f0ce120
2019-04-29 09:50:19.331057 I | etcdserver/api/v3rpc: dialing to target with scheme: ""
2019-04-29 09:50:19.331063 I | etcdserver/api/v3rpc: could not get resolver for scheme: ""
2019-04-29 09:50:19.331179 I | etcdserver/api/v3rpc: balancerWrapper: is pickfirst: false
2019-04-29 09:50:19.331193 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:19.331207 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:19.331317 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ee887e0, CONNECTING
2019-04-29 09:50:19.331328 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:19.334045 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ee887e0, READY
2019-04-29 09:50:19.334072 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: READY, 0xcc7f0ce120
2019-04-29 09:50:19.334209 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:50:19.735-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:50:19.735-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:50:19.735-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:50:19.735-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:50:19.736-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556619.7401352,"msg":"waiting for dynamic namespace registry initialization, if this takes a long time, make sure that a namespace is configured"}
{"level":"info","ts":1556556619.7403467,"msg":"initial namespace value received"}
2019-04-29T09:50:19.740-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:50:19.740-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:50:19.764-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestDynamicNamespaceDelete
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/dynamic_namespace_delete_test.go:95
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:50:20.778-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:50:20.778-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:50:20.778-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:50:20.778-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:50:20.778-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:50:20.779-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:50:20.779-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:50:20.832-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:50:20.833-0700	INFO	test data is now generated	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556621.014901,"msg":"successfully updated topology","numHosts":1}
{"level":"warn","ts":1556556621.3067508,"msg":"dynamic namespace registry received nil, skipping"}
2019-04-29T09:50:21.407-0700	INFO	deleted namespace key propagated from KV to testSetup	{"cache-policy": "recently_read"}
2019-04-29T09:50:21.408-0700	INFO	new namespace added to kv	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556621.4086795,"msg":"dynamic namespace registry updated to version","version":1}
2019-04-29T09:50:21.408-0700	INFO	received update from kv namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:50:21.408-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
{"level":"warn","ts":1556556621.4086838,"msg":"dynamic namespace registry received identical update, skipping"}
2019-04-29T09:50:21.422-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.(*dbNamespaceWatch).startWatch
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/namespace_watch.go:146
2019-04-29T09:50:21.422-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.(*dbNamespaceWatch).startWatch
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/namespace_watch.go:146
2019-04-29T09:50:21.422-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:50:21.422-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:50:21.422-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:50:21.508-0700	INFO	new namespace propagated from KV to testSetup	{"cache-policy": "recently_read"}
2019-04-29T09:50:21.705-0700	INFO	test data is now written	{"cache-policy": "recently_read"}
2019-04-29T09:50:21.950-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "tried to flush ns: testNs1, but did not have shard bootstrap times"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
{"level":"info","ts":1556556632.424349,"msg":"successfully updated topology","numHosts":1}
2019-04-29T09:50:33.721-0700	INFO	reading data from testSetup	{"cache-policy": "recently_read"}
2019-04-29T09:50:33.722-0700	INFO	data is verified	{"cache-policy": "recently_read"}
2019-04-29T09:50:33.724-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T15:00:00.000-0700", "error": "encountered errors when cleaning up index files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 15:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:50:33.724-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T15:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:50:33.827-0700	INFO	server is now down	{"cache-policy": "recently_read"}
2019-04-29 09:50:33.995280 I | etcdserver: skipped leadership transfer for single member cluster
2019-04-29 09:50:34.019492 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ee887e0, TRANSIENT_FAILURE
2019-04-29 09:50:34.019535 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:34.019560 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ee887e0, CONNECTING
2019-04-29 09:50:34.019571 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:34.019598 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:34.019614 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:34.019701 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:34.019722 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ee887e0, TRANSIENT_FAILURE
2019-04-29 09:50:34.019731 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:34.019779 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ec1bad0, TRANSIENT_FAILURE
2019-04-29 09:50:34.019797 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:34.019807 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ec1bad0, CONNECTING
2019-04-29 09:50:34.019814 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:34.019846 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:34.019857 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:34.020323 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:34.020396 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ec1bad0, TRANSIENT_FAILURE
2019-04-29 09:50:34.020412 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:34.021737 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp [::1]:2379: connect: connection refused"; Reconnecting to {localhost:2379 0  <nil>}
2019-04-29 09:50:34.332302 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:34.332337 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:34.332368 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:34.332390 I | etcdserver/api/v3rpc: grpc: addrConn.transportMonitor exits due to: context canceled
2019-04-29 09:50:34.332418 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:34.332437 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:34.332445 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ee887e0, SHUTDOWN
2019-04-29 09:50:34.332460 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:34.332471 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd2520d5e0, CONNECTING
2019-04-29 09:50:34.332478 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:34.333088 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:34.333120 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd2520d5e0, TRANSIENT_FAILURE
2019-04-29 09:50:34.333128 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:34.465275 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:34.465301 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:34.465324 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:34.465337 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:34.465359 I | etcdserver/api/v3rpc: grpc: addrConn.transportMonitor exits due to: context canceled
2019-04-29 09:50:34.465396 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc7ec1bad0, SHUTDOWN
2019-04-29 09:50:34.465430 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:34.465442 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd252849a0, CONNECTING
2019-04-29 09:50:34.465450 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:34.465457 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:35.333227 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd2520d5e0, CONNECTING
2019-04-29 09:50:35.333254 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:36.052982 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp [::1]:2379: connect: connection refused"; Reconnecting to {localhost:2379 0  <nil>}
2019-04-29 09:50:36.063514 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:36.063563 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd2520d5e0, TRANSIENT_FAILURE
2019-04-29 09:50:36.063577 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:36.328909 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp [::1]:2379: connect: connection refused"; Reconnecting to {localhost:2379 0  <nil>}
2019-04-29 09:50:36.415446 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:36.415528 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd252849a0, TRANSIENT_FAILURE
2019-04-29 09:50:36.415557 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:36.415983 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd252849a0, CONNECTING
2019-04-29 09:50:36.416009 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:36.417034 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:36.417115 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd252849a0, TRANSIENT_FAILURE
2019-04-29 09:50:36.417147 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:36.743274 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd2520d5e0, CONNECTING
2019-04-29 09:50:36.743301 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:37.332415 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:37.332438 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:37.332457 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:37.332470 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:37.332476 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd2520d5e0, SHUTDOWN
2019-04-29 09:50:37.332492 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:37.332518 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:37.332534 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25496550, CONNECTING
2019-04-29 09:50:37.332541 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:37.466042 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:37.466074 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:37.466116 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:37.466138 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:37.466148 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd252849a0, SHUTDOWN
2019-04-29 09:50:37.466164 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:37.466172 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:37.466183 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:50:37.466191 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25494d20, CONNECTING
2019-04-29 09:50:37.466200 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:37.983013 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:37.983099 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25494d20, TRANSIENT_FAILURE
2019-04-29 09:50:37.983120 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:38.054471 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:38.054500 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25496550, TRANSIENT_FAILURE
2019-04-29 09:50:38.054509 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:38.332757 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25496550, CONNECTING
2019-04-29 09:50:38.332795 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:38.466297 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25494d20, CONNECTING
2019-04-29 09:50:38.466332 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:40.332608 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:40.332637 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:40.332663 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:40.332675 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:40.332690 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:40.332698 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25496550, SHUTDOWN
2019-04-29 09:50:40.332707 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:40.332715 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd2570f570, CONNECTING
2019-04-29 09:50:40.332722 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:40.466146 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:40.466177 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:40.466204 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:40.466218 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:40.466227 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25494d20, SHUTDOWN
2019-04-29 09:50:40.466236 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:40.466286 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:40.466300 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25736300, CONNECTING
2019-04-29 09:50:40.466326 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:41.018935 I | etcdserver/api/v3rpc: transport: http2Server.HandleStreams failed to read frame: read tcp 127.0.0.1:2379->127.0.0.1:57467: use of closed network connection
2019-04-29 09:50:41.019117 I | etcdserver/api/v3rpc: transport: http2Server.HandleStreams failed to read frame: read tcp 127.0.0.1:2379->127.0.0.1:57695: use of closed network connection
2019-04-29 09:50:41.019129 I | etcdserver: skipped leadership transfer for single member cluster
2019-04-29 09:50:41.019187 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: operation was canceled"; Reconnecting to {localhost:2379 0  <nil>}
2019-04-29 09:50:41.019196 I | etcdserver/api/v3rpc: grpc: addrConn.transportMonitor exits due to: grpc: the connection is closing
--- PASS: TestDynamicNamespaceDelete (22.73s)
=== RUN   TestFetchTaggedQuorumNormalOnlyOneUp
2019-04-29 09:50:42.049782 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:42.049808 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:43.354456 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:43.354753 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:43.354810 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:43.354849 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:43.356729 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd2570f570, SHUTDOWN
2019-04-29 09:50:43.356791 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:43.356821 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdd449a6c0, CONNECTING
2019-04-29 09:50:43.356860 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:43.356939 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:43.468116 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:43.468321 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:43.468410 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:43.468485 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:43.469783 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcd25736300, SHUTDOWN
2019-04-29 09:50:43.469894 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:43.469928 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcda4ad7920, CONNECTING
2019-04-29 09:50:43.469957 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:43.470029 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:43.999811 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:43.999936 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcda4ad7920, TRANSIENT_FAILURE
2019-04-29 09:50:44.000001 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:44.074588 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:44.074663 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:44.102554 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:44.102628 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdd449a6c0, TRANSIENT_FAILURE
2019-04-29 09:50:44.102661 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:44.399421 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdd449a6c0, CONNECTING
2019-04-29 09:50:44.399547 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:44.490930 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcda4ad7920, CONNECTING
2019-04-29 09:50:44.491025 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:50:45.550-0700	INFO	starting server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556645.550424,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556645.550492,"msg":"initial topology / placement value received"}
2019-04-29T09:50:45.550-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:50:45.550-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:50:45.550-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:50:45.552-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:50:45.552-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:50:45.552-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 0, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:50:45.609-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumNormalOnlyOneUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:63
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:50:46.355413 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:46.355962 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:46.356061 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:46.356218 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:46.357297 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdd449a6c0, SHUTDOWN
2019-04-29 09:50:46.357334 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:46.357414 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcde9948330, CONNECTING
2019-04-29 09:50:46.357474 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:46.357557 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:46.468229 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:46.468264 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:46.468306 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:46.468320 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:46.469357 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcda4ad7920, SHUTDOWN
2019-04-29 09:50:46.469562 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:46.469711 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcde9948b50, CONNECTING
2019-04-29 09:50:46.469744 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:46.469806 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:48.042055 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:48.042195 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:48.152073 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:48.152102 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:49.356557 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:49.356620 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:49.356731 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:49.356782 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:49.358609 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcde9948330, SHUTDOWN
2019-04-29 09:50:49.358635 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:49.358660 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf1d4a680, CONNECTING
2019-04-29 09:50:49.358668 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:49.358701 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:50:49.470-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9000"}
2019-04-29T09:50:49.471-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9002"}
2019-04-29T09:50:49.472-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9001"}
2019-04-29T09:50:49.472-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9003"}
2019-04-29T09:50:49.472-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:50:49.473-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:50:49.473-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:50:49.473-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:50:49.473-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:50:49.474-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:50:49.474-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:50:49.474-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:50:49.474-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:50:49.474-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:50:49.474-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:50:49.474-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:50:49.475-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:50:49.475-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:50:49.475-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:50:49.475-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:50:49.475-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:50:49.476-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:50:49.476-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29 09:50:49.476585 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:49.476604 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:49.476644 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:49.476669 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:49.477180 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcde9948b50, SHUTDOWN
2019-04-29 09:50:49.477197 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:49.477233 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf22b9290, CONNECTING
2019-04-29 09:50:49.477243 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:49.477275 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:49.996343 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:49.996514 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf22b9290, TRANSIENT_FAILURE
2019-04-29 09:50:49.996789 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:50.002710 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:50.002767 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:50.086185 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:50.086361 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf1d4a680, TRANSIENT_FAILURE
2019-04-29 09:50:50.086443 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:50.112056 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:50.112082 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:50.289729 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:50.289910 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:50.357883 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf1d4a680, CONNECTING
2019-04-29 09:50:50.357927 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:50.358081 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:50.358128 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf1d4a680, TRANSIENT_FAILURE
2019-04-29 09:50:50.358165 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:50.478955 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf22b9290, CONNECTING
2019-04-29 09:50:50.479013 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:50.479161 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:50.479200 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf22b9290, TRANSIENT_FAILURE
2019-04-29 09:50:50.479240 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29T09:50:50.757-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29 09:50:51.785499 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf1d4a680, CONNECTING
2019-04-29 09:50:51.785569 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:52.007183 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:52.007280 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf1d4a680, TRANSIENT_FAILURE
2019-04-29 09:50:52.007306 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:52.371116 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf22b9290, CONNECTING
2019-04-29 09:50:52.371232 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:52.378513 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:52.379555 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf22b9290, TRANSIENT_FAILURE
2019-04-29 09:50:52.379580 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:52.379766 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:52.379861 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:52.383422 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:52.384764 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:52.386446 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:50:52.387292 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf1d4a680, SHUTDOWN
2019-04-29 09:50:52.387401 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:52.387519 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7b60, CONNECTING
2019-04-29 09:50:52.387599 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:52.387676 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:52.389206 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:52.389409 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7b60, TRANSIENT_FAILURE
2019-04-29 09:50:52.389522 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:52.481828 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:52.481889 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:52.481919 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:52.481942 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:52.482851 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:50:52.482936 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf22b9290, SHUTDOWN
2019-04-29 09:50:52.482963 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:52.483050 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7ff0, CONNECTING
2019-04-29 09:50:52.483079 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:52.483136 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:52.483256 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:52.483309 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7ff0, TRANSIENT_FAILURE
2019-04-29 09:50:52.483321 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
{"level":"info","ts":1556556652.888708,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:50:53.400191 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7b60, CONNECTING
2019-04-29 09:50:53.400287 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:53.454943 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:53.455077 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7b60, TRANSIENT_FAILURE
2019-04-29 09:50:53.455143 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:53.556223 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7ff0, CONNECTING
2019-04-29 09:50:53.556248 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:53.557012 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:53.557089 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7ff0, TRANSIENT_FAILURE
2019-04-29 09:50:53.557147 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:53.883646 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:53.883791 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:54.940567 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7ff0, CONNECTING
2019-04-29 09:50:54.940971 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:55.389995 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:55.390045 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:55.390073 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:55.390192 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:55.390889 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7b60, CONNECTING
2019-04-29 09:50:55.390915 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:55.390943 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7b60, SHUTDOWN
2019-04-29 09:50:55.390951 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:55.390967 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d44ef0, CONNECTING
2019-04-29 09:50:55.390975 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:55.513539 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:55.520240 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:55.520275 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:55.520330 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:55.520353 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:55.560217 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcdf94a7ff0, SHUTDOWN
2019-04-29 09:50:55.560265 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:55.560281 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d453c0, CONNECTING
2019-04-29 09:50:55.560299 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:55.560339 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:55.976922 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:55.976991 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d44ef0, TRANSIENT_FAILURE
2019-04-29 09:50:55.977026 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:55.984570 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:55.984746 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d453c0, TRANSIENT_FAILURE
2019-04-29 09:50:55.984824 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:56.202629 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:56.202764 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:56.235890 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:56.236527 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:56.499549 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d44ef0, CONNECTING
2019-04-29 09:50:56.500074 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:56.500333 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:56.500428 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d44ef0, TRANSIENT_FAILURE
2019-04-29 09:50:56.500510 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:56.563927 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d453c0, CONNECTING
2019-04-29 09:50:56.564050 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:56.586221 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:56.586306 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d453c0, TRANSIENT_FAILURE
2019-04-29 09:50:56.586319 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
{"level":"info","ts":1556556657.6114821,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:50:58.012334 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d44ef0, CONNECTING
2019-04-29 09:50:58.012706 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:58.049426 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:58.056975 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d44ef0, TRANSIENT_FAILURE
2019-04-29 09:50:58.057421 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:58.369017 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d453c0, CONNECTING
2019-04-29 09:50:58.372259 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:58.379324 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:58.379380 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d453c0, TRANSIENT_FAILURE
2019-04-29 09:50:58.379393 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:58.384269 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:58.384295 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:50:58.459259 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:58.459296 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:58.459684 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:58.460031 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:58.462267 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:50:58.462302 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d44ef0, SHUTDOWN
2019-04-29 09:50:58.462441 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:58.462524 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce5311f170, CONNECTING
2019-04-29 09:50:58.462602 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:58.462791 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:58.634225 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:58.634368 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce5311f170, TRANSIENT_FAILURE
2019-04-29 09:50:58.634452 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:50:58.662880 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:50:58.663416 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:50:58.664511 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:58.664718 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:50:58.668240 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:50:58.668461 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce33d453c0, SHUTDOWN
2019-04-29 09:50:58.668579 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:58.668941 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce534a6820, CONNECTING
2019-04-29 09:50:58.669110 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:58.669292 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:50:58.833845 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:58.834041 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce534a6820, TRANSIENT_FAILURE
2019-04-29 09:50:58.834114 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:50:59.463606 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce5311f170, CONNECTING
2019-04-29 09:50:59.463892 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:50:59.669885 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce534a6820, CONNECTING
2019-04-29 09:50:59.669921 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:50:59.999678 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:50:59.999738 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce534a6820, TRANSIENT_FAILURE
2019-04-29 09:50:59.999750 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:00.007979 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:00.008203 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce5311f170, TRANSIENT_FAILURE
2019-04-29 09:51:00.008272 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:01.421251 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce5311f170, CONNECTING
2019-04-29 09:51:01.421288 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:01.461894 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:01.461931 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:01.461977 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:01.462001 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:01.463325 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce5311f170, SHUTDOWN
2019-04-29 09:51:01.463351 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:01.463439 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce6a4cab30, CONNECTING
2019-04-29 09:51:01.463465 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:01.463547 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:01.699001 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:01.699035 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:01.699077 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:01.699116 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:01.702003 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:01.730189 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce534a6820, CONNECTING
2019-04-29 09:51:01.730315 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:01.730473 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce534a6820, SHUTDOWN
2019-04-29 09:51:01.730484 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:01.730507 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce6a4cb090, CONNECTING
2019-04-29 09:51:01.730516 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:02.060965 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:02.061102 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:02.061275 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:02.061295 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:02.083704 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:02.083762 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce6a4cab30, TRANSIENT_FAILURE
2019-04-29 09:51:02.083850 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:02.090774 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:02.090869 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce6a4cb090, TRANSIENT_FAILURE
2019-04-29 09:51:02.090885 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:02.594899 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce6a4cab30, CONNECTING
2019-04-29 09:51:02.594982 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:02.703693 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce6a4cb090, CONNECTING
2019-04-29 09:51:02.703946 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
{"level":"info","ts":1556556663.895498,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:51:04.549569 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:04.549632 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:04.550730 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:04.550807 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:04.552760 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce6a4cab30, SHUTDOWN
2019-04-29 09:51:04.553071 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:04.553408 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce83b83b20, CONNECTING
2019-04-29 09:51:04.553496 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:04.553928 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:04.701826 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:04.702007 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:04.702102 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:04.702160 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:04.703227 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce6a4cb090, SHUTDOWN
2019-04-29 09:51:04.703324 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:04.703404 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce84895750, CONNECTING
2019-04-29 09:51:04.703417 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:04.703467 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:06.565107 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:06.565136 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:06.565845 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:06.565865 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:07.558100 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:07.558136 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:07.558251 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:07.558270 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:07.561826 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce83b83b20, SHUTDOWN
2019-04-29 09:51:07.561979 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:07.562064 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89af35b0, CONNECTING
2019-04-29 09:51:07.574611 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:07.589709 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:07.710628 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:07.710665 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:07.710701 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:07.710718 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:07.718535 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce84895750, SHUTDOWN
2019-04-29 09:51:07.722751 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:07.722817 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89b71e50, CONNECTING
2019-04-29 09:51:07.722853 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:07.722925 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:07.976027 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:07.976123 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89af35b0, TRANSIENT_FAILURE
2019-04-29 09:51:07.976157 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:08.038014 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:08.038216 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89b71e50, TRANSIENT_FAILURE
2019-04-29 09:51:08.038263 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:08.301549 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:08.301576 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:08.439801 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:08.439831 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:08.575029 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89af35b0, CONNECTING
2019-04-29 09:51:08.575166 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:08.575317 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:08.575413 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89af35b0, TRANSIENT_FAILURE
2019-04-29 09:51:08.575443 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:08.712166 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89b71e50, CONNECTING
2019-04-29 09:51:08.712205 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:10.135203 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:10.135264 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89b71e50, TRANSIENT_FAILURE
2019-04-29 09:51:10.135300 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:10.570704 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89af35b0, CONNECTING
2019-04-29 09:51:10.570842 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:10.880961 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:10.881124 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:10.881256 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:10.881312 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:11.024935 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89b71e50, CONNECTING
2019-04-29 09:51:11.025031 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:11.025069 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89b71e50, SHUTDOWN
2019-04-29 09:51:11.025140 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:11.025159 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce953a0d80, CONNECTING
2019-04-29 09:51:11.025178 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:11.025229 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:11.056294 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:11.056373 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:11.056410 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:11.056461 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:11.057975 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce89af35b0, SHUTDOWN
2019-04-29 09:51:11.058061 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:11.058128 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce95387490, CONNECTING
2019-04-29 09:51:11.058150 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:11.058242 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:11.957951 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:11.958215 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce95387490, TRANSIENT_FAILURE
2019-04-29 09:51:11.958271 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:11.958861 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:11.958887 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:12.107495 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce95387490, CONNECTING
2019-04-29 09:51:12.107532 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
{"level":"info","ts":1556556673.3137562,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:51:13.883332 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:13.883372 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:13.883456 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:13.883827 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:13.884848 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce953a0d80, SHUTDOWN
2019-04-29 09:51:13.884898 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:13.884925 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb678bff0, CONNECTING
2019-04-29 09:51:13.884947 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:13.884994 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:14.057405 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:14.057572 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:14.057723 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:14.057958 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:14.059588 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xce95387490, SHUTDOWN
2019-04-29 09:51:14.059633 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:14.059690 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb7d4ae30, CONNECTING
2019-04-29 09:51:14.059702 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:14.059755 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:14.060315 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:14.060364 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb7d4ae30, TRANSIENT_FAILURE
2019-04-29 09:51:14.060384 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:14.066207 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:14.066262 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb678bff0, TRANSIENT_FAILURE
2019-04-29 09:51:14.066273 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:14.100980 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:14.101026 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:14.757981 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:14.758038 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:14.760238 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:14.760299 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:14.885545 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb678bff0, CONNECTING
2019-04-29 09:51:14.885654 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:14.885742 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:14.885766 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb678bff0, TRANSIENT_FAILURE
2019-04-29 09:51:14.885782 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:15.220946 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb7d4ae30, CONNECTING
2019-04-29 09:51:15.220983 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:15.233222 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:15.233265 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb7d4ae30, TRANSIENT_FAILURE
2019-04-29 09:51:15.233277 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:16.549259 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb7d4ae30, CONNECTING
2019-04-29 09:51:16.549597 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:16.695103 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb678bff0, CONNECTING
2019-04-29 09:51:16.695132 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:16.882870 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:16.882919 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:16.882945 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:16.882968 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:16.884274 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb678bff0, SHUTDOWN
2019-04-29 09:51:16.884302 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:16.884334 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebc4b9f90, CONNECTING
2019-04-29 09:51:16.884342 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:16.884400 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:17.057856 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:17.057978 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:17.058014 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:17.058042 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:17.059112 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xceb7d4ae30, SHUTDOWN
2019-04-29 09:51:17.059168 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:17.059208 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebc57d4e0, CONNECTING
2019-04-29 09:51:17.059219 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:17.059256 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:18.090184 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:18.090361 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:51:19.396-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:51:19.397-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:51:19.885191 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:19.885227 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:19.885266 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:19.885283 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:19.886012 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebc4b9f90, SHUTDOWN
2019-04-29 09:51:19.886043 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:19.886072 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcb90f40, CONNECTING
2019-04-29 09:51:19.886081 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:19.886111 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:19.999344 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:19.999383 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcb90f40, TRANSIENT_FAILURE
2019-04-29 09:51:19.999405 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:20.073588 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:20.073645 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:20.073670 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:20.073696 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:20.074924 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebc57d4e0, SHUTDOWN
2019-04-29 09:51:20.074965 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:20.074995 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcd825f0, CONNECTING
2019-04-29 09:51:20.075005 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:20.075043 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:20.903605 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcb90f40, CONNECTING
2019-04-29 09:51:20.903634 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:22.928880 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:22.928912 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:22.928987 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:22.929034 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:22.932826 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcb90f40, SHUTDOWN
2019-04-29 09:51:22.932851 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:22.932864 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcfbc780, CONNECTING
2019-04-29 09:51:22.932878 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:22.932914 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:23.080271 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:23.080318 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:23.080345 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:23.080368 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:23.081148 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcd825f0, SHUTDOWN
2019-04-29 09:51:23.081183 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:23.081211 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcff6930, CONNECTING
2019-04-29 09:51:23.081229 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:23.081265 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
--- PASS: TestFetchTaggedQuorumNormalOnlyOneUp (42.10s)
=== RUN   TestFetchTaggedQuorumNormalOnlyTwoUp
2019-04-29 09:51:24.033563 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:24.033588 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:24.167798 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:24.167864 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcfbc780, TRANSIENT_FAILURE
2019-04-29 09:51:24.167887 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:24.207813 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcfbc780, CONNECTING
2019-04-29 09:51:24.207852 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:24.210320 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:24.210355 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcfbc780, TRANSIENT_FAILURE
2019-04-29 09:51:24.210376 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:25.934062 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:25.934094 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:25.934663 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:25.934741 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:25.936028 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:51:25.936171 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcfbc780, SHUTDOWN
2019-04-29 09:51:25.936191 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:25.936198 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:25.936218 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcefbb50e70, CONNECTING
2019-04-29 09:51:25.936228 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:26.083170 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:26.083234 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:26.083281 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:26.083307 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:26.083946 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcebcff6930, SHUTDOWN
2019-04-29 09:51:26.083961 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:26.083983 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcefb099ea0, CONNECTING
2019-04-29 09:51:26.083991 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:26.084024 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:28.627868 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:28.627953 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcefb099ea0, TRANSIENT_FAILURE
2019-04-29 09:51:28.627995 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:28.635015 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:28.635300 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcefbb50e70, TRANSIENT_FAILURE
2019-04-29 09:51:28.635444 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:28.645432 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcefbb50e70, CONNECTING
2019-04-29 09:51:28.645500 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:28.647733 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcefb099ea0, CONNECTING
2019-04-29 09:51:28.647760 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:29.029865 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:29.030384 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:29.030847 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:29.030968 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:29.035138 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcefbb50e70, SHUTDOWN
2019-04-29 09:51:29.035282 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:29.035350 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf36ef00d0, CONNECTING
2019-04-29 09:51:29.035383 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:29.035493 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:29.091071 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:29.091478 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:29.091819 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:29.092243 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:29.095081 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcefb099ea0, SHUTDOWN
2019-04-29 09:51:29.095172 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:29.095295 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf343eb440, CONNECTING
2019-04-29 09:51:29.095313 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:29.095577 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:30.001708 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:30.001852 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf343eb440, TRANSIENT_FAILURE
2019-04-29 09:51:30.001967 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:30.300126 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf343eb440, CONNECTING
2019-04-29 09:51:30.300175 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:32.037419 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:32.037447 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:32.065714 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:32.065899 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:32.067531 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:32.067561 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:32.067934 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf36ef00d0, SHUTDOWN
2019-04-29 09:51:32.067963 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:32.067983 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf60771210, CONNECTING
2019-04-29 09:51:32.067999 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:32.068034 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:32.078714 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:32.079024 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf60771210, TRANSIENT_FAILURE
2019-04-29 09:51:32.079065 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:32.098443 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:32.098473 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:32.098522 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:32.098541 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:32.099269 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf343eb440, SHUTDOWN
2019-04-29 09:51:32.099520 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:32.099619 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf60771f00, CONNECTING
2019-04-29 09:51:32.099641 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:32.099721 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:32.108547 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:32.108684 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf60771f00, TRANSIENT_FAILURE
2019-04-29 09:51:32.108735 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:33.107608 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf60771f00, CONNECTING
2019-04-29 09:51:33.107637 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:33.107664 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf60771210, CONNECTING
2019-04-29 09:51:33.107674 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:51:33.788-0700	INFO	starting server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556693.7890282,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556693.7890673,"msg":"initial topology / placement value received"}
2019-04-29T09:51:33.789-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:51:33.789-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:51:33.789-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:51:33.804-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:51:33.804-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:51:33.804-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 0, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:51:33.852-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumNormalOnlyTwoUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:89
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:51:35.066837 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:35.066898 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:35.066946 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:35.066972 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:35.067530 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf60771210, SHUTDOWN
2019-04-29 09:51:35.067563 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:35.067577 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf828be630, CONNECTING
2019-04-29 09:51:35.067593 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:35.067647 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:35.100858 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:35.100923 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:35.100972 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:35.100989 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:35.101155 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf60771f00, SHUTDOWN
2019-04-29 09:51:35.101178 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:35.101210 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf82b0aa50, CONNECTING
2019-04-29 09:51:35.101219 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:35.101505 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:36.909306 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:36.909738 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:37.945412 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:37.945449 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:38.078749 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:38.078808 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:38.078839 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:38.078865 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:38.079557 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf828be630, SHUTDOWN
2019-04-29 09:51:38.079574 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:38.079598 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a0c8350, CONNECTING
2019-04-29 09:51:38.079608 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:38.079644 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:38.102423 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:38.102482 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:38.102515 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:38.102553 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:38.103235 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf82b0aa50, SHUTDOWN
2019-04-29 09:51:38.103278 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:38.103296 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a59d3a0, CONNECTING
2019-04-29 09:51:38.103306 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:38.103339 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:51:38.141-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9000"}
2019-04-29T09:51:38.143-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9002"}
2019-04-29T09:51:38.145-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9001"}
2019-04-29T09:51:38.148-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9003"}
2019-04-29T09:51:38.150-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:51:38.152-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:51:38.152-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:51:38.152-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:51:38.152-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:51:38.153-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:51:38.153-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:51:38.153-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:51:38.153-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:51:38.153-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:51:38.156-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:51:38.157-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:51:38.159-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:51:38.159-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:51:38.159-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:51:38.159-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:51:38.159-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:51:38.160-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:51:38.160-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:51:39.310-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:51:39.312-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:51:39.313-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:51:39.313-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:51:39.313-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:51:39.315-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:51:39.315-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:51:39.315-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 1, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:51:39.354-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumNormalOnlyTwoUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:90
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:51:40.155451 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:40.155580 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a0c8350, TRANSIENT_FAILURE
2019-04-29 09:51:40.155601 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:40.158461 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:40.158489 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:40.196229 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a0c8350, CONNECTING
2019-04-29 09:51:40.196254 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:40.208585 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:40.208797 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a59d3a0, TRANSIENT_FAILURE
2019-04-29 09:51:40.208909 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:40.217533 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:40.217674 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a0c8350, TRANSIENT_FAILURE
2019-04-29 09:51:40.217767 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:40.241153 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a59d3a0, CONNECTING
2019-04-29 09:51:40.241179 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:40.248894 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:40.248984 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a59d3a0, TRANSIENT_FAILURE
2019-04-29 09:51:40.248999 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:40.494958 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:40.495101 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:40.495198 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:40.495224 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:40.940014 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:40.940123 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:41.175354 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:41.175415 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:41.175451 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:41.175487 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:41.176224 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:41.176244 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:41.176287 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:41.176305 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:41.176929 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:51:41.176975 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a0c8350, SHUTDOWN
2019-04-29 09:51:41.176988 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:41.177009 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf900dc8e0, CONNECTING
2019-04-29 09:51:41.177020 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:41.177053 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:41.177083 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:51:41.177097 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf8a59d3a0, SHUTDOWN
2019-04-29 09:51:41.177127 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:41.177149 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf900dcaa0, CONNECTING
2019-04-29 09:51:41.177158 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:41.177185 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:44.305550 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:44.305575 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:44.305643 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:44.305672 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:44.306373 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:44.306463 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:44.306509 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:44.306559 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:44.310992 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf900dc8e0, SHUTDOWN
2019-04-29 09:51:44.311092 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:44.311128 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63e20, CONNECTING
2019-04-29 09:51:44.311157 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:44.311216 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:44.311246 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf900dcaa0, SHUTDOWN
2019-04-29 09:51:44.311286 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:44.311303 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63fe0, CONNECTING
2019-04-29 09:51:44.311324 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:44.311373 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:44.369255 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:44.369309 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63fe0, TRANSIENT_FAILURE
2019-04-29 09:51:44.369320 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:44.369415 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:44.369450 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63e20, TRANSIENT_FAILURE
2019-04-29 09:51:44.369463 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29T09:51:44.916-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9005"}
2019-04-29T09:51:44.917-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9007"}
2019-04-29T09:51:44.919-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9006"}
2019-04-29T09:51:44.921-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9008"}
2019-04-29T09:51:44.921-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:51:44.922-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:51:44.926-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:51:44.927-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:51:44.927-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:51:44.928-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:51:44.928-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:51:44.928-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:51:44.928-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:51:44.928-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:51:44.928-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:51:44.929-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:51:44.929-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:51:44.929-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:51:44.931-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:51:44.931-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:51:44.931-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:51:44.938-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:51:44.944-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9009", "error": "listen tcp 127.0.0.1:9009: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29 09:51:45.312596 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63e20, CONNECTING
2019-04-29 09:51:45.312627 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:45.312654 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63fe0, CONNECTING
2019-04-29 09:51:45.312670 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:46.067811 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:46.067990 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63e20, TRANSIENT_FAILURE
2019-04-29 09:51:46.068010 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:46.069256 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:46.069308 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63fe0, TRANSIENT_FAILURE
2019-04-29 09:51:46.069323 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29T09:51:46.173-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29 09:51:46.611205 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63fe0, CONNECTING
2019-04-29 09:51:46.611282 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:46.611452 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:46.611719 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63fe0, TRANSIENT_FAILURE
2019-04-29 09:51:46.611734 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:47.151535 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63e20, CONNECTING
2019-04-29 09:51:47.151557 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:47.312239 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:47.312314 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:47.312415 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:47.312500 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:47.313910 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:47.313943 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:47.313984 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:47.314001 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:47.315214 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63e20, SHUTDOWN
2019-04-29 09:51:47.315248 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:47.315262 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe5e0, CONNECTING
2019-04-29 09:51:47.315276 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:47.315318 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:47.315339 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:51:47.315366 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcf95e63fe0, SHUTDOWN
2019-04-29 09:51:47.315374 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:47.315392 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe7a0, CONNECTING
2019-04-29 09:51:47.315400 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:47.315423 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:48.068332 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.068360 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.068393 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe5e0, TRANSIENT_FAILURE
2019-04-29 09:51:48.068404 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:48.068422 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe7a0, TRANSIENT_FAILURE
2019-04-29 09:51:48.068432 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:48.314340 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe5e0, CONNECTING
2019-04-29 09:51:48.314490 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:48.314654 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.314679 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe5e0, TRANSIENT_FAILURE
2019-04-29 09:51:48.314792 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:48.573590 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.573616 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:48.573819 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.573833 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:48.596270 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.596399 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:48.601373 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe7a0, CONNECTING
2019-04-29 09:51:48.601481 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:48.601852 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.601944 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe7a0, TRANSIENT_FAILURE
2019-04-29 09:51:48.601965 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:48.648390 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.648452 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:48.648993 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:48.649074 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
{"level":"info","ts":1556556708.969015,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:51:49.035201 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:49.035227 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:49.806320 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe5e0, CONNECTING
2019-04-29 09:51:49.941731 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:49.965751 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:49.966007 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe5e0, TRANSIENT_FAILURE
2019-04-29 09:51:49.966202 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:50.239487 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe7a0, CONNECTING
2019-04-29 09:51:50.239519 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:50.287011 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:50.287041 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:51.216898 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:51.216975 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:51.217024 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:51.217056 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:51.218358 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:51.218424 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:51.218449 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:51.218471 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:51.230012 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:51:51.230077 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe5e0, SHUTDOWN
2019-04-29 09:51:51.230101 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:51.230117 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfcb128820, CONNECTING
2019-04-29 09:51:51.230131 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:51.230167 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:51.230183 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfa48fe7a0, SHUTDOWN
2019-04-29 09:51:51.230202 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:51.230213 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfcb1289e0, CONNECTING
2019-04-29 09:51:51.230226 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:51.230620 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:52.358297 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:52.358348 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfcb128820, TRANSIENT_FAILURE
2019-04-29 09:51:52.358360 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:52.358433 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:52.358449 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfcb1289e0, TRANSIENT_FAILURE
2019-04-29 09:51:52.358480 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:52.406011 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:52.406053 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:52.410858 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfcb128820, CONNECTING
2019-04-29 09:51:52.410902 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:52.412226 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfcb1289e0, CONNECTING
2019-04-29 09:51:52.437495 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
{"level":"info","ts":1556556713.525596,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:51:54.403640 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:54.403669 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:54.403701 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:54.403715 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:54.404437 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:54.404456 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:54.404486 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:54.404499 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:54.405104 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfcb128820, SHUTDOWN
2019-04-29 09:51:54.405133 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:54.405144 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af4f0, CONNECTING
2019-04-29 09:51:54.405158 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:54.405188 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:54.405202 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcfcb1289e0, SHUTDOWN
2019-04-29 09:51:54.405219 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:54.405228 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af6b0, CONNECTING
2019-04-29 09:51:54.405240 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:54.405262 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:54.503231 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:54.503269 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af6b0, TRANSIENT_FAILURE
2019-04-29 09:51:54.503292 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:54.503350 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:54.503381 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af4f0, TRANSIENT_FAILURE
2019-04-29 09:51:54.503392 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:54.633832 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:54.633854 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:54.633991 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:54.634110 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:55.102328 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:55.102443 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:55.123934 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:55.124150 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:51:55.409125 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af6b0, CONNECTING
2019-04-29 09:51:55.409176 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:55.409199 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af4f0, CONNECTING
2019-04-29 09:51:55.409226 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
{"level":"info","ts":1556556715.6045718,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:51:55.969210 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:55.969261 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af4f0, TRANSIENT_FAILURE
2019-04-29 09:51:55.969295 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:55.971206 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:55.971267 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af6b0, TRANSIENT_FAILURE
2019-04-29 09:51:55.971357 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:57.005690 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af6b0, CONNECTING
2019-04-29 09:51:57.005760 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:57.077984 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af4f0, CONNECTING
2019-04-29 09:51:57.078013 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:57.480809 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:57.480840 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:57.480877 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:57.480892 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:57.481734 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:51:57.481754 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:51:57.481802 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:57.481827 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:51:57.482584 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af4f0, SHUTDOWN
2019-04-29 09:51:57.482618 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:57.482631 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd05a780790, CONNECTING
2019-04-29 09:51:57.482638 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:57.482668 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:51:57.482693 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0031af6b0, SHUTDOWN
2019-04-29 09:51:57.482702 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:57.482720 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd05a780950, CONNECTING
2019-04-29 09:51:57.482728 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:51:57.482749 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
{"level":"info","ts":1556556717.7626312,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:51:57.944732 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:57.944789 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd05a780950, TRANSIENT_FAILURE
2019-04-29 09:51:57.944840 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:51:57.944850 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:51:57.944881 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd05a780790, TRANSIENT_FAILURE
2019-04-29 09:51:57.944900 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:51:58.527643 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd05a780790, CONNECTING
2019-04-29 09:51:58.527669 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:51:58.527744 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd05a780950, CONNECTING
2019-04-29 09:51:58.527769 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:00.483162 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:00.483203 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:00.483274 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:00.483451 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:00.488846 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:00.488876 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd05a780950, SHUTDOWN
2019-04-29 09:52:00.488896 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:00.488921 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:00.488929 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0623e8950, CONNECTING
2019-04-29 09:52:00.488938 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:00.488953 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:00.488980 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:00.489024 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd05a780790, SHUTDOWN
2019-04-29 09:52:00.489049 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:00.489077 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:00.489110 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:00.489146 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0623e8b10, CONNECTING
2019-04-29 09:52:00.489186 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:03.493530 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:03.493600 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:03.493632 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:03.493656 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:03.495096 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:03.495117 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:03.495207 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:03.495266 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:03.496624 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0623e8b10, SHUTDOWN
2019-04-29 09:52:03.496731 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:03.496904 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0630c9f00, CONNECTING
2019-04-29 09:52:03.496963 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:03.497044 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:03.497102 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0623e8950, SHUTDOWN
2019-04-29 09:52:03.497161 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:03.497207 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd063242180, CONNECTING
2019-04-29 09:52:03.497268 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:03.497348 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:52:03.822-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:52:03.822-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:52:03.966341 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:03.966493 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd063242180, TRANSIENT_FAILURE
2019-04-29 09:52:03.966555 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:03.967016 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:03.967102 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0630c9f00, TRANSIENT_FAILURE
2019-04-29 09:52:03.967299 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:04.536435 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd063242180, CONNECTING
2019-04-29 09:52:04.537693 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:04.617366 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0630c9f00, CONNECTING
2019-04-29 09:52:04.618293 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:52:05.672-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:52:05.673-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:52:06.499001 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:06.499166 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:06.499291 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:06.499483 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:06.500737 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:06.500846 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:06.500893 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:06.500924 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:06.501859 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd063242180, SHUTDOWN
2019-04-29 09:52:06.501897 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:06.501979 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd06399d130, CONNECTING
2019-04-29 09:52:06.502038 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:06.502116 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:06.502194 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0630c9f00, SHUTDOWN
2019-04-29 09:52:06.502208 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:06.502259 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd06399d2f0, CONNECTING
2019-04-29 09:52:06.502291 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:06.502335 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
--- PASS: TestFetchTaggedQuorumNormalOnlyTwoUp (45.74s)
=== RUN   TestFetchTaggedQuorumNormalAllUp
2019-04-29 09:52:09.502488 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:09.503043 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:09.503093 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:09.503129 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:09.505288 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:09.505390 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:09.505737 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:09.505793 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:09.507025 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd06399d2f0, SHUTDOWN
2019-04-29 09:52:09.507146 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:09.507227 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd06ebc7ec0, CONNECTING
2019-04-29 09:52:09.507287 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:09.507370 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:09.507459 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd06399d130, SHUTDOWN
2019-04-29 09:52:09.507473 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:09.507506 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd08c476080, CONNECTING
2019-04-29 09:52:09.507517 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:09.507542 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:09.954178 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:09.954326 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd06ebc7ec0, TRANSIENT_FAILURE
2019-04-29 09:52:09.954383 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:09.955271 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:09.955306 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd08c476080, TRANSIENT_FAILURE
2019-04-29 09:52:09.955328 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:10.505339 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd06ebc7ec0, CONNECTING
2019-04-29 09:52:10.505369 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:10.507292 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd08c476080, CONNECTING
2019-04-29 09:52:10.507318 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:12.506504 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:12.506649 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:12.506722 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:12.506832 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:12.508842 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:12.509015 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:12.509100 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:12.509294 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:12.530472 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd08c476080, SHUTDOWN
2019-04-29 09:52:12.531189 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:12.531272 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0a32a99a0, CONNECTING
2019-04-29 09:52:12.531318 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:12.531901 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:12.531959 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd06ebc7ec0, SHUTDOWN
2019-04-29 09:52:12.531970 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:12.532002 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0a32a9b60, CONNECTING
2019-04-29 09:52:12.532243 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:12.532311 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:17.292395 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:17.292576 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:22.570005 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:22.570075 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:22.570167 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:22.570201 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:22.570991 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:22.571014 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:22.571038 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:22.571332 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:25.204649 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:25.204714 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.423302 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.423538 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.424021 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.425387 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.425500 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.425626 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.425668 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0a32a99a0, SHUTDOWN
2019-04-29 09:52:43.426087 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:43.426239 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0d73dde20, CONNECTING
2019-04-29 09:52:43.426291 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:43.426392 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.426470 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:43.426493 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:43.426550 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0d73dde20, SHUTDOWN
2019-04-29 09:52:43.426623 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:43.445398 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0a32a9b60, SHUTDOWN
2019-04-29 09:52:43.445531 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:43.445605 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0d73ddfe0, CONNECTING
2019-04-29 09:52:43.445617 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:43.445670 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.445728 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:43.445743 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:43.445783 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0d73ddfe0, SHUTDOWN
2019-04-29 09:52:43.445793 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:43.445915 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.445934 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:43.446002 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.446118 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:43.446126 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:43.446139 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.446150 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:43.447166 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447291 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447372 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447431 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447506 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447581 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447613 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447639 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447685 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447718 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447749 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447770 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447858 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: operation timed out"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447899 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447908 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447935 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447943 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447952 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447958 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447966 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.447972 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:43.447980 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.448048 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.448065 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:43.448197 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.448215 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:43.448224 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:43.448236 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.448247 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:43.448784 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9676f9c80, CONNECTING
2019-04-29 09:52:43.448895 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:43.448911 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9676f9c80, SHUTDOWN
2019-04-29 09:52:43.448931 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:43.448940 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9676f9e20, CONNECTING
2019-04-29 09:52:43.448957 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:43.448983 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.449047 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:43.449645 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96770c310, CONNECTING
2019-04-29 09:52:43.449666 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:43.449679 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96770c310, SHUTDOWN
2019-04-29 09:52:43.449688 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:43.449700 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96770c4b0, CONNECTING
2019-04-29 09:52:43.449707 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:43.450180 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:52:43.450218 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:46.452133 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:46.452254 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:46.453001 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:46.453075 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:46.453993 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96770c4b0, SHUTDOWN
2019-04-29 09:52:46.454014 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:46.454066 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc7c37c85c0, CONNECTING
2019-04-29 09:52:46.454077 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:46.454095 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:46.454109 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:46.454138 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:46.454162 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:46.454219 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:46.454272 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9676f9e20, SHUTDOWN
2019-04-29 09:52:46.454292 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:46.454304 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc7c37c8810, CONNECTING
2019-04-29 09:52:46.454322 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:46.454354 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:49.454403 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:49.454527 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:49.454692 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:49.454776 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:49.455052 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc7c37c8810, SHUTDOWN
2019-04-29 09:52:49.455132 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:49.455196 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc4ac5b730, CONNECTING
2019-04-29 09:52:49.455237 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:49.455271 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:49.455306 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:49.455371 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:49.455432 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:49.456698 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:49.456826 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc7c37c85c0, SHUTDOWN
2019-04-29 09:52:49.456844 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:49.456859 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc4ad1c760, CONNECTING
2019-04-29 09:52:49.456883 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:49.456937 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:52.456035 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:52.456064 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:52.456090 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:52.456106 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:52.456592 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:52.456609 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:52.456628 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:52.456641 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:52.457024 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc4ac5b730, SHUTDOWN
2019-04-29 09:52:52.457039 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:52.457050 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9a3df70c0, CONNECTING
2019-04-29 09:52:52.457056 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:52.457075 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:52.457089 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcc4ad1c760, SHUTDOWN
2019-04-29 09:52:52.457097 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:52.457120 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9a3df7280, CONNECTING
2019-04-29 09:52:52.457127 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:52.457152 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:52:53.027-0700	INFO	starting server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556773.0281763,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556773.029526,"msg":"initial topology / placement value received"}
2019-04-29T09:52:53.029-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:52:53.029-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:52:53.029-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:52:53.038-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:52:53.038-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:52:53.038-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 0, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:52:53.107-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumNormalAllUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:116
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:52:54.926-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9000"}
2019-04-29T09:52:54.936-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9002"}
2019-04-29T09:52:54.938-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9001"}
2019-04-29T09:52:54.938-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9003"}
2019-04-29T09:52:54.939-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:54.939-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:52:54.939-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:54.939-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:52:54.940-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:54.940-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:54.940-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:54.940-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:52:54.940-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:54.940-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:54.940-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:54.940-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:52:54.941-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:54.941-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:54.941-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:54.941-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:52:54.942-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:54.942-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:52:54.942-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:52:54.970-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:52:54.970-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:52:54.970-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:52:54.970-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:52:54.970-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:52:54.973-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:52:54.973-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:52:54.973-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 1, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:52:55.026-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumNormalAllUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:117
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:52:55.482916 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:55.482953 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:55.482994 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:55.483021 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:55.483796 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:55.483817 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:55.483838 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:55.483851 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:55.484374 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9a3df7280, SHUTDOWN
2019-04-29 09:52:55.484394 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:55.484407 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbce92f0a0, CONNECTING
2019-04-29 09:52:55.484413 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:55.484452 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:55.484466 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9a3df70c0, SHUTDOWN
2019-04-29 09:52:55.484472 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:55.484479 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbce92f260, CONNECTING
2019-04-29 09:52:55.484485 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:55.484512 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:55.959206 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:55.959661 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbce92f0a0, TRANSIENT_FAILURE
2019-04-29 09:52:55.959739 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:55.959964 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:52:55.960026 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbce92f260, TRANSIENT_FAILURE
2019-04-29 09:52:55.960086 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:56.483402 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbce92f0a0, CONNECTING
2019-04-29 09:52:56.483450 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:52:56.484861 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbce92f260, CONNECTING
2019-04-29 09:52:56.484887 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:52:56.937-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9005"}
2019-04-29T09:52:56.939-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9007"}
2019-04-29T09:52:56.940-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9006"}
2019-04-29T09:52:56.940-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9008"}
2019-04-29T09:52:56.941-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:56.941-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9009", "error": "listen tcp 127.0.0.1:9009: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:52:56.941-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:56.941-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:52:56.941-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:56.941-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:56.942-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:56.942-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:52:56.942-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:56.942-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:56.942-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:56.942-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:52:56.942-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:56.942-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:56.943-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:56.943-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:52:56.943-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:56.943-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:52:56.943-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:52:57.162-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:52:57.163-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:52:57.163-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:52:57.163-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:52:57.163-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:52:57.165-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:52:57.165-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:52:57.165-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 2, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:52:57.200-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 2, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumNormalAllUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:118
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:52:58.484256 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:58.484290 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:58.484313 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:58.484325 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:58.484675 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:52:58.484693 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:52:58.484714 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:58.484726 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:52:58.485240 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbce92f260, SHUTDOWN
2019-04-29 09:52:58.485255 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:58.485268 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:52:58.485276 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:52:58.485285 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xca9cc6afb0, CONNECTING
2019-04-29 09:52:58.485291 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:52:58.485298 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xcbce92f0a0, SHUTDOWN
2019-04-29 09:52:58.485304 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:52:58.485312 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc89911eb20, CONNECTING
2019-04-29 09:52:58.485316 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:52:58.958-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 2, "address": "127.0.0.1:9010"}
2019-04-29T09:52:58.959-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 2, "address": "127.0.0.1:9012"}
2019-04-29T09:52:58.961-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 2, "address": "127.0.0.1:9011"}
2019-04-29T09:52:58.962-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 2, "address": "127.0.0.1:9013"}
2019-04-29T09:52:58.962-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 2, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:58.963-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 2, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:58.963-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 2, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:52:58.965-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 2, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:58.965-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 2, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:58.965-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 2, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:58.965-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 2, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:52:58.965-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 2, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:58.965-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 2, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:58.966-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 2, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:58.966-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 2, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:52:58.966-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 2, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:58.966-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 2, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:52:58.966-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 2, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:52:58.966-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 2, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:52:58.966-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 2, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:52:58.966-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 2, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:52:58.966-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 2, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:52:58.989-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29 09:53:00.340225 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:00.340273 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:00.343686 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:00.343935 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:00.481508 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:00.481538 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:00.481548 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:00.481559 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:00.662547 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:00.662591 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc89911eb20, TRANSIENT_FAILURE
2019-04-29 09:53:00.662605 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:00.662666 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:00.662684 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xca9cc6afb0, TRANSIENT_FAILURE
2019-04-29 09:53:00.662693 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:00.664174 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc89911eb20, CONNECTING
2019-04-29 09:53:00.664265 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:00.664589 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xca9cc6afb0, CONNECTING
2019-04-29 09:53:00.664631 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:00.664686 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:00.664723 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc89911eb20, TRANSIENT_FAILURE
2019-04-29 09:53:00.664731 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:00.664836 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:00.664857 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xca9cc6afb0, TRANSIENT_FAILURE
2019-04-29 09:53:00.664864 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:01.485962 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:01.486095 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:01.486137 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:01.486166 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:01.488214 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:01.488392 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:01.488424 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:01.488439 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:01.489672 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:53:01.489697 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc89911eb20, SHUTDOWN
2019-04-29 09:53:01.489709 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:01.489721 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11ef00, CONNECTING
2019-04-29 09:53:01.489730 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:01.489752 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:01.489773 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:53:01.489785 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xca9cc6afb0, SHUTDOWN
2019-04-29 09:53:01.489793 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:01.489801 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11f1d0, CONNECTING
2019-04-29 09:53:01.489810 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:01.489847 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:01.490117 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:01.490135 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11ef00, TRANSIENT_FAILURE
2019-04-29 09:53:01.490146 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:01.490457 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:01.490498 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11f1d0, TRANSIENT_FAILURE
2019-04-29 09:53:01.490520 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:01.576646 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:01.576702 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:01.589701 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:01.589741 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
{"level":"info","ts":1556556782.1199882,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:53:02.487207 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11ef00, CONNECTING
2019-04-29 09:53:02.487238 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:02.488282 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:02.488320 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11ef00, TRANSIENT_FAILURE
2019-04-29 09:53:02.488345 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:02.489660 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11f1d0, CONNECTING
2019-04-29 09:53:02.489861 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:02.494992 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:02.495028 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11f1d0, TRANSIENT_FAILURE
2019-04-29 09:53:02.495228 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:03.449330 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:03.449363 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:03.450060 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:03.450084 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:06.871329 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:06.871352 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:06.873996 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:06.874026 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:06.883094 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11ef00, CONNECTING
2019-04-29 09:53:06.883295 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:06.888558 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11f1d0, CONNECTING
2019-04-29 09:53:06.888585 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:06.892411 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:06.892635 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:06.892663 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:06.892675 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:07.212980 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:07.213021 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:07.214790 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11ef00, SHUTDOWN
2019-04-29 09:53:07.214863 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:07.214920 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f806a2c0, CONNECTING
2019-04-29 09:53:07.214931 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:07.214981 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:07.248082 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:07.249530 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11f1d0, TRANSIENT_FAILURE
2019-04-29 09:53:07.249560 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:07.260282 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:07.260316 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f806a2c0, TRANSIENT_FAILURE
2019-04-29 09:53:07.260329 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:07.270668 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:07.270951 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:07.270982 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:07.271028 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:07.281331 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:53:07.281383 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc96c11f1d0, SHUTDOWN
2019-04-29 09:53:07.281399 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:07.281413 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f842bfc0, CONNECTING
2019-04-29 09:53:07.281421 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:07.281443 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:07.330572 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:07.330616 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f842bfc0, TRANSIENT_FAILURE
2019-04-29 09:53:07.330628 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:07.892958 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f806a2c0, CONNECTING
2019-04-29 09:53:07.892985 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:07.894467 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:07.894527 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f806a2c0, TRANSIENT_FAILURE
2019-04-29 09:53:07.894554 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:08.273603 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f842bfc0, CONNECTING
2019-04-29 09:53:08.273626 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:08.274175 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:08.274228 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f842bfc0, TRANSIENT_FAILURE
2019-04-29 09:53:08.274243 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
{"level":"info","ts":1556556789.3867722,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:53:09.521315 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f806a2c0, CONNECTING
2019-04-29 09:53:09.521342 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:09.524720 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:09.525205 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f806a2c0, TRANSIENT_FAILURE
2019-04-29 09:53:09.525430 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:09.759982 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f842bfc0, CONNECTING
2019-04-29 09:53:09.760171 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:09.764286 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:09.764635 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f842bfc0, TRANSIENT_FAILURE
2019-04-29 09:53:09.764733 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:09.894696 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:09.894921 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:09.898533 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:09.899040 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:09.901157 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f806a2c0, SHUTDOWN
2019-04-29 09:53:09.901239 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:09.901308 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ede869d0, CONNECTING
2019-04-29 09:53:09.901319 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:09.901338 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:53:09.901395 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:09.901430 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:09.901515 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ede869d0, TRANSIENT_FAILURE
2019-04-29 09:53:09.901525 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
{"level":"info","ts":1556556790.045494,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:53:10.277134 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:10.277181 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:10.277271 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:10.277294 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:10.309564 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:53:10.309596 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xc9f842bfc0, SHUTDOWN
2019-04-29 09:53:10.309610 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:10.309625 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ededfac0, CONNECTING
2019-04-29 09:53:10.309635 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:10.309680 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:10.313600 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:10.313648 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ededfac0, TRANSIENT_FAILURE
2019-04-29 09:53:10.313744 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
{"level":"info","ts":1556556790.644925,"msg":"successfully updated topology","numHosts":3}
2019-04-29 09:53:10.900344 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ede869d0, CONNECTING
2019-04-29 09:53:10.900388 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:10.903623 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:10.903647 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ede869d0, TRANSIENT_FAILURE
2019-04-29 09:53:10.903657 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29T09:53:11.015-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:53:11.016-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
--- PASS: TestFetchTaggedQuorumNormalAllUp (62.30s)
=== RUN   TestFetchTaggedQuorumAddNodeOnlyLeavingInitializingUp
2019-04-29 09:53:11.313500 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ededfac0, CONNECTING
2019-04-29 09:53:11.313558 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:53:11.991-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:53:11.991-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:53:12.268392 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ede869d0, CONNECTING
2019-04-29 09:53:12.268486 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:53:12.786-0700	INFO	starting server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556792.7865505,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556792.7865791,"msg":"initial topology / placement value received"}
2019-04-29T09:53:12.786-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:12.786-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:12.786-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:12.790-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:12.790-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:12.790-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 0, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:53:12.812-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeOnlyLeavingInitializingUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:145
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:53:12.894892 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:12.894926 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:12.894955 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:12.894973 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:12.896696 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ede869d0, SHUTDOWN
2019-04-29 09:53:12.896721 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:12.896736 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd243bd6e00, CONNECTING
2019-04-29 09:53:12.896745 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:12.896770 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:13.277854 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:13.277986 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:13.278275 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:13.278500 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:13.278645 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:13.282022 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd0ededfac0, SHUTDOWN
2019-04-29 09:53:13.282461 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:13.282853 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd24b7db360, CONNECTING
2019-04-29 09:53:13.283082 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:53:13.322-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9000"}
2019-04-29T09:53:13.326-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9002"}
2019-04-29T09:53:13.327-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9001"}
2019-04-29T09:53:13.335-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9003"}
2019-04-29T09:53:13.336-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:13.336-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:13.336-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:53:13.337-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:53:13.337-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:13.337-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:13.338-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:13.338-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:53:13.338-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:13.338-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:53:13.338-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:53:13.560-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 2, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:53:13.560-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 2, "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:53:13.641-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:53:13.641-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:53:13.641-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:53:13.641-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:53:13.642-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:53:13.645-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:53:13.645-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:53:13.645-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 3, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:53:13.658-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 3, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerDontWaitBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:515
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeOnlyLeavingInitializingUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:146
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:53:14.417-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 3, "address": "127.0.0.1:9015"}
2019-04-29T09:53:14.418-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 3, "address": "127.0.0.1:9017"}
2019-04-29T09:53:14.419-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 3, "address": "127.0.0.1:9016"}
2019-04-29T09:53:14.420-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 3, "address": "127.0.0.1:9018"}
2019-04-29T09:53:14.420-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 3, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:14.420-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 3, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:14.420-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 3, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:53:14.420-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 3, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:14.420-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 3, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 3, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 3, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 3, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 3, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 3, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 3, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 3, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 3, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:14.421-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 3, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:14.422-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 3, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:53:14.422-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 3, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:14.422-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 3, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:53:14.422-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 3, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:53:14.498-0700	INFO	started server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556795.4312787,"msg":"successfully updated topology","numHosts":4}
2019-04-29 09:53:15.896345 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:15.896519 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:15.897111 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:15.897710 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:15.898072 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd243bd6e00, SHUTDOWN
2019-04-29 09:53:15.898123 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:15.898170 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd284504b30, CONNECTING
2019-04-29 09:53:15.898185 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:15.898416 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:16.216034 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:16.216222 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd284504b30, TRANSIENT_FAILURE
2019-04-29 09:53:16.216280 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:16.357713 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:16.357738 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:16.357762 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:16.357778 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:16.367714 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd24b7db360, SHUTDOWN
2019-04-29 09:53:16.367746 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:16.367765 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd288896690, CONNECTING
2019-04-29 09:53:16.367777 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:16.367810 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:16.898864 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd284504b30, CONNECTING
2019-04-29 09:53:16.898898 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:18.215521 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:18.215551 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd284504b30, TRANSIENT_FAILURE
2019-04-29 09:53:18.215561 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:18.816761 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd284504b30, CONNECTING
2019-04-29 09:53:18.817082 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:18.968386 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:19.194589 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:19.194630 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:19.194652 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:19.246219 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd284504b30, SHUTDOWN
2019-04-29 09:53:19.247103 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:19.247133 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd298782640, CONNECTING
2019-04-29 09:53:19.247146 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:19.247192 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:19.478190 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:19.478558 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:19.500754 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:19.500780 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:19.505510 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd288896690, SHUTDOWN
2019-04-29 09:53:19.505534 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:19.505550 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2a0f62fc0, CONNECTING
2019-04-29 09:53:19.505579 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:19.505646 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
{"level":"info","ts":1556556799.7611988,"msg":"successfully updated topology","numHosts":4}
2019-04-29 09:53:21.984755 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:21.984786 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:21.984815 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:21.984861 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:21.988006 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd298782640, SHUTDOWN
2019-04-29 09:53:21.988038 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:21.988058 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2b76f7a50, CONNECTING
2019-04-29 09:53:21.988068 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:21.988097 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:22.091005 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:22.091076 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2a0f62fc0, TRANSIENT_FAILURE
2019-04-29 09:53:22.091092 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:22.159641 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2a0f62fc0, CONNECTING
2019-04-29 09:53:22.159770 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:22.486766 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:22.486791 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:22.486817 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:22.486834 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:22.488920 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2a0f62fc0, SHUTDOWN
2019-04-29 09:53:22.488941 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:22.488954 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2b9214480, CONNECTING
2019-04-29 09:53:22.488961 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:22.488983 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
{"level":"info","ts":1556556802.9948838,"msg":"successfully updated topology","numHosts":4}
2019-04-29 09:53:24.985032 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:24.985057 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:24.985077 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:24.985111 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:24.985129 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:24.985144 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2b76f7a50, SHUTDOWN
2019-04-29 09:53:24.985196 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:24.985218 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2e59dc070, CONNECTING
2019-04-29 09:53:24.985230 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:25.506009 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:25.506042 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:25.506265 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:25.506302 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:25.514023 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2b9214480, SHUTDOWN
2019-04-29 09:53:25.514052 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:25.514069 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2ea5fa070, CONNECTING
2019-04-29 09:53:25.514078 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:25.514572 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
{"level":"info","ts":1556556805.978345,"msg":"successfully updated topology","numHosts":4}
2019-04-29 09:53:26.086069 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:26.086106 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2ea5fa070, TRANSIENT_FAILURE
2019-04-29 09:53:26.086119 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:26.512803 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2ea5fa070, CONNECTING
2019-04-29 09:53:26.512838 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:27.984160 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:27.984192 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2ea5fa070, TRANSIENT_FAILURE
2019-04-29 09:53:27.984203 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:28.030985 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:28.031017 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:28.031047 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:28.031065 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:28.156185 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2e59dc070, SHUTDOWN
2019-04-29 09:53:28.156215 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:28.156233 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd313b4e4f0, CONNECTING
2019-04-29 09:53:28.156243 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:28.156273 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:28.216752 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2ea5fa070, CONNECTING
2019-04-29 09:53:28.216795 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:28.506141 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:28.506175 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:28.506224 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:28.506249 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:28.506311 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd2ea5fa070, SHUTDOWN
2019-04-29 09:53:28.506328 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:28.506341 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd313bb38c0, CONNECTING
2019-04-29 09:53:28.506349 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:28.506372 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:53:29.035-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:53:29.035-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:53:31.032172 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:31.032204 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:31.032244 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:31.032276 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:31.032336 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd313b4e4f0, SHUTDOWN
2019-04-29 09:53:31.032426 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:31.032443 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3144c6e80, CONNECTING
2019-04-29 09:53:31.032451 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:31.032458 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:31.313435 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:31.313457 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:31.506609 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:31.506645 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:31.506729 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:31.506754 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:31.506917 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd313bb38c0, SHUTDOWN
2019-04-29 09:53:31.506926 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:31.506948 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:31.506976 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd31461c270, CONNECTING
2019-04-29 09:53:31.506984 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:31.956961 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:31.957050 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3144c6e80, TRANSIENT_FAILURE
2019-04-29 09:53:31.957081 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:32.058253 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:32.058290 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd31461c270, TRANSIENT_FAILURE
2019-04-29 09:53:32.058319 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:32.073489 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3144c6e80, CONNECTING
2019-04-29 09:53:32.073581 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:32.269152 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:32.269180 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:32.508195 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd31461c270, CONNECTING
2019-04-29 09:53:32.508221 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:32.905743 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:32.905769 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:33.279987 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:33.280009 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:34.070522 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:34.070547 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:34.070571 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:34.070584 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:34.075972 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3144c6e80, SHUTDOWN
2019-04-29 09:53:34.108769 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:34.108814 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd314ef1cd0, CONNECTING
2019-04-29 09:53:34.108829 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:34.108838 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:34.506815 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:34.506848 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:34.506887 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:34.506916 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:34.508477 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd31461c270, SHUTDOWN
2019-04-29 09:53:34.508495 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:34.508521 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3151d4700, CONNECTING
2019-04-29 09:53:34.508530 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:34.508561 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
--- PASS: TestFetchTaggedQuorumAddNodeOnlyLeavingInitializingUp (23.50s)
=== RUN   TestFetchTaggedQuorumAddNodeOnlyOneNormalAndLeavingInitializingUp
2019-04-29 09:53:36.256399 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:36.256465 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3151d4700, TRANSIENT_FAILURE
2019-04-29 09:53:36.256520 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:36.427058 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:36.427077 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:36.439179 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3151d4700, CONNECTING
2019-04-29 09:53:36.439209 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:37.070987 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:37.071019 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:37.071046 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:37.071060 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:37.071076 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd314ef1cd0, SHUTDOWN
2019-04-29 09:53:37.071094 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:37.071132 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:37.071174 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd350583b80, CONNECTING
2019-04-29 09:53:37.071200 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:37.583307 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:37.583337 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:37.583368 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:37.583386 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:37.583849 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3151d4700, SHUTDOWN
2019-04-29 09:53:37.583869 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:37.583897 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd368e2b520, CONNECTING
2019-04-29 09:53:37.583907 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:37.583933 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:38.056588 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:38.056624 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd368e2b520, TRANSIENT_FAILURE
2019-04-29 09:53:38.056657 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:38.340673 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:38.340713 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd350583b80, TRANSIENT_FAILURE
2019-04-29 09:53:38.340737 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:38.626196 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd368e2b520, CONNECTING
2019-04-29 09:53:38.657148 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:38.849368 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd350583b80, CONNECTING
2019-04-29 09:53:38.849590 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:38.870948 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:38.871099 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:39.196000 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:39.196068 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:40.150686 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:40.150722 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:40.150749 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:40.150767 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:40.165827 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd350583b80, SHUTDOWN
2019-04-29 09:53:40.166653 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:40.166804 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd323a074e0, CONNECTING
2019-04-29 09:53:40.166927 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:40.189504 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:40.189555 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd368e2b520, TRANSIENT_FAILURE
2019-04-29 09:53:40.189571 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:40.195237 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:40.230490 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd368e2b520, CONNECTING
2019-04-29 09:53:40.230520 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:40.583786 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:40.583815 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:40.583840 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:40.583852 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:40.583857 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd368e2b520, SHUTDOWN
2019-04-29 09:53:40.583917 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:40.583932 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3cac06fd0, CONNECTING
2019-04-29 09:53:40.583942 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:40.583949 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:41.987466 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:41.987492 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:42.179503 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:42.179623 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3cac06fd0, TRANSIENT_FAILURE
2019-04-29 09:53:42.179683 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:42.211715 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:42.211856 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:42.685070 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:42.685140 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:42.862113 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3cac06fd0, CONNECTING
2019-04-29 09:53:42.862408 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:43.151698 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:43.151732 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:43.151772 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:43.151798 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:43.151868 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd323a074e0, SHUTDOWN
2019-04-29 09:53:43.151977 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:43.152054 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3f1b38e80, CONNECTING
2019-04-29 09:53:43.152120 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:43.152200 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:43.650796 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:43.650828 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:43.650851 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:43.650873 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:43.670178 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3cac06fd0, SHUTDOWN
2019-04-29 09:53:43.670313 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:43.670405 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd40f63aa00, CONNECTING
2019-04-29 09:53:43.670471 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:43.670541 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:44.388894 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:44.389131 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd40f63aa00, TRANSIENT_FAILURE
2019-04-29 09:53:44.389196 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:44.682465 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:44.682554 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3f1b38e80, TRANSIENT_FAILURE
2019-04-29 09:53:44.682585 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:44.773888 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd40f63aa00, CONNECTING
2019-04-29 09:53:44.774035 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:44.817756 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3f1b38e80, CONNECTING
2019-04-29 09:53:44.817786 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:45.507774 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:45.507799 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:53:45.887-0700	INFO	starting server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556825.888049,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556825.8881457,"msg":"initial topology / placement value received"}
2019-04-29T09:53:45.888-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:45.888-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:45.888-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:45.901-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:45.901-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:53:45.901-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 0, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29 09:53:45.986245 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:45.986286 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd40f63aa00, TRANSIENT_FAILURE
2019-04-29 09:53:45.986317 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29T09:53:46.041-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeOnlyOneNormalAndLeavingInitializingUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:173
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:53:46.331084 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:46.331115 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:46.331138 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:46.331152 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:46.376337 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd3f1b38e80, SHUTDOWN
2019-04-29 09:53:46.376380 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:46.376399 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd418183720, CONNECTING
2019-04-29 09:53:46.376421 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:46.376467 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:46.620833 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd40f63aa00, CONNECTING
2019-04-29 09:53:46.620870 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:46.758298 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:46.758342 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:46.758377 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:46.758400 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:46.764594 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd40f63aa00, SHUTDOWN
2019-04-29 09:53:46.764663 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:46.764695 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd418cc0fe0, CONNECTING
2019-04-29 09:53:46.764706 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:46.764730 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:48.464069 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:48.464097 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:48.532264 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:48.532391 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:48.736608 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:48.736644 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:48.741953 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:48.741982 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:49.336012 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:49.336044 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:49.336073 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:49.336207 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:49.346785 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd418183720, SHUTDOWN
2019-04-29 09:53:49.349781 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:49.349810 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd420523500, CONNECTING
2019-04-29 09:53:49.349822 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:49.349831 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:49.768429 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:49.768464 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:49.768490 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:49.768506 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:49.768595 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd418cc0fe0, SHUTDOWN
2019-04-29 09:53:49.768632 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:49.768665 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd420553e00, CONNECTING
2019-04-29 09:53:49.768674 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:49.768690 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:50.153269 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:50.153293 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:50.178472 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:50.178527 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd420523500, TRANSIENT_FAILURE
2019-04-29 09:53:50.178553 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:50.366366 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd420523500, CONNECTING
2019-04-29 09:53:50.366392 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:53:50.370-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9000"}
2019-04-29T09:53:50.372-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9002"}
2019-04-29T09:53:50.384-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9001"}
2019-04-29T09:53:50.387-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9003"}
2019-04-29T09:53:50.387-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:50.388-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:50.388-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:53:50.389-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:50.389-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:50.389-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:50.390-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:53:50.390-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:50.390-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:50.390-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:50.389-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:53:50.391-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:53:50.391-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:50.391-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:53:50.392-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:53:50.392-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:53:50.404-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:53:50.404-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:53:50.509-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29 09:53:52.074390 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:52.074491 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:52.341512 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:52.341642 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:52.341725 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:52.341794 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:52.358750 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd420523500, SHUTDOWN
2019-04-29 09:53:52.358781 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:52.358796 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd424b9e200, CONNECTING
2019-04-29 09:53:52.358805 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:52.358835 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:52.769194 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:52.771150 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:52.771371 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:52.771411 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:52.771874 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd420553e00, SHUTDOWN
2019-04-29 09:53:52.771895 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:52.771926 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd424b9f740, CONNECTING
2019-04-29 09:53:52.771935 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:52.772826 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:53.206959 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:53.206983 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:53.214459 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:53.214484 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:53.214539 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:53.214551 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:53.514998 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:53.515019 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:54.074464 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:54.074487 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:55.296046 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:55.296101 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd424b9f740, TRANSIENT_FAILURE
2019-04-29 09:53:55.296112 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:55.302451 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd424b9f740, CONNECTING
2019-04-29 09:53:55.302482 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:55.342443 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:55.342472 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:55.342513 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:55.342531 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:55.343316 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd424b9e200, SHUTDOWN
2019-04-29 09:53:55.343338 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:55.343349 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd425c0f7a0, CONNECTING
2019-04-29 09:53:55.343355 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:55.343383 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:55.769756 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:55.769794 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:55.769962 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd424b9f740, SHUTDOWN
2019-04-29 09:53:55.769982 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:55.769993 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:55.770043 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:55.770076 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:55.770089 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd425cc6790, CONNECTING
2019-04-29 09:53:55.770109 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:56.497336 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:56.497513 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:53:58.344480 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:58.344530 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:58.344553 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:58.344565 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:58.344607 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd425c0f7a0, SHUTDOWN
2019-04-29 09:53:58.344618 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:53:58.344627 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd426635250, CONNECTING
2019-04-29 09:53:58.344634 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:53:58.344650 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:58.771850 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:53:58.771890 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:53:58.771914 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:58.771926 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:53:58.772056 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd425cc6790, SHUTDOWN
2019-04-29 09:53:58.772292 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:53:58.772306 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4266b2880, CONNECTING
2019-04-29 09:53:58.772314 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:53:58.772333 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:53:58.839573 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:53:58.839602 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:53:59.644-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:53:59.644-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:53:59.645-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:53:59.645-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:53:59.645-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:53:59.647-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:53:59.647-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:53:59.647-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 1, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:53:59.664-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeOnlyOneNormalAndLeavingInitializingUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:174
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:54:00.277858 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:00.277886 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:00.293828 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:00.293856 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:01.345176 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:01.345219 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:01.345317 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:01.345354 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:01.347686 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd426635250, SHUTDOWN
2019-04-29 09:54:01.347747 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:01.347766 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd430395200, CONNECTING
2019-04-29 09:54:01.347776 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:01.347906 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:01.792765 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:01.792866 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:01.792930 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:01.792977 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:01.795304 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4266b2880, SHUTDOWN
2019-04-29 09:54:01.795420 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:01.795527 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd43327f140, CONNECTING
2019-04-29 09:54:01.795537 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:01.795569 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:54:01.806-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9005"}
2019-04-29T09:54:01.808-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9007"}
2019-04-29T09:54:01.870-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9006"}
2019-04-29T09:54:01.882-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9008"}
2019-04-29T09:54:01.885-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:54:01.886-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:54:01.886-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:54:01.888-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:54:01.888-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:54:01.889-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:54:01.890-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:54:01.890-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:54:01.890-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:54:01.971-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:54:01.972-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:54:01.973-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:54:01.973-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:54:01.974-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:54:01.976-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:54:02.036-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:54:02.036-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:54:02.037-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:54:02.042-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9009", "error": "listen tcp 127.0.0.1:9009: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29 09:54:02.100959 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:02.101080 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd43327f140, TRANSIENT_FAILURE
2019-04-29 09:54:02.101122 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:02.101612 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:02.101663 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:02.154252 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:02.154329 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd430395200, TRANSIENT_FAILURE
2019-04-29 09:54:02.154346 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:02.502736 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd430395200, CONNECTING
2019-04-29 09:54:02.503436 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:02.799740 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd43327f140, CONNECTING
2019-04-29 09:54:02.800802 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:54:03.174-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:54:03.175-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:54:03.176-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:54:03.176-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:54:03.176-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:54:03.178-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:54:03.179-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 3}
2019-04-29T09:54:03.179-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 3, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:54:03.195-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 3, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerDontWaitBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:515
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeOnlyOneNormalAndLeavingInitializingUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:175
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:54:03.423-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29 09:54:03.979221 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:03.979244 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:04.422728 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:04.422859 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd43327f140, TRANSIENT_FAILURE
2019-04-29 09:54:04.422880 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:04.437925 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:04.437958 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:04.437993 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:04.438011 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:04.466751 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd430395200, SHUTDOWN
2019-04-29 09:54:04.466891 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:04.590795 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd457e1fa00, CONNECTING
2019-04-29 09:54:04.590892 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:04.608823 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:04.608896 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:04.608929 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:04.714177 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd43327f140, CONNECTING
2019-04-29 09:54:04.714199 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:04.828696 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:04.828721 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:04.829130 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:04.829152 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:04.831101 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd43327f140, SHUTDOWN
2019-04-29 09:54:04.831150 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:04.831179 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd1fb83a640, CONNECTING
2019-04-29 09:54:04.831188 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:04.831236 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
{"level":"info","ts":1556556845.5314722,"msg":"successfully updated topology","numHosts":4}
2019-04-29T09:54:05.903-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 3, "address": "127.0.0.1:9015"}
2019-04-29 09:54:06.075882 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:06.075908 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:06.090360 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:06.090521 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd457e1fa00, TRANSIENT_FAILURE
2019-04-29 09:54:06.090595 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:06.139966 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd457e1fa00, CONNECTING
2019-04-29 09:54:06.140040 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:06.759842 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:06.759868 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
{"level":"info","ts":1556556846.9489143,"msg":"successfully updated topology","numHosts":4}
2019-04-29 09:54:07.516314 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:07.516516 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:07.516569 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:07.516601 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:07.520734 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd457e1fa00, SHUTDOWN
2019-04-29 09:54:07.520765 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:07.520800 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a151cc40, CONNECTING
2019-04-29 09:54:07.520812 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:07.520853 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:08.454017 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:08.454046 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:08.454071 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:08.454087 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:08.471501 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd1fb83a640, SHUTDOWN
2019-04-29 09:54:08.471678 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:08.471696 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a419ac70, CONNECTING
2019-04-29 09:54:08.471719 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:08.471727 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:08.609997 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:08.709086 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:08.709170 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a151cc40, TRANSIENT_FAILURE
2019-04-29 09:54:08.709205 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:08.712406 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:08.715965 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: socket: too many open files"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:08.716108 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a151cc40, CONNECTING
2019-04-29 09:54:08.716181 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:08.716202 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a151cc40, TRANSIENT_FAILURE
2019-04-29 09:54:08.716227 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:08.718846 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:54:08.731550 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:54:08.753688 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 40ms
2019-04-29T09:54:08.787-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "namespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/0/fileset-1556553600000000000-0-summaries.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/1/fileset-1556553600000000000-0-info.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/2/fileset-1556553600000000000-0-summaries.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-0-info.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/4/fileset-1556553600000000000-0-bloomfilter.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/5/fileset-1556553600000000000-0-bloomfilter.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/6/fileset-1556553600000000000-0-summaries.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/7/fileset-1556553600000000000-4-summaries.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/8/fileset-1556553600000000000-4-summaries.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/9/fileset-1556553600000000000-4-summaries.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/10/fileset-1556553600000000000-4-data.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/11/fileset-1556553600000000000-4-summaries.db: too many open files\nnamespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/0/fileset-1556546400000000000-4-info.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/1/fileset-1556546400000000000-4-data.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/2/fileset-1556546400000000000-4-info.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556546400000000000-4-summaries.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/4/fileset-1556546400000000000-4-index.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/5/fileset-1556546400000000000-4-digest.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/6/fileset-1556546400000000000-4-info.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/7/fileset-1556546400000000000-4-summaries.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/8/fileset-1556546400000000000-4-index.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/9/fileset-1556546400000000000-4-index.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/10/fileset-1556546400000000000-4-bloomfilter.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/11/fileset-1556546400000000000-4-digest.db: too many open files\nerror writing out snapshot metadata file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots: too many open files\npersist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:08.798209 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 80ms
2019-04-29 09:54:08.881497 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 160ms
2019-04-29 09:54:09.043568 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.049349 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.055081 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:54:09.065764 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:54:09.087104 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.096255 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:54:09.108039 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:54:09.130851 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 40ms
2019-04-29 09:54:09.171446 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 80ms
2019-04-29 09:54:09.252000 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 160ms
{"level":"info","ts":1556556849.3180168,"msg":"successfully updated topology","numHosts":4}
2019-04-29T09:54:09.349-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "namespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/0/fileset-1556553600000000000-0-summaries.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/1/fileset-1556553600000000000-0-bloomfilter.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/2/fileset-1556553600000000000-0-digest.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/3/fileset-1556553600000000000-0-info.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/4/fileset-1556553600000000000-0-summaries.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/5/fileset-1556553600000000000-0-info.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/6/fileset-1556553600000000000-0-data.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/7/fileset-1556553600000000000-0-index.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/8/fileset-1556553600000000000-0-info.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/9/fileset-1556553600000000000-0-data.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/10/fileset-1556553600000000000-0-info.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/11/fileset-1556553600000000000-0-index.db: too many open files\nnamespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/0/fileset-1556546400000000000-0-info.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/1/fileset-1556546400000000000-0-info.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/2/fileset-1556546400000000000-0-info.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/3/fileset-1556546400000000000-0-digest.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/4/fileset-1556546400000000000-0-info.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/5/fileset-1556546400000000000-0-summaries.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/6/fileset-1556546400000000000-0-data.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/7/fileset-1556546400000000000-0-data.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/8/fileset-1556546400000000000-0-info.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/9/fileset-1556546400000000000-0-data.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/10/fileset-1556546400000000000-0-info.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots/testNs1/11/fileset-1556546400000000000-0-index.db: too many open files\nerror writing out snapshot metadata file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-33050399397/snapshots: too many open files\npersist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:09.413157 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 320ms
2019-04-29 09:54:09.735209 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.740594 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:54:09.751548 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:54:09.772212 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.778061 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:54:09.788397 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.794528 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.799954 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.805793 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.811710 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:09.818593 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:54:10.048190 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:10.048224 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a419ac70, TRANSIENT_FAILURE
2019-04-29 09:54:10.048237 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:10.061699 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a419ac70, CONNECTING
2019-04-29 09:54:10.061740 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:10.092272 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:10.092316 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a419ac70, TRANSIENT_FAILURE
2019-04-29 09:54:10.092331 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29T09:54:10.336-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "namespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/0/fileset-1556553600000000000-0-data.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/1/fileset-1556553600000000000-0-info.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/2/fileset-1556553600000000000-0-info.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/3/fileset-1556553600000000000-0-summaries.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/4/fileset-1556553600000000000-0-digest.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/5/fileset-1556553600000000000-0-info.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/6/fileset-1556553600000000000-0-digest.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/7/fileset-1556553600000000000-0-summaries.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/8/fileset-1556553600000000000-0-summaries.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/9/fileset-1556553600000000000-0-info.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/10/fileset-1556553600000000000-0-info.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/11/fileset-1556553600000000000-0-info.db: too many open files\nnamespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/0/fileset-1556546400000000000-0-index.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/1/fileset-1556546400000000000-0-info.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/2/fileset-1556546400000000000-0-index.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/3/fileset-1556546400000000000-0-info.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/4/fileset-1556546400000000000-0-index.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/5/fileset-1556546400000000000-0-data.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/6/fileset-1556546400000000000-0-data.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/7/fileset-1556546400000000000-0-digest.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/8/fileset-1556546400000000000-0-info.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/9/fileset-1556546400000000000-0-digest.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/10/fileset-1556546400000000000-0-digest.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots/testNs1/11/fileset-1556546400000000000-0-index.db: too many open files\nerror writing out snapshot metadata file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-35898320927/snapshots: too many open files\npersist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:10.447942 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a151cc40, CONNECTING
2019-04-29 09:54:10.447967 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:10.516529 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:10.516559 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:10.516585 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:10.516690 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a151cc40, SHUTDOWN
2019-04-29 09:54:10.516708 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:10.516755 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:10.516782 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:10.516790 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4cf355a40, CONNECTING
2019-04-29 09:54:10.516854 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:11.454372 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:11.454400 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:11.454424 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:11.454437 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:11.454592 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4a419ac70, SHUTDOWN
2019-04-29 09:54:11.454662 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:11.454687 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4cfb183a0, CONNECTING
2019-04-29 09:54:11.454704 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:11.454729 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:54:11.454747 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:54:11.793-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/0/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/0/fileset-1556546400000000000-4-data.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/0/fileset-1556546400000000000-4-digest.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.794-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/1/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/1/fileset-1556546400000000000-4-bloomfilter.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/1/fileset-1556546400000000000-4-summaries.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.795-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/2/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/2/fileset-1556546400000000000-4-data.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/2/fileset-1556546400000000000-4-digest.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.796-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556546400000000000-4-index.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.797-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/4/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/4/fileset-1556546400000000000-4-info.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.800-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/5/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/5/fileset-1556546400000000000-4-data.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.802-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/6/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/6/fileset-1556546400000000000-4-digest.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.802-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/6/fileset-1556553600000000000-0-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/6/fileset-1556553600000000000-0-info.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/6/fileset-1556553600000000000-0-index.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.803-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/7/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/7/fileset-1556546400000000000-4-index.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.804-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/7/fileset-1556553600000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/7/fileset-1556553600000000000-4-index.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/7/fileset-1556553600000000000-4-info.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.805-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/8/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/8/fileset-1556546400000000000-4-info.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.807-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/8/fileset-1556553600000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/8/fileset-1556553600000000000-4-index.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/8/fileset-1556553600000000000-4-info.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.808-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/9/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/9/fileset-1556546400000000000-4-info.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.809-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/9/fileset-1556553600000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/9/fileset-1556553600000000000-4-index.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/9/fileset-1556553600000000000-4-info.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.810-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/10/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/10/fileset-1556546400000000000-4-summaries.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.810-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/10/fileset-1556553600000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/10/fileset-1556553600000000000-4-bloomfilter.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/10/fileset-1556553600000000000-4-summaries.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.812-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/11/fileset-1556546400000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/11/fileset-1556546400000000000-4-data.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.812-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/11/fileset-1556553600000000000-4-checkpoint.db: no such file or directory", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/11/fileset-1556553600000000000-4-index.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/11/fileset-1556553600000000000-4-info.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:11.828-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:11.946815 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:11.946928 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4cfb183a0, TRANSIENT_FAILURE
2019-04-29 09:54:11.946954 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:11.967314 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:11.967359 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:12.365818 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:12.365844 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:12.473912 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4cfb183a0, CONNECTING
2019-04-29 09:54:12.474096 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:54:12.490-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:13.519-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:13.520734 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:13.520809 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:13.521447 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:13.521467 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:13.523806 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4cf355a40, SHUTDOWN
2019-04-29 09:54:13.523901 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:13.523967 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4dfb09a80, CONNECTING
2019-04-29 09:54:13.524028 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:13.524079 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:14.039744 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:14.039785 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4dfb09a80, TRANSIENT_FAILURE
2019-04-29 09:54:14.039799 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
{"level":"info","ts":1556556854.3275547,"msg":"successfully updated topology","numHosts":4}
2019-04-29 09:54:14.454935 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:14.454975 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:14.455066 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:14.455088 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:14.455197 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4cfb183a0, SHUTDOWN
2019-04-29 09:54:14.458211 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:14.458352 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4f8485fa0, CONNECTING
2019-04-29 09:54:14.458418 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:14.458430 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:14.522583 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4dfb09a80, CONNECTING
2019-04-29 09:54:14.522612 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:54:14.901-0700	ERROR	encountered corrupt snapshot file during cleanup, marking files for deletion	{"cache-policy": "recently_read", "instance": 0, "error": "error reading snapshot info file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-3-checkpoint.db: too many open files", "files": ["/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-3-bloomfilter.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-3-checkpoint.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-3-data.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-3-digest.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-3-index.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-3-info.db", "/var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-34143004096/snapshots/testNs1/3/fileset-1556553600000000000-3-summaries.db"]}
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).cleanupSnapshotsAndCommitlogs
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:380
github.com/m3db/m3/src/dbnode/storage.(*cleanupManager).Cleanup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cleanup.go:162
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:133
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:14.904-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:15.509-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:16.523643 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:16.523672 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:16.523699 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:16.523716 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:16.525256 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4dfb09a80, SHUTDOWN
2019-04-29 09:54:16.525283 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:16.525296 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4ff1bc5c0, CONNECTING
2019-04-29 09:54:16.525305 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:16.525332 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:54:16.545-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:17.455514 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:17.455551 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:17.455579 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:17.455593 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4f8485fa0, SHUTDOWN
2019-04-29 09:54:17.455652 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:17.455721 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:17.455793 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:17.455821 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4ff6ac2e0, CONNECTING
2019-04-29 09:54:17.455830 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:54:17.907-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:17.909-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:18.168293 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:18.168330 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4ff6ac2e0, TRANSIENT_FAILURE
2019-04-29 09:54:18.168341 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:18.654636 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:18.654662 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:54:18.755-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:18.777871 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:18.777898 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:18.794220 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4ff6ac2e0, CONNECTING
2019-04-29 09:54:18.794254 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:19.524321 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:19.525343 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:19.525552 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:19.525814 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4ff1bc5c0, SHUTDOWN
2019-04-29 09:54:19.526427 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:19.526532 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:19.527282 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:19.530046 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd50062a4e0, CONNECTING
2019-04-29 09:54:19.530102 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:54:19.698-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:19.698-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:20.455962 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:20.455995 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:20.456046 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:20.456061 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:20.456099 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd4ff6ac2e0, SHUTDOWN
2019-04-29 09:54:20.456111 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:20.456120 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd500b8c1e0, CONNECTING
2019-04-29 09:54:20.456126 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:20.456141 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:54:21.816-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:21.952743 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:21.952862 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd500b8c1e0, TRANSIENT_FAILURE
2019-04-29 09:54:21.952898 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:21.956034 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd500b8c1e0, CONNECTING
2019-04-29 09:54:21.956064 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:21.971942 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:21.971976 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd500b8c1e0, TRANSIENT_FAILURE
2019-04-29 09:54:21.972009 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:22.438419 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:22.438446 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:22.524684 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:22.524712 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:22.524739 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:22.524755 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:22.524777 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd50062a4e0, SHUTDOWN
2019-04-29 09:54:22.524794 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:22.524807 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd50174f860, CONNECTING
2019-04-29 09:54:22.524816 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:22.524871 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
--- PASS: TestFetchTaggedQuorumAddNodeOnlyOneNormalAndLeavingInitializingUp (48.31s)
=== RUN   TestFetchTaggedQuorumAddNodeAllUp
2019-04-29 09:54:23.456165 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:23.456218 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:23.456252 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:23.456266 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:23.456322 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd500b8c1e0, SHUTDOWN
2019-04-29 09:54:23.456424 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:23.456484 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd501a33b20, CONNECTING
2019-04-29 09:54:23.456543 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:23.456595 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:54:23.456645 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:24.023152 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:24.023189 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd501a33b20, TRANSIENT_FAILURE
2019-04-29 09:54:24.023204 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:24.163542 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:24.163580 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd50174f860, TRANSIENT_FAILURE
2019-04-29 09:54:24.163596 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:24.202097 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd50174f860, CONNECTING
2019-04-29 09:54:24.202126 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:24.457266 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd501a33b20, CONNECTING
2019-04-29 09:54:24.457297 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:24.830667 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:24.830739 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:54:24.835-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:25.525065 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:25.525523 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:25.525934 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:25.525983 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:25.526056 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:25.526323 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd50174f860, SHUTDOWN
2019-04-29 09:54:25.529424 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:25.529655 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd57bee2c30, CONNECTING
2019-04-29 09:54:25.529707 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:26.543868 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:26.543897 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:26.767990 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:26.768025 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:26.768057 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:26.768077 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:26.826987 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd501a33b20, SHUTDOWN
2019-04-29 09:54:27.032751 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:27.032788 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd58126c4c0, CONNECTING
2019-04-29 09:54:27.032821 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:27.032846 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:29.005062 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:29.005093 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:29.005289 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:29.005320 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:29.086106 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd57bee2c30, SHUTDOWN
2019-04-29 09:54:29.086138 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:29.086171 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5932eb6d0, CONNECTING
2019-04-29 09:54:29.086184 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:29.086227 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:54:29.418-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:29.862520 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:29.862549 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:29.862621 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:29.862641 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:29.867149 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd58126c4c0, SHUTDOWN
2019-04-29 09:54:29.867177 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:29.867205 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5946a73a0, CONNECTING
2019-04-29 09:54:29.867212 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:29.867245 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:30.116930 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:30.116967 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5946a73a0, TRANSIENT_FAILURE
2019-04-29 09:54:30.117009 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:30.907802 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:30.909736 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:31.217340 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:31.217393 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:31.285113 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5946a73a0, CONNECTING
2019-04-29 09:54:31.285152 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:32.291140 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:32.291241 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:32.291344 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:32.291383 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:32.309773 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5932eb6d0, SHUTDOWN
2019-04-29 09:54:32.309855 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:32.309877 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd597c6c1f0, CONNECTING
2019-04-29 09:54:32.309899 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:32.309950 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:32.491234 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:32.491274 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:54:32.742-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:32.935987 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:32.936019 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:32.936089 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:32.936115 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:32.955383 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5946a73a0, SHUTDOWN
2019-04-29 09:54:32.955473 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:32.955511 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd59a246300, CONNECTING
2019-04-29 09:54:32.955543 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:32.955617 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:34.677353 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:34.677407 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:34.708772 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:34.708879 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:36.028845 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:36.029079 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:36.029224 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:36.029277 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:36.470906 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:36.786757 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:36.786882 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:36.786999 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:36.787031 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29T09:54:37.284-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:37.298689 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd597c6c1f0, SHUTDOWN
2019-04-29 09:54:37.298747 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:37.298776 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b81c8b60, CONNECTING
2019-04-29 09:54:37.298786 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:37.547501 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd59a246300, SHUTDOWN
2019-04-29 09:54:37.547555 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:37.547570 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b83aa740, CONNECTING
2019-04-29 09:54:37.547586 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:37.547628 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:37.574385 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:37.574409 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:39.315229 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:39.316142 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b81c8b60, TRANSIENT_FAILURE
2019-04-29 09:54:39.316254 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:39.938454 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:39.938525 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:39.938962 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:39.939016 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:39.947513 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:39.950791 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b81c8b60, SHUTDOWN
2019-04-29 09:54:39.950821 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:39.950921 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b9b2a600, CONNECTING
2019-04-29 09:54:39.950972 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:39.951029 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:40.115126 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:40.115151 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:40.373558 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:40.373628 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:40.373719 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:40.373750 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:40.391044 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b83aa740, SHUTDOWN
2019-04-29 09:54:40.391094 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:40.391126 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b9cbcdf0, CONNECTING
2019-04-29 09:54:40.391154 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:40.391211 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:41.414296 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:41.414520 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b9b2a600, TRANSIENT_FAILURE
2019-04-29 09:54:41.414565 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:41.867368 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:41.867415 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:42.082548 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b9b2a600, CONNECTING
2019-04-29 09:54:42.082582 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:43.291079 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:43.291277 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:43.291385 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:43.291460 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:43.311771 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b9b2a600, SHUTDOWN
2019-04-29 09:54:43.313661 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:43.313819 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5ba58e860, CONNECTING
2019-04-29 09:54:43.314251 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:43.314371 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:43.404157 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:43.404294 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:43.404346 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:43.404403 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:43.802727 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5b9cbcdf0, SHUTDOWN
2019-04-29 09:54:43.802836 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:43.802880 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5bb324c80, CONNECTING
2019-04-29 09:54:43.802918 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:44.055815 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:54:44.394-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:44.444053 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:44.444255 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5ba58e860, TRANSIENT_FAILURE
2019-04-29 09:54:44.444276 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:44.467861 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:44.467889 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:44.546048 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:44.546122 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:44.964815 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:44.964871 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:45.836498 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5ba58e860, CONNECTING
2019-04-29 09:54:45.836532 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:46.197416 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:46.197612 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:46.643242 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:46.643281 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:46.643317 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:46.643355 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:46.851532 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:46.851565 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:46.851622 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:46.851643 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:46.866117 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5ba58e860, SHUTDOWN
2019-04-29 09:54:46.866148 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:46.866178 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5be810760, CONNECTING
2019-04-29 09:54:46.866189 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:46.866243 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:46.866286 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5bb324c80, SHUTDOWN
2019-04-29 09:54:46.866299 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:46.866333 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5be811510, CONNECTING
2019-04-29 09:54:46.866343 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:46.866371 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:48.298750 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:48.298880 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5be811510, TRANSIENT_FAILURE
2019-04-29 09:54:48.298915 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29T09:54:48.412-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:48.421224 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5be811510, CONNECTING
2019-04-29 09:54:48.421251 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:49.106886 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:49.107002 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:49.689553 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:49.689718 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:49.689763 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:49.689798 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:49.692846 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5be810760, SHUTDOWN
2019-04-29 09:54:49.692894 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:49.692963 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd600cd3950, CONNECTING
2019-04-29 09:54:49.693011 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:49.693108 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:49.866029 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:49.866079 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:49.866108 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:49.866131 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:49.867585 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd5be811510, SHUTDOWN
2019-04-29 09:54:49.867625 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:49.867638 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd602fc7bc0, CONNECTING
2019-04-29 09:54:49.867652 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:49.867692 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:49.986219 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:49.986303 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd600cd3950, TRANSIENT_FAILURE
2019-04-29 09:54:49.986371 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:50.097924 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:50.098019 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd602fc7bc0, TRANSIENT_FAILURE
2019-04-29 09:54:50.098048 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29T09:54:50.656-0700	INFO	starting server	{"cache-policy": "recently_read"}
{"level":"info","ts":1556556890.6561005,"msg":"waiting for dynamic topology initialization, if this takes a long time, make sure that a topology/placement is configured"}
{"level":"info","ts":1556556890.656236,"msg":"initial topology / placement value received"}
2019-04-29T09:54:50.656-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:54:50.656-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:54:50.656-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:54:50.660-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:54:50.661-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 0}
2019-04-29T09:54:50.661-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 0, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29 09:54:50.799019 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd600cd3950, CONNECTING
2019-04-29 09:54:50.799190 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:50.869093 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd602fc7bc0, CONNECTING
2019-04-29 09:54:50.869122 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:54:50.889-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeAllUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:205
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:54:51.069685 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:51.069710 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:54:51.679-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:54:52.293418 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:52.293682 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:52.737154 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:52.737238 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:52.737297 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:52.737365 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:52.851005 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd600cd3950, SHUTDOWN
2019-04-29 09:54:52.851036 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:52.851084 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd60d52c280, CONNECTING
2019-04-29 09:54:52.851112 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:52.851216 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:52.877941 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:52.877970 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:52.878042 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:52.878085 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:52.879211 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd602fc7bc0, SHUTDOWN
2019-04-29 09:54:52.879246 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:52.879259 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd60da0cfd0, CONNECTING
2019-04-29 09:54:52.879276 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:52.879313 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:52.938732 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:52.938827 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:54:54.834-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9000"}
2019-04-29T09:54:54.836-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9002"}
2019-04-29T09:54:54.843-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9001"}
2019-04-29T09:54:54.847-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9003"}
2019-04-29T09:54:54.847-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:54:54.848-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:54:54.848-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:54:54.848-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:54:54.869-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:54:54.870-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:54:54.870-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:54:54.870-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:54:54.870-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:54:54.870-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:54:54.870-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:54:54.870-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:54:54.870-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:54:54.871-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:54:54.871-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 0, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:54:54.871-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 0, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:54:54.871-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:54:54.852-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 0, "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:54:54.871-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 0, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:54:55.133-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:54:55.434-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:54:55.434-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:54:55.434-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:54:55.434-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:54:55.434-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:54:55.436-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:54:55.436-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 1}
2019-04-29T09:54:55.436-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 1, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:54:55.524-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeAllUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:206
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:54:55.770605 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:55.770637 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:55.770674 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:55.770691 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:55.922056 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd60d52c280, SHUTDOWN
2019-04-29 09:54:55.922238 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:55.923068 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd616406420, CONNECTING
2019-04-29 09:54:55.923248 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:55.924600 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:56.024727 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:56.024760 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:56.024840 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:56.024871 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:56.091375 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd60da0cfd0, SHUTDOWN
2019-04-29 09:54:56.091675 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:56.097653 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd61647d950, CONNECTING
2019-04-29 09:54:56.097695 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:56.097830 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:56.439629 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:56.439815 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd616406420, TRANSIENT_FAILURE
2019-04-29 09:54:56.439933 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:57.194107 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd616406420, CONNECTING
2019-04-29 09:54:57.194138 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:57.371054 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:57.371109 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd61647d950, TRANSIENT_FAILURE
2019-04-29 09:54:57.371121 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:57.452185 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:57.452316 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:58.219567 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd61647d950, CONNECTING
2019-04-29 09:54:58.219716 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:58.354177 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:54:58.354203 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:54:58.816094 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:58.816351 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:58.816593 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:58.816694 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:58.828280 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd616406420, SHUTDOWN
2019-04-29 09:54:58.828339 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:54:58.828361 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd619054360, CONNECTING
2019-04-29 09:54:58.828384 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:54:58.828439 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:59.100912 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:54:59.100938 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:54:59.100981 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:54:59.101006 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:54:59.317771 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd61647d950, SHUTDOWN
2019-04-29 09:54:59.317880 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:54:59.317968 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd61984a320, CONNECTING
2019-04-29 09:54:59.317983 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:54:59.318038 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:54:59.487-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:00.010856 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:00.010928 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd61984a320, TRANSIENT_FAILURE
2019-04-29 09:55:00.010946 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:00.228270 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd61984a320, CONNECTING
2019-04-29 09:55:00.228319 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:01.818899 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:01.818934 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:01.818969 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:01.818984 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:01.828398 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd619054360, SHUTDOWN
2019-04-29 09:55:01.828456 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:01.828469 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd620b7c200, CONNECTING
2019-04-29 09:55:01.828493 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:01.828543 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:02.363517 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:02.363543 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:02.624190 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:02.624427 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:02.624538 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:02.624660 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:02.627867 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd61984a320, SHUTDOWN
2019-04-29 09:55:02.650017 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:02.650376 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:02.654672 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd621272990, CONNECTING
2019-04-29 09:55:03.140500 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:03.140679 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd620b7c200, TRANSIENT_FAILURE
2019-04-29 09:55:03.140747 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:03.397690 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:03.880513 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd620b7c200, CONNECTING
2019-04-29 09:55:03.880574 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:55:04.206-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:04.717513 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:04.717661 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:04.831152 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:04.831192 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:04.831228 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:04.831252 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:04.888796 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd620b7c200, SHUTDOWN
2019-04-29 09:55:04.888822 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:04.888847 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd622b43890, CONNECTING
2019-04-29 09:55:04.888857 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:04.888972 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:05.130092 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:05.130353 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:05.165287 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:05.165374 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd621272990, TRANSIENT_FAILURE
2019-04-29 09:55:05.165407 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:05.341446 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd621272990, CONNECTING
2019-04-29 09:55:05.341606 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:05.660098 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:05.660228 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:05.660397 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:05.660443 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:05.662804 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd621272990, SHUTDOWN
2019-04-29 09:55:05.662834 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:05.662868 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd623754410, CONNECTING
2019-04-29 09:55:05.662880 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:05.662926 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:06.155840 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:06.155995 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:06.305630 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:06.305681 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd623754410, TRANSIENT_FAILURE
2019-04-29 09:55:06.305884 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:06.324198 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:06.324301 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:06.647708 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:06.647763 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:06.663632 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd623754410, CONNECTING
2019-04-29 09:55:06.663682 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:55:06.764-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9005"}
2019-04-29T09:55:06.777-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9007"}
2019-04-29T09:55:06.779-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9006"}
2019-04-29T09:55:06.782-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9008"}
2019-04-29T09:55:06.782-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:55:06.783-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:55:06.783-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:55:06.784-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:55:06.784-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:55:06.784-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:55:06.784-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:55:06.784-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:55:06.784-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:55:06.785-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:55:06.785-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:55:06.785-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T03:00:00.000-0700", "to": "2019-04-29T07:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:55:06.785-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s"}
2019-04-29T09:55:06.785-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:55:06.786-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "instance": 1, "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numBlocks": 0, "numSegments": 0}
2019-04-29T09:55:06.786-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "instance": 1, "run": "bootstrap-index", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2019-04-29T07:00:00.000-0700", "to": "2019-04-29T11:00:00.000-0700", "range": "4h0m0s", "took": "0s"}
2019-04-29T09:55:06.786-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:55:06.794-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "instance": 1, "address": "127.0.0.1:9009", "error": "listen tcp 127.0.0.1:9009: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:55:06.794-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "instance": 1, "namespace": "testNs1", "duration": "0s"}
2019-04-29T09:55:07.322-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:55:07.322-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:55:07.323-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:55:07.323-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:55:07.323-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:55:07.325-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:55:07.325-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read", "instance": 2}
2019-04-29T09:55:07.325-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "instance": 2, "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:55:07.360-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "instance": 2, "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeAllUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:207
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:55:07.519-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:07.834441 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:07.834534 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:07.834645 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:07.834722 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:07.900849 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd622b43890, SHUTDOWN
2019-04-29 09:55:07.900897 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:07.900965 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6280896a0, CONNECTING
2019-04-29 09:55:07.901630 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:07.901721 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:08.257886 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:08.257929 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6280896a0, TRANSIENT_FAILURE
2019-04-29 09:55:08.257981 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:08.468879 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:08.468916 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd623754410, TRANSIENT_FAILURE
2019-04-29 09:55:08.468949 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:08.963505 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:08.963682 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:08.963818 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:08.963905 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:08.994468 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd623754410, SHUTDOWN
2019-04-29 09:55:08.995349 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:08.995481 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62a360ff0, CONNECTING
2019-04-29 09:55:08.995552 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:08.995641 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:09.048469 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:09.073806 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:09.073832 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:09.869110 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6280896a0, CONNECTING
2019-04-29 09:55:09.869232 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:10.806066 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:10.806136 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6280896a0, TRANSIENT_FAILURE
2019-04-29 09:55:10.806176 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:10.972919 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:10.972971 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:10.973002 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:10.973046 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:10.975059 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:10.975636 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6280896a0, SHUTDOWN
2019-04-29 09:55:10.975724 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:10.977865 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62be1e590, CONNECTING
2019-04-29 09:55:10.977994 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:10.978090 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:11.020100 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:11.020153 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:11.540318 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:11.540458 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:55:11.668-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:12.102524 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:12.102621 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:12.102773 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:12.102828 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:12.214640 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62a360ff0, SHUTDOWN
2019-04-29 09:55:12.214691 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:12.214711 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62d65ba10, CONNECTING
2019-04-29 09:55:12.214732 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:12.214821 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:12.254628 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:12.254690 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62d65ba10, TRANSIENT_FAILURE
2019-04-29 09:55:12.254707 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:12.303027 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:12.596364 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:12.904682 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:12.904874 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62be1e590, TRANSIENT_FAILURE
2019-04-29 09:55:12.904944 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:12.905722 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:55:12.907104 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:12.907153 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:12.908887 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:12.908923 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:55:12.926-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "namespace testNs1 failed to snapshot data: shard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/9/fileset-1556553600000000000-0-data.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/10/fileset-1556553600000000000-0-info.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/11/fileset-1556553600000000000-0-summaries.db: too many open files\nnamespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/0/fileset-1556546400000000000-2-digest.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/1/fileset-1556546400000000000-2-data.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/2/fileset-1556546400000000000-2-data.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/3/fileset-1556546400000000000-2-data.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/4/fileset-1556546400000000000-2-summaries.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/5/fileset-1556546400000000000-2-data.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/6/fileset-1556546400000000000-2-summaries.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/7/fileset-1556546400000000000-2-index.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/8/fileset-1556546400000000000-2-info.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/9/fileset-1556546400000000000-2-index.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/10/fileset-1556546400000000000-2-bloomfilter.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-38855091956/snapshots/testNs1/11/fileset-1556546400000000000-2-bloomfilter.db: too many open files"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:12.933750 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:12.936499 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62be1e590, CONNECTING
2019-04-29 09:55:12.936535 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:55:12.937-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "namespace testNs1 failed to flush data: shard 11 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/11/fileset-1556539200000000000-digest.db: too many open files\nnamespace testNs1 failed to flush data: shard 0 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/0/fileset-1556532000000000000-data.db: too many open files\nshard 1 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/1/fileset-1556532000000000000-data.db: too many open files\nshard 2 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/2/fileset-1556532000000000000-data.db: too many open files\nshard 7 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/7/fileset-1556532000000000000-data.db: too many open files\nshard 8 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/8/fileset-1556532000000000000-data.db: too many open files\nshard 9 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/9/fileset-1556532000000000000-data.db: too many open files\nshard 10 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/10/fileset-1556532000000000000-info.db: too many open files\nshard 11 failed to flush data: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/data/testNs1/11/fileset-1556532000000000000-summaries.db: too many open files\nnamespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/0/fileset-1556553600000000000-1-data.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/1/fileset-1556553600000000000-1-summaries.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/2/fileset-1556553600000000000-1-summaries.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/3/fileset-1556553600000000000-1-bloomfilter.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/4/fileset-1556553600000000000-0-data.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/5/fileset-1556553600000000000-0-index.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/6/fileset-1556553600000000000-1-index.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/7/fileset-1556553600000000000-1-bloomfilter.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/8/fileset-1556553600000000000-1-index.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/9/fileset-1556553600000000000-1-info.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/10/fileset-1556553600000000000-1-index.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/11/fileset-1556553600000000000-0-bloomfilter.db: too many open files\nnamespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/0/fileset-1556546400000000000-1-summaries.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/1/fileset-1556546400000000000-1-info.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/3/fileset-1556546400000000000-1-digest.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/4/fileset-1556546400000000000-1-index.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/5/fileset-1556546400000000000-0-bloomfilter.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/6/fileset-1556546400000000000-0-info.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/7/fileset-1556546400000000000-0-info.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/8/fileset-1556546400000000000-0-bloomfilter.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/9/fileset-1556546400000000000-0-info.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/10/fileset-1556546400000000000-0-bloomfilter.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/11/fileset-1556546400000000000-0-info.db: too many open files\nnamespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/0/fileset-1556539200000000000-0-info.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/1/fileset-1556539200000000000-0-index.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/2/fileset-1556539200000000000-0-bloomfilter.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/3/fileset-1556539200000000000-0-summaries.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/4/fileset-1556539200000000000-0-info.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/5/fileset-1556539200000000000-0-data.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/6/fileset-1556539200000000000-0-info.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/7/fileset-1556539200000000000-0-data.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/8/fileset-1556539200000000000-0-info.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/9/fileset-1556539200000000000-0-info.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/10/fileset-1556539200000000000-0-bloomfilter.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/11/fileset-1556539200000000000-0-summaries.db: too many open files\nnamespace testNs1 failed to snapshot data: shard 0 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/0/fileset-1556532000000000000-0-bloomfilter.db: too many open files\nshard 1 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/1/fileset-1556532000000000000-0-info.db: too many open files\nshard 2 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/2/fileset-1556532000000000000-0-digest.db: too many open files\nshard 3 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/3/fileset-1556532000000000000-0-info.db: too many open files\nshard 4 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/4/fileset-1556532000000000000-0-bloomfilter.db: too many open files\nshard 5 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/5/fileset-1556532000000000000-0-digest.db: too many open files\nshard 6 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/6/fileset-1556532000000000000-0-digest.db: too many open files\nshard 7 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/7/fileset-1556532000000000000-0-info.db: too many open files\nshard 8 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/8/fileset-1556532000000000000-0-info.db: too many open files\nshard 9 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/9/fileset-1556532000000000000-0-bloomfilter.db: too many open files\nshard 10 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/10/fileset-1556532000000000000-0-info.db: too many open files\nshard 11 failed to snapshot: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/testNs1/11/fileset-1556532000000000000-0-data.db: too many open files\nerror writing out snapshot metadata file: open /var/folders/vg/khsllfnx1qg8q5jk6ggj774c0000gn/T/integration-test-39689342403/snapshots/snapshot-8e2d24626a9f11e9bdaaa45e60cb7ff9-0-metadata.db: too many open files\npersist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:12.945315 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.040236 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.056012 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:55:13.081386 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:55:13.105499 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 40ms
2019-04-29 09:55:13.105748 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: socket: too many open files"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:13.105787 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62d65ba10, CONNECTING
2019-04-29 09:55:13.105800 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:13.105824 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62d65ba10, TRANSIENT_FAILURE
2019-04-29 09:55:13.105833 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:13.146641 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.156020 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.161914 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.168699 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.205208 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.302452 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.326689 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:55:13.353207 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:55:13.394050 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 40ms
2019-04-29 09:55:13.435123 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.440721 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:55:13.451466 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:55:13.476406 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.533611 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:55:13.545218 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:55:13.566603 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 40ms
2019-04-29 09:55:13.612004 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.620219 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:55:13.631015 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:55:13.667806 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 40ms
2019-04-29 09:55:13.711508 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 80ms
2019-04-29 09:55:13.792551 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 160ms
2019-04-29 09:55:13.960101 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:13.984360 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:14.005094 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:14.005150 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:14.005217 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:14.005243 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:14.029458 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62be1e590, SHUTDOWN
2019-04-29 09:55:14.030363 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:14.030469 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd632bda9e0, CONNECTING
2019-04-29 09:55:14.030536 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:14.030815 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:14.036022 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:14.036227 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd632bda9e0, TRANSIENT_FAILURE
2019-04-29 09:55:14.036300 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:14.357938 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:14.357965 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:14.664177 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62d65ba10, CONNECTING
2019-04-29 09:55:14.664244 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:14.703614 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:14.779577 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:14.788402 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:14.801606 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 10ms
2019-04-29 09:55:14.813272 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 20ms
2019-04-29 09:55:14.839613 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 40ms
2019-04-29 09:55:14.881031 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 80ms
2019-04-29T09:55:14.913-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:15.001638 I | http: Accept error: accept tcp 127.0.0.1:9003: accept: too many open files; retrying in 5ms
2019-04-29 09:55:15.085213 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd632bda9e0, CONNECTING
2019-04-29 09:55:15.085247 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:15.169226 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:15.169257 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:15.169295 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:15.169341 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:15.177237 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd62d65ba10, SHUTDOWN
2019-04-29 09:55:15.177386 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:15.177641 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd635b8d270, CONNECTING
2019-04-29 09:55:15.178140 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:15.178262 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:55:15.330-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 2, "address": "127.0.0.1:9010"}
2019-04-29T09:55:15.587-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "instance": 2, "address": "127.0.0.1:9012"}
2019-04-29T09:55:15.588-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "instance": 2, "address": "127.0.0.1:9011"}
2019-04-29T09:55:15.590-0700	ERROR	start server error	{"cache-policy": "recently_read", "error": "could not open httpjson interface 127.0.0.1:9013: listen tcp 127.0.0.1:9013: socket: too many open files"}
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:578
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFetchTaggedQuorumAddNodeAllUp
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fetch_tagged_quorum_test.go:207
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:55:16.858-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:55:16.858-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 0, "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:55:16.858-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:55:16.858-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 1, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:16.966480 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:16.966508 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:17.117882 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:17.117955 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd632bda9e0, TRANSIENT_FAILURE
2019-04-29 09:55:17.117974 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:17.335421 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:17.335580 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd635b8d270, TRANSIENT_FAILURE
2019-04-29 09:55:17.335623 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:17.575689 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd632bda9e0, CONNECTING
2019-04-29 09:55:17.575851 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:17.681676 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:17.681704 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:17.681807 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:17.681833 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:17.687113 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd632bda9e0, SHUTDOWN
2019-04-29 09:55:17.687270 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:17.965803 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6375b4a50, CONNECTING
2019-04-29 09:55:17.965829 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:17.965867 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:18.450717 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd635b8d270, CONNECTING
2019-04-29 09:55:18.450779 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:18.631029 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:18.631059 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:18.631108 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:18.631128 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:18.642185 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd635b8d270, SHUTDOWN
2019-04-29 09:55:18.642214 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:18.642283 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd637a1f410, CONNECTING
2019-04-29 09:55:18.642322 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:18.642400 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:18.853306 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:19.253488 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:19.253514 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:19.253601 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:19.253615 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:19.253796 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6375b4a50, TRANSIENT_FAILURE
2019-04-29 09:55:19.253887 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:19.345842 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:19.345882 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:19.467726 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:19.467755 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:19.842799 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6375b4a50, CONNECTING
2019-04-29 09:55:19.842855 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:55:19.877-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:20.258807 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:20.258847 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6375b4a50, TRANSIENT_FAILURE
2019-04-29 09:55:20.258879 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:20.487941 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:20.488017 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:20.830785 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:20.830814 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:20.830851 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:20.830875 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:21.758509 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:55:21.758620 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6375b4a50, SHUTDOWN
2019-04-29 09:55:21.758659 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:21.758686 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638345d40, CONNECTING
2019-04-29 09:55:21.758716 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:21.758773 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:21.982677 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:21.982709 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:21.982824 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:21.982846 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:21.988555 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd637a1f410, SHUTDOWN
2019-04-29 09:55:21.988603 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:21.988619 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638609fa0, CONNECTING
2019-04-29 09:55:21.988638 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:21.988739 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:22.052138 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:22.052205 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638609fa0, TRANSIENT_FAILURE
2019-04-29 09:55:22.052229 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:22.987370 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638609fa0, CONNECTING
2019-04-29 09:55:22.987616 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:55:23.394-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:23.831200 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:23.831271 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:23.831348 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:23.831395 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:23.834086 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638345d40, SHUTDOWN
2019-04-29 09:55:23.834113 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:23.834152 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638babec0, CONNECTING
2019-04-29 09:55:23.834163 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:23.834216 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:23.980725 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:23.980862 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638babec0, TRANSIENT_FAILURE
2019-04-29 09:55:24.004673 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:24.873751 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:24.873875 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:24.881735 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638babec0, CONNECTING
2019-04-29 09:55:24.881769 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:25.103393 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:25.103423 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:25.103463 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:25.103480 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:25.118767 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638609fa0, SHUTDOWN
2019-04-29 09:55:25.118791 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:25.118814 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638dfefb0, CONNECTING
2019-04-29 09:55:25.118822 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:25.118854 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
--- FAIL: TestFetchTaggedQuorumAddNodeAllUp (62.87s)
	assertions.go:221:                         	Error Trace:	fetch_tagged_quorum_test.go:207
			Error:		Received unexpected error "could not open httpjson interface 127.0.0.1:9013: listen tcp 127.0.0.1:9013: socket: too many open files"
		
=== RUN   TestFilesystemBootstrapIndexWithIndexingEnabled
2019-04-29 09:55:26.106216 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:26.106332 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638dfefb0, TRANSIENT_FAILURE
2019-04-29 09:55:26.106361 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:26.622817 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638dfefb0, CONNECTING
2019-04-29 09:55:26.622923 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:26.898646 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:26.898890 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:26.899051 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:26.899128 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:26.921857 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638babec0, SHUTDOWN
2019-04-29 09:55:26.921986 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:26.922102 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd639a2faf0, CONNECTING
2019-04-29 09:55:26.923363 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:26.923423 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:55:26.957-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:28.686515 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:28.686559 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:28.686646 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:28.686667 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:28.688667 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd638dfefb0, SHUTDOWN
2019-04-29 09:55:28.688694 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:28.688719 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd65bc2e270, CONNECTING
2019-04-29 09:55:28.688731 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:28.688772 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:28.969489 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:28.969525 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:29.951450 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:29.951477 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:29.951546 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:29.951573 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:29.973658 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:30.124625 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd639a2faf0, SHUTDOWN
2019-04-29 09:55:30.124681 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:30.124705 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd674c243d0, CONNECTING
2019-04-29 09:55:30.124727 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:30.124754 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd674c243d0, TRANSIENT_FAILURE
2019-04-29 09:55:30.124765 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:30.125078 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:55:30.796-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:31.035608 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd674c243d0, CONNECTING
2019-04-29 09:55:31.035646 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:31.689928 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:31.689956 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:31.689996 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:31.690029 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:31.691684 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd65bc2e270, SHUTDOWN
2019-04-29 09:55:31.691733 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:31.691765 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd679a9f7b0, CONNECTING
2019-04-29 09:55:31.691807 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:31.691865 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:32.234747 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:32.234826 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd679a9f7b0, TRANSIENT_FAILURE
2019-04-29 09:55:32.234849 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:32.913419 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd679a9f7b0, CONNECTING
2019-04-29 09:55:32.913443 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:33.103631 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:33.103714 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:33.103748 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:33.103786 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:33.257674 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd674c243d0, SHUTDOWN
2019-04-29 09:55:33.257716 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:33.257781 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67ab1cbd0, CONNECTING
2019-04-29 09:55:33.257811 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:33.257924 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:34.231761 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:34.231854 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67ab1cbd0, TRANSIENT_FAILURE
2019-04-29 09:55:34.231874 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:34.824946 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:34.824972 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:34.827264 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67ab1cbd0, CONNECTING
2019-04-29 09:55:34.827320 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:34.881495 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:34.881613 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:34.881703 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:34.881760 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:34.902439 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd679a9f7b0, SHUTDOWN
2019-04-29 09:55:34.902509 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:34.902530 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67c884db0, CONNECTING
2019-04-29 09:55:34.902550 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:34.902603 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:55:35.423-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:55:35.424-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:55:35.424-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:55:35.425-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:55:35.439-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:55:35.440-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:55:35.440-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:55:35.492-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:55:35.867-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrapIndexWithIndexingEnabled
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_index_test.go:143
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:55:35.867-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrapIndexWithIndexingEnabled
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_index_test.go:143
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:55:36.516981 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:36.517009 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:36.517084 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:36.517129 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:36.939921 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67ab1cbd0, SHUTDOWN
2019-04-29 09:55:36.939948 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:36.940011 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67e477d50, CONNECTING
2019-04-29 09:55:36.940036 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:36.940591 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:38.535844 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:38.535982 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:38.536186 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:38.536249 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:38.556444 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67c884db0, SHUTDOWN
2019-04-29 09:55:38.556652 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:38.556756 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67f24e1a0, CONNECTING
2019-04-29 09:55:38.556782 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:38.556833 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:38.944134 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:38.944342 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:39.737660 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:39.737782 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:39.737871 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:39.738195 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:39.743873 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67e477d50, SHUTDOWN
2019-04-29 09:55:39.743909 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:39.743924 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67f888ae0, CONNECTING
2019-04-29 09:55:39.743939 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:39.743991 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:40.139168 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:40.139273 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67f888ae0, TRANSIENT_FAILURE
2019-04-29 09:55:40.139288 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29T09:55:40.454-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:40.864493 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:40.864533 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:40.866519 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67f888ae0, CONNECTING
2019-04-29 09:55:40.866679 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:41.874196 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:41.874244 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:41.874459 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:41.874513 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:42.341910 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:42.366699 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67f24e1a0, SHUTDOWN
2019-04-29 09:55:42.367070 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:42.367353 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6831508f0, CONNECTING
2019-04-29 09:55:42.367607 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:42.367678 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6831508f0, TRANSIENT_FAILURE
2019-04-29 09:55:42.367738 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:42.367917 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:43.096081 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:43.096116 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:43.096183 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:43.096210 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:43.105912 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6831508f0, CONNECTING
2019-04-29 09:55:43.105939 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:43.212800 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd67f888ae0, SHUTDOWN
2019-04-29 09:55:43.212850 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:43.212869 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd684211070, CONNECTING
2019-04-29 09:55:43.213049 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:43.213096 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:43.723387 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:43.723485 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:44.695927 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:44.696150 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd684211070, TRANSIENT_FAILURE
2019-04-29 09:55:44.696238 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:44.965093 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:44.965455 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:44.966019 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:44.966186 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:44.980792 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6831508f0, SHUTDOWN
2019-04-29 09:55:44.981428 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:44.981514 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd684afeb60, CONNECTING
2019-04-29 09:55:44.981590 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:44.981988 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:45.075345 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:45.075375 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:55:45.098-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:45.191471 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd684211070, CONNECTING
2019-04-29 09:55:45.191570 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:46.605112 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:46.605180 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd684211070, TRANSIENT_FAILURE
2019-04-29 09:55:46.605195 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:47.037239 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:47.037350 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:47.051734 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:47.051761 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:47.052003 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd684211070, CONNECTING
2019-04-29 09:55:47.052021 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:47.078976 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:47.079598 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:47.079757 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:47.079895 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:47.323112 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd684211070, SHUTDOWN
2019-04-29 09:55:47.323492 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:47.323833 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68722b1d0, CONNECTING
2019-04-29 09:55:47.323963 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:47.324026 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:48.453448 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:48.453478 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:48.453539 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:48.453554 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:48.502398 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd684afeb60, SHUTDOWN
2019-04-29 09:55:48.502539 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:51.752002 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd687c4c9d0, CONNECTING
2019-04-29 09:55:51.752043 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:51.752073 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:51.752089 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:51.752129 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68722b1d0, TRANSIENT_FAILURE
2019-04-29 09:55:51.752154 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:52.074466 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:52.074508 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:52.074538 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:52.074556 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:52.074566 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:52.074583 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:52.074643 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:52.074672 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:52.077065 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:55:52.077415 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68722b1d0, SHUTDOWN
2019-04-29 09:55:52.077649 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:52.077772 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd688d232e0, CONNECTING
2019-04-29 09:55:52.078671 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:52.078751 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:53.443136 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:53.443286 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:53.480254 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:53.975019 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:53.975073 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:53.975118 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29T09:55:54.018-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:54.056641 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:54.056781 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:54.056866 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:54.056935 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:54.057037 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:54.057086 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:54.061625 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd687c4c9d0, SHUTDOWN
2019-04-29 09:55:54.061814 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:54.061913 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd689879a40, CONNECTING
2019-04-29 09:55:54.062066 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:54.062192 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:54.081385 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd688d232e0, SHUTDOWN
2019-04-29 09:55:54.081408 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:54.081451 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd689b7dfe0, CONNECTING
2019-04-29 09:55:54.081473 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:54.081533 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:54.591854 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:54.591912 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd689879a40, TRANSIENT_FAILURE
2019-04-29 09:55:54.591929 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:55.499578 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:55.499607 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:55.501169 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:55.501190 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:55.871191 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd689879a40, CONNECTING
2019-04-29 09:55:55.871215 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:56.447177 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:56.447235 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd689879a40, TRANSIENT_FAILURE
2019-04-29 09:55:56.447253 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:56.516655 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:56.516682 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:56.516721 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:56.516748 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:56.519643 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:55:56.519674 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd689879a40, SHUTDOWN
2019-04-29 09:55:56.519696 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:56.519707 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68bdf04a0, CONNECTING
2019-04-29 09:55:56.519721 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:56.519757 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:56.535437 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:56.535471 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:56.778644 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:56.778785 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:56.778871 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:56.778927 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:56.780361 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd689b7dfe0, SHUTDOWN
2019-04-29 09:55:56.780407 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:56.780435 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68c715e30, CONNECTING
2019-04-29 09:55:56.780446 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:56.780515 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:55:56.810-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:55:56.836-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:55:56.842-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:55:56.847-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
{"level":"info","ts":1556556956.8674893,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556539200,"to":1556546400,"range":7200}
2019-04-29T09:55:56.983-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
{"level":"info","ts":1556556957.0320933,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs1","from":1556539200,"to":1556546400,"range":7200,"shards":12}
{"level":"info","ts":1556556957.0321422,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556957.0322902,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
{"level":"info","ts":1556556957.4492931,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs1","from":1556539200,"to":1556546400,"range":7200,"shards":12,"took":0.417095105,"numSeries":0}
{"level":"info","ts":1556556957.4495578,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556539200,"to":1556546400,"range":7200,"took":0.578601808}
{"level":"info","ts":1556556957.4495933,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556560800,"range":14400}
{"level":"info","ts":1556556957.5208495,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs1","from":1556546400,"to":1556560800,"range":14400,"shards":12}
{"level":"info","ts":1556556957.5209177,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556957.5209935,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
{"level":"info","ts":1556556957.5358186,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs1","from":1556546400,"to":1556560800,"range":14400,"shards":12,"took":0.014895881,"numSeries":0}
{"level":"info","ts":1556556957.5359771,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556560800,"range":14400,"took":0.08637092}
{"level":"info","ts":1556556957.5361042,"msg":"bootstrapping shards for range starting","run":"bootstrap-index","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556539200,"to":1556539200,"range":0}
{"level":"info","ts":1556556957.5361302,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-index","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556539200,"to":1556539200,"range":0,"took":0.000005405}
{"level":"info","ts":1556556957.536156,"msg":"bootstrapping shards for range starting","run":"bootstrap-index","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556539200,"to":1556568000,"range":28800}
{"level":"info","ts":1556556957.8323345,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs1","from":1556539200,"to":1556560800,"range":21600,"shards":12}
2019-04-29T09:55:57.931-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:55:58.411280 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:58.500192 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68c715e30, TRANSIENT_FAILURE
2019-04-29 09:55:58.501871 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:58.594292 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:58.594737 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:55:58.597969 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68c715e30, CONNECTING
2019-04-29 09:55:58.598019 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
{"level":"info","ts":1556556958.7144039,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs1","from":1556539200,"to":1556560800,"range":21600,"shards":12,"took":0.882014188,"numBlocks":2,"numSegments":2}
{"level":"info","ts":1556556958.7148654,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-index","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556539200,"to":1556568000,"range":28800,"took":1.178672879}
2019-04-29T09:55:58.714-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:55:58.736-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
{"level":"info","ts":1556556958.7368262,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556539200,"to":1556546400,"range":7200}
{"level":"info","ts":1556556958.9486609,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs2","from":1556539200,"to":1556546400,"range":7200,"shards":12}
{"level":"info","ts":1556556958.9487627,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs2"}
{"level":"info","ts":1556556958.9487967,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs2","shards":12}
{"level":"info","ts":1556556959.1140747,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs2","from":1556539200,"to":1556546400,"range":7200,"shards":12,"took":0.16535075,"numSeries":0}
{"level":"info","ts":1556556959.1142557,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556539200,"to":1556546400,"range":7200,"took":0.377387816}
{"level":"info","ts":1556556959.114283,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556560800,"range":14400}
{"level":"info","ts":1556556959.1470013,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs2","from":1556546400,"to":1556560800,"range":14400,"shards":12}
{"level":"info","ts":1556556959.1471238,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs2"}
{"level":"info","ts":1556556959.1471457,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs2","shards":12}
{"level":"info","ts":1556556959.1851597,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs2","from":1556546400,"to":1556560800,"range":14400,"shards":12,"took":0.038127916,"numSeries":0}
{"level":"info","ts":1556556959.185293,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556560800,"range":14400,"took":0.070995387}
{"level":"info","ts":1556556959.1853218,"msg":"bootstrapping shards for range starting","run":"bootstrap-index","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556539200,"to":1556539200,"range":0}
{"level":"info","ts":1556556959.185336,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-index","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556539200,"to":1556539200,"range":0,"took":0.000003375}
{"level":"info","ts":1556556959.1853583,"msg":"bootstrapping shards for range starting","run":"bootstrap-index","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556539200,"to":1556568000,"range":28800}
{"level":"info","ts":1556556959.4034235,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs2","from":1556539200,"to":1556560800,"range":21600,"shards":12}
2019-04-29 09:55:59.697872 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:59.697975 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:59.698026 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:59.698073 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:59.723306 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68bdf04a0, SHUTDOWN
2019-04-29 09:55:59.723344 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:55:59.723409 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68ec6e560, CONNECTING
2019-04-29 09:55:59.723421 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:55:59.723464 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:59.814284 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:55:59.851650 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:55:59.851850 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:59.852021 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:55:59.854473 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68c715e30, SHUTDOWN
2019-04-29 09:55:59.854502 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:55:59.854573 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68ed216d0, CONNECTING
2019-04-29 09:55:59.854598 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:55:59.854664 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:55:59.958808 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:55:59.958871 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68ec6e560, TRANSIENT_FAILURE
2019-04-29 09:55:59.958885 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:00.051300 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:00.051549 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68ed216d0, TRANSIENT_FAILURE
2019-04-29 09:56:00.120554 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:00.120670 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:00.120728 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
{"level":"info","ts":1556556960.1753702,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs2","from":1556539200,"to":1556560800,"range":21600,"shards":12,"took":0.769998636,"numBlocks":2,"numSegments":2}
{"level":"info","ts":1556556960.175638,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-index","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556539200,"to":1556568000,"range":28800,"took":0.990235695}
2019-04-29T09:56:00.175-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:56:00.243-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:56:00.329-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29 09:56:00.833387 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68ec6e560, CONNECTING
2019-04-29 09:56:00.833429 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:00.853639 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68ed216d0, CONNECTING
2019-04-29 09:56:00.853672 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:00.870029 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:00.870056 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:56:01.034-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
{"level":"info","ts":1556556961.5873866,"msg":"successfully updated topology","numHosts":1}
2019-04-29 09:56:02.725381 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:02.726837 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:02.726955 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:02.727012 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:02.755042 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68ec6e560, SHUTDOWN
2019-04-29 09:56:02.755102 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:02.755139 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6b94726d0, CONNECTING
2019-04-29 09:56:02.755178 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:02.755246 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:02.933058 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:02.933104 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:02.933131 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:02.933156 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:02.937493 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd68ed216d0, SHUTDOWN
2019-04-29 09:56:02.937559 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:02.937585 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6ba2b84c0, CONNECTING
2019-04-29 09:56:02.937594 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:02.937643 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:03.007756 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:03.007792 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:05.208671 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:05.208699 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:56:05.408-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:05.726744 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:05.726881 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:05.727017 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:05.727096 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:05.729365 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6b94726d0, SHUTDOWN
2019-04-29 09:56:05.729389 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:05.729425 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bb8e46e0, CONNECTING
2019-04-29 09:56:05.729439 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:05.729478 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:05.945655 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:05.945714 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:05.945763 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:05.945793 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:05.950503 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6ba2b84c0, SHUTDOWN
2019-04-29 09:56:05.950983 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:05.951031 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bb901a10, CONNECTING
2019-04-29 09:56:05.951068 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:05.951152 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:05.971158 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:05.971403 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bb901a10, TRANSIENT_FAILURE
2019-04-29 09:56:05.971481 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:06.170191 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:06.170256 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bb8e46e0, TRANSIENT_FAILURE
2019-04-29 09:56:06.170318 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:07.111412 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bb8e46e0, CONNECTING
2019-04-29 09:56:07.111566 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:07.165173 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bb901a10, CONNECTING
2019-04-29 09:56:07.165205 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:08.485473 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:08.485514 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
--- PASS: TestFilesystemBootstrapIndexWithIndexingEnabled (42.73s)
=== RUN   TestFilesystemBootstrapMultipleNamespaces
2019-04-29 09:56:08.733110 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:08.733255 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:08.733352 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:08.733542 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:08.745822 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bb8e46e0, SHUTDOWN
2019-04-29 09:56:08.745869 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:08.745884 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bc38b7b0, CONNECTING
2019-04-29 09:56:08.745897 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:08.745999 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:08.770-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:09.071599 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:09.071634 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:09.071684 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:09.071707 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:09.128693 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bb901a10, SHUTDOWN
2019-04-29 09:56:09.128900 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:09.128945 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6c80b6910, CONNECTING
2019-04-29 09:56:09.128971 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:09.129023 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:10.115-0700	WARN	unable to find a single blockSize from known retention periods	{"cache-policy": "recently_read", "guessing": "6h0m0s"}
github.com/m3db/m3/src/dbnode/integration.newTestSetup
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:279
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrapMultipleNamespaces
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_multi_ns_test.go:62
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:56:10.210511 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:10.210562 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6c80b6910, TRANSIENT_FAILURE
2019-04-29 09:56:10.210600 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:10.283725 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6c80b6910, CONNECTING
2019-04-29 09:56:10.283753 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:56:10.507-0700	INFO	generating data	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.923-0700	INFO	generated data	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.923-0700	INFO	filesystem bootstrap test	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.924-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.924-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.924-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.924-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.924-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.925-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:56:10.925-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:56:11.019-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrapMultipleNamespaces
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_multi_ns_test.go:112
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:56:11.019-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrapMultipleNamespaces
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_multi_ns_test.go:112
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:56:11.735708 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:11.735880 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:11.735916 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:11.735980 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:11.737729 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6bc38b7b0, SHUTDOWN
2019-04-29 09:56:11.737778 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:11.737850 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd702d713f0, CONNECTING
2019-04-29 09:56:11.737935 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:11.738020 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:11.853-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:12.230030 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:12.230091 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:12.230116 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:12.230137 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:12.275131 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd6c80b6910, SHUTDOWN
2019-04-29 09:56:12.275207 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:12.275250 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7046df5f0, CONNECTING
2019-04-29 09:56:12.275262 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:12.275323 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:12.318449 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:12.318665 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:12.539760 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:12.539916 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd702d713f0, TRANSIENT_FAILURE
2019-04-29 09:56:12.539983 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:12.952651 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd702d713f0, CONNECTING
2019-04-29 09:56:12.952697 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:14.222269 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:14.222324 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:14.771914 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:14.772025 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:14.772116 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:14.772156 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:14.783045 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd702d713f0, SHUTDOWN
2019-04-29 09:56:14.783081 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:14.783114 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70a6e5fe0, CONNECTING
2019-04-29 09:56:14.783129 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:14.783179 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:14.987-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:15.244247 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:15.246829 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:15.247114 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:15.247189 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:15.248617 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7046df5f0, SHUTDOWN
2019-04-29 09:56:15.248641 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:15.248663 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70c2183a0, CONNECTING
2019-04-29 09:56:15.248673 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:15.248708 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:15.356-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:56:15.357-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:56:15.359-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:56:15.361-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
{"level":"info","ts":1556556975.361903,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556517600,"to":1556532000,"range":14400}
2019-04-29T09:56:15.363-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
{"level":"info","ts":1556556975.586087,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs1","from":1556517600,"to":1556532000,"range":14400,"shards":12}
{"level":"info","ts":1556556975.586188,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556975.5862136,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
{"level":"info","ts":1556556975.926155,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs1","from":1556517600,"to":1556532000,"range":14400,"shards":12,"took":0.339993845,"numSeries":0}
{"level":"info","ts":1556556975.9263375,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556517600,"to":1556532000,"range":14400,"took":0.56427214}
{"level":"info","ts":1556556975.9263632,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556532000,"to":1556546400,"range":14400}
{"level":"info","ts":1556556976.0714219,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs1","from":1556532000,"to":1556546400,"range":14400,"shards":12}
{"level":"info","ts":1556556976.0715353,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556976.0716956,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
{"level":"info","ts":1556556976.096479,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs1","from":1556532000,"to":1556546400,"range":14400,"shards":12,"took":0.025015525,"numSeries":0}
{"level":"info","ts":1556556976.0967157,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556532000,"to":1556546400,"range":14400,"took":0.170336976}
2019-04-29T09:56:16.096-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:56:16.122-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
{"level":"info","ts":1556556976.1236491,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556517600,"to":1556528400,"range":10800}
{"level":"info","ts":1556556976.16354,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs2","from":1556517600,"to":1556528400,"range":10800,"shards":12}
{"level":"info","ts":1556556976.164311,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs2"}
{"level":"info","ts":1556556976.1643336,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs2","shards":12}
{"level":"info","ts":1556556976.3317623,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs2","from":1556517600,"to":1556528400,"range":10800,"shards":12,"took":0.168182983,"numSeries":0}
{"level":"info","ts":1556556976.331907,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556517600,"to":1556528400,"range":10800,"took":0.208131802}
{"level":"info","ts":1556556976.3319225,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556528400,"to":1556550000,"range":21600}
2019-04-29 09:56:16.384302 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:16.384588 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70c2183a0, TRANSIENT_FAILURE
2019-04-29 09:56:16.385051 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
{"level":"info","ts":1556556976.5227773,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs2","from":1556528400,"to":1556550000,"range":21600,"shards":12}
{"level":"info","ts":1556556976.522866,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs2"}
{"level":"info","ts":1556556976.5229495,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs2","shards":12}
2019-04-29 09:56:16.778584 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70c2183a0, CONNECTING
2019-04-29 09:56:16.778613 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:16.779242 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:16.779278 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
{"level":"info","ts":1556556976.95851,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs2","from":1556528400,"to":1556550000,"range":21600,"shards":12,"took":0.435626386,"numSeries":0}
{"level":"info","ts":1556556976.959311,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556528400,"to":1556550000,"range":21600,"took":0.627338727}
2019-04-29T09:56:16.959-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:56:16.982-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:56:17.672-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:56:17.672-0700	INFO	server is now up	{"cache-policy": "recently_read"}
2019-04-29T09:56:17.762-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T05:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 05:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 05:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 05:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 05:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 05:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:56:17.762-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T05:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:17.780933 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:17.781031 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:17.781122 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:17.781175 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:17.782633 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70a6e5fe0, SHUTDOWN
2019-04-29 09:56:17.782774 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:17.782807 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:17.782869 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70e3e75a0, CONNECTING
2019-04-29 09:56:17.782899 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:18.109330 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:18.109370 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70c2183a0, TRANSIENT_FAILURE
2019-04-29 09:56:18.109403 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:18.120270 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:18.120551 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:18.159121 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:18.159533 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:56:18.287-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:18.325007 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:18.325065 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70e3e75a0, TRANSIENT_FAILURE
2019-04-29 09:56:18.325130 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:18.354317 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:18.404596 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:18.404847 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:18.404955 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:18.406291 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70c2183a0, CONNECTING
2019-04-29 09:56:18.406335 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:18.406351 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70c2183a0, SHUTDOWN
2019-04-29 09:56:18.406371 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:18.406385 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70e73be00, CONNECTING
2019-04-29 09:56:18.406407 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:18.406469 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:18.793685 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70e3e75a0, CONNECTING
2019-04-29 09:56:18.793743 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:20.110667 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:20.110712 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:20.800866 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:20.800897 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:20.801018 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:20.801911 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:20.802092 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:20.828826 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70e3e75a0, SHUTDOWN
2019-04-29 09:56:20.829056 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:20.829080 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70f66eba0, CONNECTING
2019-04-29 09:56:20.829090 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:21.354499 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:21.354587 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:21.354626 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:21.354664 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:21.354747 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70e73be00, SHUTDOWN
2019-04-29 09:56:21.354763 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:21.354804 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70fa48b30, CONNECTING
2019-04-29 09:56:21.354815 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:21.354874 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:21.452-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:22.188616 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:22.188671 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70fa48b30, TRANSIENT_FAILURE
2019-04-29 09:56:22.188687 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:22.357530 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70fa48b30, CONNECTING
2019-04-29 09:56:22.357558 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:56:22.833-0700	INFO	server is now down	{"cache-policy": "recently_read"}
2019-04-29 09:56:22.936541 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:22.936573 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
--- PASS: TestFilesystemBootstrapMultipleNamespaces (14.83s)
=== RUN   TestFilesystemBootstrapTagsWithIndexingDisabled
2019-04-29 09:56:23.855100 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:23.855705 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:23.857247 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:23.862517 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:23.874628 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70f66eba0, SHUTDOWN
2019-04-29 09:56:23.878634 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:23.879104 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7371c5210, CONNECTING
2019-04-29 09:56:23.879633 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:23.879868 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:24.084061 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:24.084158 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7371c5210, TRANSIENT_FAILURE
2019-04-29 09:56:24.084183 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:24.495653 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:24.495701 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:24.495745 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:24.495799 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29T09:56:24.517-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:24.596644 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd70fa48b30, SHUTDOWN
2019-04-29 09:56:24.596670 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:24.596696 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd746387340, CONNECTING
2019-04-29 09:56:24.596706 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:24.596747 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:24.867097 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7371c5210, CONNECTING
2019-04-29 09:56:24.867176 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:56:25.456-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:56:25.457-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:25.457-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:25.457-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:25.460-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:56:25.460-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:56:25.460-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:56:25.627-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrapTagsWithIndexingDisabled
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_tags_test.go:124
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:56:25.627-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrapTagsWithIndexingDisabled
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_tags_test.go:124
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:56:26.028886 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:26.028946 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd746387340, TRANSIENT_FAILURE
2019-04-29 09:56:26.028988 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:26.179164 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd746387340, CONNECTING
2019-04-29 09:56:26.216582 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:26.909369 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:26.910188 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:26.910442 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:26.910472 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:26.912185 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7371c5210, SHUTDOWN
2019-04-29 09:56:26.912316 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:26.913405 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd758a3f110, CONNECTING
2019-04-29 09:56:26.913684 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:26.913907 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:27.110766 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:27.110877 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:27.160747 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:27.160984 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:27.525679 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:27.525740 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:27.525837 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:27.525888 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:27.526381 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd746387340, SHUTDOWN
2019-04-29 09:56:27.526453 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:27.526489 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd759ffe930, CONNECTING
2019-04-29 09:56:27.526564 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:27.526694 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:27.576-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:28.865579 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:28.865872 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:29.915783 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:29.915855 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:29.915894 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:29.915929 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:29.917395 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd758a3f110, SHUTDOWN
2019-04-29 09:56:29.917420 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:29.917476 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd75dd2c7a0, CONNECTING
2019-04-29 09:56:29.917487 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:29.917561 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:29.973444 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:29.973669 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd759ffe930, TRANSIENT_FAILURE
2019-04-29 09:56:29.973787 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:29.995266 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd759ffe930, CONNECTING
2019-04-29 09:56:29.995467 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:30.112241 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:30.112300 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd759ffe930, TRANSIENT_FAILURE
2019-04-29 09:56:30.112350 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:30.280569 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:30.280627 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:30.540731 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:30.540793 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:30.540906 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:30.540931 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:30.542859 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: context canceled; please retry.
2019-04-29 09:56:30.542972 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd759ffe930, SHUTDOWN
2019-04-29 09:56:30.543056 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:30.543084 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd75eb896c0, CONNECTING
2019-04-29 09:56:30.543106 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:30.543156 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:30.926-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:56:31.122-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:56:31.124-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:56:31.126-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:56:31.128-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
{"level":"info","ts":1556556991.1285553,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556546400,"range":0}
{"level":"info","ts":1556556991.1287215,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556546400,"range":0,"took":0.000007821}
{"level":"info","ts":1556556991.1287525,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556560800,"range":14400}
{"level":"info","ts":1556556991.1411624,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs1","from":1556546400,"to":1556560800,"range":14400,"shards":12}
{"level":"info","ts":1556556991.1413982,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556556991.1414165,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
2019-04-29T09:56:31.219-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
{"level":"info","ts":1556556991.2921665,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs1","from":1556546400,"to":1556560800,"range":14400,"shards":12,"took":0.15095518,"numSeries":0}
{"level":"info","ts":1556556991.2925367,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556560800,"range":14400,"took":0.163764348}
2019-04-29T09:56:31.292-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:56:31.301-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
{"level":"info","ts":1556556991.3019037,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556546400,"range":0}
{"level":"info","ts":1556556991.301946,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556546400,"range":0,"took":0.000006943}
{"level":"info","ts":1556556991.3019586,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556560800,"range":14400}
{"level":"info","ts":1556556991.3433003,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs2","from":1556546400,"to":1556560800,"range":14400,"shards":12}
{"level":"info","ts":1556556991.3436253,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs2"}
{"level":"info","ts":1556556991.3436997,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs2","shards":12}
2019-04-29 09:56:32.206160 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:32.206223 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd75eb896c0, TRANSIENT_FAILURE
2019-04-29 09:56:32.206280 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:32.388458 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:32.388664 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:32.401299 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd75eb896c0, CONNECTING
2019-04-29 09:56:32.401335 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
{"level":"info","ts":1556556992.9256365,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs2","from":1556546400,"to":1556560800,"range":14400,"shards":12,"took":1.582007803,"numSeries":0}
{"level":"info","ts":1556556992.9257958,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556560800,"range":14400,"took":1.623792804}
2019-04-29T09:56:32.925-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29 09:56:33.010903 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:33.010941 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:33.011160 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:33.011945 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:33.136154 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd75dd2c7a0, SHUTDOWN
2019-04-29 09:56:33.136176 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:33.136219 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd76236e970, CONNECTING
2019-04-29 09:56:33.136231 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:33.136262 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:33.182368 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:33.182393 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:56:33.226-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:56:33.514-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29 09:56:33.682426 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:33.682479 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:33.682512 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:33.682554 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:33.752049 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd75eb896c0, SHUTDOWN
2019-04-29 09:56:33.752253 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:33.752316 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd76281d8f0, CONNECTING
2019-04-29 09:56:33.752407 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:33.752518 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:34.071588 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:34.071646 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd76281d8f0, TRANSIENT_FAILURE
2019-04-29 09:56:34.071659 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29T09:56:34.155-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:34.685949 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd76281d8f0, CONNECTING
2019-04-29 09:56:34.685999 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:34.773415 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:34.773450 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:56:35.337-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:56:35.337-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:36.022516 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:36.022546 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:36.022580 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:36.022596 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:36.065463 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd76236e970, SHUTDOWN
2019-04-29 09:56:36.065501 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:36.065536 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7635868c0, CONNECTING
2019-04-29 09:56:36.065545 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:36.065578 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:36.302623 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:36.302717 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7635868c0, TRANSIENT_FAILURE
2019-04-29 09:56:36.302764 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:36.688180 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:36.688301 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:36.688388 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:36.688449 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:36.690982 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd76281d8f0, SHUTDOWN
2019-04-29 09:56:36.691091 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:36.691116 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd763b23580, CONNECTING
2019-04-29 09:56:36.691386 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:36.692385 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:37.089910 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7635868c0, CONNECTING
2019-04-29 09:56:37.089966 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:56:37.387-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:38.641557 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:38.641930 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7635868c0, TRANSIENT_FAILURE
2019-04-29 09:56:38.642008 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:39.301168 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7635868c0, CONNECTING
2019-04-29 09:56:39.301416 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:39.460768 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:39.460912 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:39.467175 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:39.467504 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:39.467942 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:39.468007 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:39.468097 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:39.468178 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:39.478955 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7635868c0, SHUTDOWN
2019-04-29 09:56:39.479119 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:39.479232 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7649d0a70, CONNECTING
2019-04-29 09:56:39.479307 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:39.479479 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:39.483482 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:39.483505 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:39.724597 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:39.724631 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:39.724737 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:39.724813 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:39.729039 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd763b23580, SHUTDOWN
2019-04-29 09:56:39.729535 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:39.729657 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd764c52080, CONNECTING
2019-04-29 09:56:39.729815 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:39.730062 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:40.168580 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:40.168624 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7649d0a70, TRANSIENT_FAILURE
2019-04-29 09:56:40.168659 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:40.430176 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:40.430260 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd764c52080, TRANSIENT_FAILURE
2019-04-29 09:56:40.430452 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:40.573078 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7649d0a70, CONNECTING
2019-04-29 09:56:40.573109 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:40.766191 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd764c52080, CONNECTING
2019-04-29 09:56:40.766320 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:40.812434 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:40.812562 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
--- PASS: TestFilesystemBootstrapTagsWithIndexingDisabled (17.53s)
=== RUN   TestFilesystemBootstrap
2019-04-29T09:56:41.829-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:42.444599 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:42.445026 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7649d0a70, TRANSIENT_FAILURE
2019-04-29 09:56:42.445120 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:42.596350 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:42.596374 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:42.605175 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:42.605202 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:42.605252 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:42.605272 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:42.606898 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7649d0a70, CONNECTING
2019-04-29 09:56:42.606923 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:42.606943 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7649d0a70, SHUTDOWN
2019-04-29 09:56:42.606965 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:42.606979 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd79adaa250, CONNECTING
2019-04-29 09:56:42.606994 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:42.607048 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:42.747591 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:42.747962 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:42.748295 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:42.749079 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:42.753321 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd764c52080, SHUTDOWN
2019-04-29 09:56:42.753353 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:42.753395 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd79b4a7ca0, CONNECTING
2019-04-29 09:56:42.753407 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:42.753453 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:44.574851 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:44.574899 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd79adaa250, TRANSIENT_FAILURE
2019-04-29 09:56:44.574910 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:44.592030 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:44.592058 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:45.288905 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:45.289315 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:45.596546 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd79adaa250, CONNECTING
2019-04-29 09:56:45.596582 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:45.670832 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:45.670867 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:45.670923 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:45.670943 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:45.763421 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd79adaa250, SHUTDOWN
2019-04-29 09:56:45.763507 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:45.763563 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7a6022ee0, CONNECTING
2019-04-29 09:56:45.763577 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:45.763668 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:45.812756 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:45.812807 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:45.812838 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:45.812869 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:45.821162 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd79b4a7ca0, SHUTDOWN
2019-04-29 09:56:45.821192 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:45.821277 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7a648d720, CONNECTING
2019-04-29 09:56:45.821291 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:45.821328 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:46.104-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:46.412458 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:46.412482 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:56:46.770-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:56:46.771-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:46.771-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:46.771-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:56:46.771-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:56:46.771-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:56:46.771-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1, testNs2]", "updates": "[]", "removals": "[]"}
2019-04-29T09:56:46.834-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_test.go:99
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29T09:56:46.834-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs2"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.TestFilesystemBootstrap
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_bootstrap_test.go:99
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:56:46.912453 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:46.912831 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:48.837177 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:48.837384 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:48.837554 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:48.837609 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:48.842030 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7a6022ee0, SHUTDOWN
2019-04-29 09:56:48.842080 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:48.842096 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7ac663610, CONNECTING
2019-04-29 09:56:48.842111 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:48.842158 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:48.934143 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:48.934176 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:48.934251 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:48.934340 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:48.939396 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7a648d720, SHUTDOWN
2019-04-29 09:56:48.939475 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:48.939522 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7ac9b2390, CONNECTING
2019-04-29 09:56:48.939536 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:48.939639 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:49.388-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:49.959904 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:49.959965 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:51.841537 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:51.841667 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:51.841780 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:51.841893 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:51.842757 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7ac663610, SHUTDOWN
2019-04-29 09:56:51.842914 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:51.843011 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b5cbff80, CONNECTING
2019-04-29 09:56:51.843100 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:51.843179 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:51.892-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:56:51.906-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:56:51.907-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:56:51.909-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
{"level":"info","ts":1556557011.9098048,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556546400,"range":0}
{"level":"info","ts":1556557011.909989,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556546400,"range":0,"took":0.000052198}
{"level":"info","ts":1556557011.9100647,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556560800,"range":14400}
2019-04-29T09:56:51.911-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
{"level":"info","ts":1556557011.994721,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs1","from":1556546400,"to":1556560800,"range":14400,"shards":12}
{"level":"info","ts":1556557011.994779,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs1"}
{"level":"info","ts":1556557011.9948053,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs1","shards":12}
2019-04-29 09:56:51.999208 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:51.999319 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:51.999389 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:51.999453 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:52.014898 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7ac9b2390, SHUTDOWN
2019-04-29 09:56:52.014939 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:52.014965 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b63d5390, CONNECTING
2019-04-29 09:56:52.014973 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:52.015005 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:52.027922 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:52.028006 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b5cbff80, TRANSIENT_FAILURE
2019-04-29 09:56:52.028021 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:52.076679 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:52.076867 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b63d5390, TRANSIENT_FAILURE
2019-04-29 09:56:52.076956 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
{"level":"info","ts":1556557012.1442547,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs1","from":1556546400,"to":1556560800,"range":14400,"shards":12,"took":0.149484786,"numSeries":0}
{"level":"info","ts":1556557012.144471,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs1","numShards":12,"from":1556546400,"to":1556560800,"range":14400,"took":0.234269202}
2019-04-29T09:56:52.144-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:56:52.411-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
{"level":"info","ts":1556557012.4111824,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556546400,"range":0}
{"level":"info","ts":1556557012.4113088,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556546400,"range":0,"took":0.000018026}
{"level":"info","ts":1556557012.4113455,"msg":"bootstrapping shards for range starting","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556560800,"range":14400}
2019-04-29 09:56:52.438227 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:52.438875 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
{"level":"info","ts":1556557012.5771387,"msg":"bootstrapping from source starting","source":"filesystem","namespace":"testNs2","from":1556546400,"to":1556560800,"range":14400,"shards":12}
{"level":"info","ts":1556557012.5771995,"msg":"filesystem bootstrapper resolving block retriever","namespace":"testNs2"}
{"level":"info","ts":1556557012.5772238,"msg":"filesystem bootstrapper caching block retriever shard indices","namespace":"testNs2","shards":12}
2019-04-29 09:56:52.899638 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b5cbff80, CONNECTING
2019-04-29 09:56:52.899789 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:56:53.016-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:53.021233 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b63d5390, CONNECTING
2019-04-29 09:56:53.031265 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:53.031290 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:53.031332 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
{"level":"info","ts":1556557013.1104088,"msg":"bootstrapping from source completed successfully","source":"filesystem","namespace":"testNs2","from":1556546400,"to":1556560800,"range":14400,"shards":12,"took":0.53315692,"numSeries":0}
{"level":"info","ts":1556557013.1111221,"msg":"bootstrapping shards for range completed successfully","run":"bootstrap-data","bootstrapper":"base","namespace":"testNs2","numShards":12,"from":1556546400,"to":1556560800,"range":14400,"took":0.699713872}
2019-04-29T09:56:53.111-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs2", "numShards": 12, "numSeries": 0}
2019-04-29T09:56:53.123-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs2", "duration": "0s"}
2019-04-29T09:56:53.209-0700	INFO	started server	{"cache-policy": "recently_read"}
2019-04-29T09:56:54.241-0700	ERROR	error when cleaning up data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "encountered errors when cleaning up data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up index files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive data files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive snapshot files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when deleting inactive namespace files for 2019-04-29 09:00:00 -0700 PDT: database is closed\nencountered errors when cleaning up snapshot and commitlog files: database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:134
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29T09:56:54.241-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "time": "2019-04-29T09:00:00.000-0700", "error": "database is closed"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:54.804528 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:54.804733 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:54.842247 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:54.842604 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:54.842705 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:54.842834 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:54.844529 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b5cbff80, SHUTDOWN
2019-04-29 09:56:54.844592 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:54.844655 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b8b29b40, CONNECTING
2019-04-29 09:56:54.844664 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:54.844679 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:55.013358 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:55.013390 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:55.013442 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:55.013460 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:55.015610 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b63d5390, SHUTDOWN
2019-04-29 09:56:55.015636 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:55.015670 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b8d49ab0, CONNECTING
2019-04-29 09:56:55.015680 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:55.015718 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:56:56.462-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:56:56.692957 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:56.692980 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:56:57.877882 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:57.877943 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:57.877984 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:57.878018 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:57.880848 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b8b29b40, SHUTDOWN
2019-04-29 09:56:57.880915 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:56:57.880948 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b972ce10, CONNECTING
2019-04-29 09:56:57.880969 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:57.881033 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:58.166586 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:56:58.166689 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:56:58.166790 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:58.166838 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:56:58.175453 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b8d49ab0, SHUTDOWN
2019-04-29 09:56:58.175525 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:56:58.175578 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b982b370, CONNECTING
2019-04-29 09:56:58.175591 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:56:58.175630 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:56:58.403580 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:58.403694 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b972ce10, TRANSIENT_FAILURE
2019-04-29 09:56:58.403719 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
--- PASS: TestFilesystemBootstrap (17.97s)
=== RUN   TestFsCommitLogMixedModeReadWrite
2019-04-29 09:56:58.907084 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b972ce10, CONNECTING
2019-04-29 09:56:58.907109 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:56:59.309945 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:56:59.310024 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:56:59.715-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:00.981375 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:00.981403 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:00.982039 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:00.982058 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:00.982109 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:00.982126 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:01.045391 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b972ce10, SHUTDOWN
2019-04-29 09:57:01.045432 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:01.045455 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7f4e97a70, CONNECTING
2019-04-29 09:57:01.045492 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:01.045620 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:01.247709 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:01.247883 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:01.248096 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:01.248185 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:01.252921 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7b982b370, SHUTDOWN
2019-04-29 09:57:01.253013 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:01.253056 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7f6d39060, CONNECTING
2019-04-29 09:57:01.253065 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:01.253102 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:02.807112 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:02.807309 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:57:03.561-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:04.027118 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:04.027152 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:04.027257 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:04.027293 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:04.075083 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7f4e97a70, SHUTDOWN
2019-04-29 09:57:04.075110 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:04.075174 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7fbf0c760, CONNECTING
2019-04-29 09:57:04.075184 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:04.075247 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:04.473805 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:04.637929 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:04.638099 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:04.638171 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29T09:57:04.598-0700	INFO	commit log & fileset files, write, read, and merge bootstrap test	{"cache-policy": "recently_read"}
2019-04-29 09:57:04.774486 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7f6d39060, SHUTDOWN
2019-04-29 09:57:04.775861 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:04.776064 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7fc197a30, CONNECTING
2019-04-29 09:57:04.786087 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:04.786259 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:05.238125 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:05.238265 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:57:05.460-0700	INFO	starting server	{"cache-policy": "recently_read"}
2019-04-29T09:57:05.460-0700	INFO	cluster database initializing topology	{"cache-policy": "recently_read"}
2019-04-29T09:57:05.460-0700	INFO	cluster database resolving topology	{"cache-policy": "recently_read"}
2019-04-29T09:57:05.460-0700	INFO	cluster database resolved topology	{"cache-policy": "recently_read"}
2019-04-29T09:57:05.462-0700	INFO	creating namespaces watch	{"cache-policy": "recently_read"}
2019-04-29T09:57:05.462-0700	INFO	resolving namespaces with namespace watch	{"cache-policy": "recently_read"}
2019-04-29T09:57:05.462-0700	INFO	updating database namespaces	{"cache-policy": "recently_read", "adds": "[testNs1]", "updates": "[]", "removals": "[]"}
2019-04-29T09:57:05.498-0700	WARN	skip updating namespace schema to empty	{"cache-policy": "recently_read", "namespace": "testNs1"}
github.com/m3db/m3/src/dbnode/storage.(*db).UpdateSchemaRegistry
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:243
github.com/m3db/m3/src/dbnode/storage.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/database.go:215
github.com/m3db/m3/src/dbnode/storage/cluster.NewDatabase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/cluster/database.go:116
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServerBase
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:540
github.com/m3db/m3/src/dbnode/integration.(*testSetup).startServer
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/setup.go:519
github.com/m3db/m3/src/dbnode/integration.startServerWithNewInspection
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_commitlog_mixed_mode_read_write_test.go:175
github.com/m3db/m3/src/dbnode/integration.TestFsCommitLogMixedModeReadWrite
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/fs_commitlog_mixed_mode_read_write_test.go:82
testing.tRunner
	/usr/local/go/src/testing/testing.go:777
2019-04-29 09:57:05.675089 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:05.675128 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:05.838996 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:05.839050 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:06.380297 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:06.380349 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7fc197a30, TRANSIENT_FAILURE
2019-04-29 09:57:06.380361 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:06.572764 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7fc197a30, CONNECTING
2019-04-29 09:57:06.572800 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29T09:57:06.882-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:07.029806 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:07.035923 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:07.036111 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:07.036534 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:07.038228 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7fbf0c760, SHUTDOWN
2019-04-29 09:57:07.038277 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:07.038294 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd804e71e90, CONNECTING
2019-04-29 09:57:07.038312 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:07.038366 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:07.534309 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:07.534350 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:07.534421 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:07.534448 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:07.538938 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd7fc197a30, SHUTDOWN
2019-04-29 09:57:07.539006 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:07.539032 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8080b2b60, CONNECTING
2019-04-29 09:57:07.539058 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:07.539115 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:07.951427 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:07.951505 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:08.840880 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:08.841029 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:08.935804 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:08.935901 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:57:09.238-0700	INFO	node tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9003"}
2019-04-29T09:57:09.239-0700	INFO	node httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9002"}
2019-04-29T09:57:09.241-0700	INFO	cluster tchannelthrift: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9001"}
2019-04-29T09:57:09.242-0700	INFO	cluster httpjson: listening	{"cache-policy": "recently_read", "address": "127.0.0.1:9000"}
2019-04-29T09:57:09.243-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2017-02-13T12:00:00.000-0800", "to": "2017-02-13T15:00:00.000-0800", "range": "3h0m0s"}
2019-04-29T09:57:09.244-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:57:09.244-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:57:09.244-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2017-02-13T12:00:00.000-0800", "to": "2017-02-13T15:00:00.000-0800", "range": "3h0m0s", "shards": 12}
2019-04-29T09:57:09.247-0700	WARN	debug server could not listen	{"cache-policy": "recently_read", "address": "127.0.0.1:9004", "error": "listen tcp 127.0.0.1:9004: bind: address already in use"}
github.com/m3db/m3/src/dbnode/integration.openAndServe.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/integration/serve.go:139
2019-04-29T09:57:09.271-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:57:09.272-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "785.834µs"}
2019-04-29T09:57:09.272-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 0, "datapointsRead": 0}
2019-04-29T09:57:09.272-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2017-02-13T12:00:00.000-0800", "to": "2017-02-13T15:00:00.000-0800", "range": "3h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:57:09.272-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2017-02-13T12:00:00.000-0800", "to": "2017-02-13T15:00:00.000-0800", "range": "3h0m0s", "took": "0s"}
2019-04-29T09:57:09.272-0700	INFO	bootstrapping shards for range starting	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2017-02-13T15:00:00.000-0800", "to": "2017-02-13T16:00:00.000-0800", "range": "1h0m0s"}
2019-04-29T09:57:09.272-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12}
2019-04-29T09:57:09.272-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "filesystem", "namespace": "testNs1", "from": "1754-08-30T22:43:41.128Z", "to": "1754-08-30T22:43:41.128Z", "range": "0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:57:09.273-0700	INFO	bootstrapping from source starting	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2017-02-13T15:00:00.000-0800", "to": "2017-02-13T16:00:00.000-0800", "range": "1h0m0s", "shards": 12}
2019-04-29T09:57:09.274-0700	INFO	starting merge...	{"cache-policy": "recently_read", "bootstrapper": "commitlog"}
2019-04-29T09:57:09.274-0700	INFO	done merging...	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "took": "119.897µs"}
2019-04-29T09:57:09.274-0700	INFO	ReadData finished	{"cache-policy": "recently_read", "bootstrapper": "commitlog", "seriesSkipped": 0, "datapointsSkipped": 0, "datapointsRead": 0}
2019-04-29T09:57:09.274-0700	INFO	bootstrapping from source completed successfully	{"cache-policy": "recently_read", "source": "commitlog", "namespace": "testNs1", "from": "2017-02-13T15:00:00.000-0800", "to": "2017-02-13T16:00:00.000-0800", "range": "1h0m0s", "shards": 12, "took": "0s", "numSeries": 0}
2019-04-29T09:57:09.274-0700	INFO	bootstrapping shards for range completed successfully	{"cache-policy": "recently_read", "run": "bootstrap-data", "bootstrapper": "base", "namespace": "testNs1", "numShards": 12, "from": "2017-02-13T15:00:00.000-0800", "to": "2017-02-13T16:00:00.000-0800", "range": "1h0m0s", "took": "0s"}
2019-04-29T09:57:09.274-0700	INFO	bootstrap data fetched now initializing shards with series blocks	{"cache-policy": "recently_read", "namespace": "testNs1", "numShards": 12, "numSeries": 0}
2019-04-29T09:57:09.275-0700	INFO	bootstrap finished	{"cache-policy": "recently_read", "namespace": "testNs1", "duration": "0s"}
2019-04-29 09:57:10.032525 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:10.032631 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:10.032841 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:10.035584 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:10.035887 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:10.035965 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd804e71e90, SHUTDOWN
2019-04-29 09:57:10.036001 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:10.036055 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd80ecbd450, CONNECTING
2019-04-29 09:57:10.036223 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29T09:57:10.040-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:10.655123 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:10.655159 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:10.655353 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:10.655391 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:10.664219 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8080b2b60, SHUTDOWN
2019-04-29 09:57:10.664247 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:10.664346 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd80efcd7c0, CONNECTING
2019-04-29 09:57:10.664358 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:10.664414 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:10.896143 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:10.896179 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:10.931848 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:10.931878 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:13.042351 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:13.042482 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:13.042851 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:13.042868 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:13.042915 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:13.042931 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:13.044820 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd80ecbd450, SHUTDOWN
2019-04-29 09:57:13.044921 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:13.044966 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8102649a0, CONNECTING
2019-04-29 09:57:13.044977 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:13.045010 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:57:13.106-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:13.660881 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:13.660937 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:13.660990 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:13.661032 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:13.669929 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd80efcd7c0, SHUTDOWN
2019-04-29 09:57:13.669974 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:13.669987 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd81071be10, CONNECTING
2019-04-29 09:57:13.670001 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:13.670040 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:13.695249 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:13.695364 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:14.850260 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:14.850287 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:15.017799 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:15.017833 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:16.044051 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:16.044144 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:16.044186 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:16.044239 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:16.046000 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8102649a0, SHUTDOWN
2019-04-29 09:57:16.046146 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:16.046225 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8115f2540, CONNECTING
2019-04-29 09:57:16.046305 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:16.046434 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:57:16.235-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:16.695105 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:16.695153 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:16.695178 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:16.695199 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:16.699377 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd81071be10, SHUTDOWN
2019-04-29 09:57:16.699466 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:16.699495 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8117102e0, CONNECTING
2019-04-29 09:57:16.699522 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:16.700029 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:18.169887 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:18.169980 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:18.899387 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:18.899435 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:19.045403 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:19.046225 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:19.046658 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:19.046740 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:19.049208 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8115f2540, SHUTDOWN
2019-04-29 09:57:19.049283 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:19.049354 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8126841b0, CONNECTING
2019-04-29 09:57:19.049367 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:19.049424 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:57:19.813-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:19.849072 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:19.849122 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:19.849196 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:19.849246 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:20.102052 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8117102e0, SHUTDOWN
2019-04-29 09:57:20.102098 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:20.102117 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd812a4d070, CONNECTING
2019-04-29 09:57:20.102135 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:20.102191 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:20.995526 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:20.995556 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:21.349610 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:21.349635 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:22.056915 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:22.057230 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:22.057333 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:22.057414 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:22.061921 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8126841b0, SHUTDOWN
2019-04-29 09:57:22.061951 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:22.061982 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd813943860, CONNECTING
2019-04-29 09:57:22.061993 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:22.062030 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:22.832019 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:22.832099 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:57:22.851-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:22.852752 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:22.852803 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:22.852866 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:22.852906 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:22.853165 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd812a4d070, SHUTDOWN
2019-04-29 09:57:22.853235 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:22.853295 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd813db3960, CONNECTING
2019-04-29 09:57:22.854077 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:22.854684 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:25.058484 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:25.058584 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:25.058625 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:25.058655 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:25.061888 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd813943860, SHUTDOWN
2019-04-29 09:57:25.061915 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:25.061936 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd814922670, CONNECTING
2019-04-29 09:57:25.061945 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:25.062012 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:25.424313 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:25.424380 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:25.425697 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:25.425720 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:25.897400 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:25.897466 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:25.897496 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:25.897526 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:25.913082 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd813db3960, SHUTDOWN
2019-04-29 09:57:25.913116 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:25.913168 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd814c489a0, CONNECTING
2019-04-29 09:57:25.913182 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:25.913222 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:57:25.982-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:27.039887 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:27.039932 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:27.547389 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:27.552982 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:28.064222 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:28.064253 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:28.064308 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:28.064336 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:28.066702 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd814922670, SHUTDOWN
2019-04-29 09:57:28.066804 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:28.066834 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd815bd8640, CONNECTING
2019-04-29 09:57:28.066851 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:28.066894 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:28.903702 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:28.903730 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:28.903765 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:28.903789 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:28.905021 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd814c489a0, SHUTDOWN
2019-04-29 09:57:28.905062 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:28.905078 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd815fdb5f0, CONNECTING
2019-04-29 09:57:28.905094 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:28.905131 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:57:29.012-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:30.235127 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:30.235288 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:31.066717 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:31.066878 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:31.066969 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:31.066997 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:31.067267 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd815bd8640, SHUTDOWN
2019-04-29 09:57:31.067291 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:31.067334 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd816ff51b0, CONNECTING
2019-04-29 09:57:31.067354 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:31.067907 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:31.904310 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:31.904345 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:31.904433 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:31.904461 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:31.905492 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd815fdb5f0, SHUTDOWN
2019-04-29 09:57:31.905627 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:31.905705 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8174809a0, CONNECTING
2019-04-29 09:57:31.905762 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:31.905841 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:31.989033 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:31.989062 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:31.989297 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:31.989315 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29T09:57:32.038-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:34.067651 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:34.067876 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:34.067931 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:34.067952 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:34.069776 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd816ff51b0, SHUTDOWN
2019-04-29 09:57:34.069803 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:34.069839 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd818351830, CONNECTING
2019-04-29 09:57:34.069852 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:34.069894 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:34.907024 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:34.907118 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:34.907266 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:34.907313 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:34.921633 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8174809a0, SHUTDOWN
2019-04-29 09:57:34.921701 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:34.921739 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8185d5df0, CONNECTING
2019-04-29 09:57:34.921748 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:34.921794 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:57:35.065-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:36.047729 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:36.048202 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:36.696760 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:36.696833 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:37.068504 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:37.068542 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:37.069779 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:37.069933 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:37.070774 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd818351830, SHUTDOWN
2019-04-29 09:57:37.070860 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:37.070976 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd81a33ec50, CONNECTING
2019-04-29 09:57:37.071044 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:37.071353 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:37.909618 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:37.909655 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:37.909707 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:37.909730 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:37.912666 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd8185d5df0, SHUTDOWN
2019-04-29 09:57:37.912704 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:37.912730 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd81ac569f0, CONNECTING
2019-04-29 09:57:37.912751 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:37.912796 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:57:38.104-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
2019-04-29 09:57:39.052367 I | etcdserver/api/v3rpc: grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout"; Reconnecting to {127.0.0.1:2379 0  <nil>}
2019-04-29 09:57:39.052440 I | etcdserver/api/v3rpc: Failed to dial 127.0.0.1:2379: grpc: the connection is closing; please retry.
2019-04-29 09:57:40.107565 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:40.107711 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:40.107802 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:40.107841 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:40.141284 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd81a33ec50, SHUTDOWN
2019-04-29 09:57:40.141626 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcc7f0ce120
2019-04-29 09:57:40.141880 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd81b3e94e0, CONNECTING
2019-04-29 09:57:40.141931 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcc7f0ce120
2019-04-29 09:57:40.142025 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:40.909917 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: []
2019-04-29 09:57:40.909956 I | etcdserver/api/v3rpc: ccBalancerWrapper: removing subconn
2019-04-29 09:57:40.910063 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29 09:57:40.910079 I | etcdserver/api/v3rpc: ccBalancerWrapper: new subconn: [{127.0.0.1:2379 0  <nil>}]
2019-04-29 09:57:40.911496 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd81ac569f0, SHUTDOWN
2019-04-29 09:57:40.911569 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xcbd8a40960
2019-04-29 09:57:40.911627 I | etcdserver/api/v3rpc: balancerWrapper: handle subconn state change: 0xd81bb282f0, CONNECTING
2019-04-29 09:57:40.911638 I | etcdserver/api/v3rpc: ccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xcbd8a40960
2019-04-29 09:57:40.911691 I | etcdserver/api/v3rpc: balancerWrapper: got update addr from Notify: [{127.0.0.1:2379 <nil>}]
2019-04-29T09:57:41.211-0700	ERROR	error when flushing data	{"cache-policy": "recently_read", "instance": 3, "time": "2019-04-29T09:00:00.000-0700", "error": "persist manager cannot start persist, not idle"}
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run.func1
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:137
github.com/m3db/m3/src/dbnode/storage.(*fileSystemManager).Run
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/fs.go:145
github.com/m3db/m3/src/dbnode/storage.(*mediator).Tick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:170
github.com/m3db/m3/src/dbnode/storage.(*mediator).ongoingTick
	/Users/haijuncao/gocode/src/github.com/m3db/m3/src/dbnode/storage/mediator.go:204
*** Test killed: ran too long (10m0s).
FAIL	github.com/m3db/m3/src/dbnode/integration	612.849s
